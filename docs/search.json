[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Scope and Methods for Political Science Research\n        ",
    "section": "",
    "text": "Scope and Methods for Political Science Research\n        \n        \n            A Hands-On Introduction to Political Science Data Analysis with R.\n        \n        \n            Summer 2025Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nLecturer\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   hgoers\n\n\n\n\nCourse details\n\n   June 2 - July 11\n   Asynchronous\n   Online\n\n\n\nOffice hours\n\n   By appointment\n   Zoom\n\n\n\n\nContacting me\nE-mail is the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours."
  },
  {
    "objectID": "content/03-wrangling.html",
    "href": "content/03-wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "I am now going to introduce you to a set of important tools for tidying your data. Oftentimes, the data we want to work with does not come to us in a format that is easy for us to use. We need to wrangle it into a better structure and remove inconsistencies.\nWe will continue to work with the variables the Gapminder Project is interested in: health and wealth. However, instead of accessing the lovely clean data set provided to you in the gapminder R package, we are going to go straight to the source and collect data on countries’ GDP per capita and average life expectancy from the World Bank. Let’s get started!",
    "crumbs": [
      "Content",
      "Session 3",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/03-wrangling.html#introduction",
    "href": "content/03-wrangling.html#introduction",
    "title": "Data Wrangling",
    "section": "",
    "text": "I am now going to introduce you to a set of important tools for tidying your data. Oftentimes, the data we want to work with does not come to us in a format that is easy for us to use. We need to wrangle it into a better structure and remove inconsistencies.\nWe will continue to work with the variables the Gapminder Project is interested in: health and wealth. However, instead of accessing the lovely clean data set provided to you in the gapminder R package, we are going to go straight to the source and collect data on countries’ GDP per capita and average life expectancy from the World Bank. Let’s get started!",
    "crumbs": [
      "Content",
      "Session 3",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/03-wrangling.html#set-up",
    "href": "content/03-wrangling.html#set-up",
    "title": "Data Wrangling",
    "section": "Set up",
    "text": "Set up\nTo complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"janitor\", \"scales\", \"wbstats\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(wbstats)",
    "crumbs": [
      "Content",
      "Session 3",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/03-wrangling.html#lengthening-messy-data",
    "href": "content/03-wrangling.html#lengthening-messy-data",
    "title": "Data Wrangling",
    "section": "Lengthening messy data",
    "text": "Lengthening messy data\nThe World Bank provides us with access to a trove of official country- and sub-national level data that are very useful for political analysis. I use their data in nearly all of my research. Sadly, they tend to provide their data in a messy format. We will now collect that messy data so we can learn how to tidy it up.\nTo access their data and some wonderful data visualizations, you can head over to their data portal: https://data.worldbank.org/. From there, you can browse which data sets they have or search for ones you are interested in.\nWe are going to start by collecting data on countries’ GDP per capita (current US$). It can be accessed here: https://data.worldbank.org/indicator/NY.GDP.PCAP.CD. You can download the data directly from this web page, save it in the appropriate place in your RProject, and read it in from there.\n\n\n\n\n\n\nNote\n\n\n\nThis process is similar to the one we used in Session 2: From Samples to the Population.\n\n\nI saved the CSV in the data folder (which is the in the content folder because this is a website), so I will use here::here() to adaptively find the correct file path and read the file in using read_csv().\n\ngdp_per_cap_raw &lt;- read_csv(here::here(\"content\", \"data\", \n                                       \"API_NY.GDP.PCAP.CD_DS2_en_csv_v2_76.csv\"))\n\nAfter running this yourself, you will see an ominous warning. Let’s take a look at our data:\n\ngdp_per_cap_raw\n\n# A tibble: 268 × 3\n   `Data Source`               `World Development Indicators` ...3              \n   &lt;chr&gt;                       &lt;chr&gt;                          &lt;chr&gt;             \n 1 Last Updated Date           2024-12-16                      &lt;NA&gt;             \n 2 Country Name                Country Code                   \"Indicator Name,I…\n 3 Aruba                       ABW                            \"GDP per capita (…\n 4 Africa Eastern and Southern AFE                            \"GDP per capita (…\n 5 Afghanistan                 AFG                            \"GDP per capita (…\n 6 Africa Western and Central  AFW                            \"GDP per capita (…\n 7 Angola                      AGO                            \"GDP per capita (…\n 8 Albania                     ALB                            \"GDP per capita (…\n 9 Andorra                     AND                            \"GDP per capita (…\n10 Arab World                  ARB                            \"GDP per capita (…\n# ℹ 258 more rows\n\n\nWe have only three columns, none of which appear to have the right names. It also looks like the first few rows may, in fact, not be rows in our data set. Rather, they are metadata, including the last time the data were updated.\nThe second row in our data set looks like the real column headings. This is good! We can skip the first few rows using read_csv()’s skip argument. Just provide it with the number of rows you want to skip when reading in the CSV.\n\ngdp_per_cap_raw &lt;- read_csv(here::here(\"content\", \"data\", \n                                       \"API_NY.GDP.PCAP.CD_DS2_en_csv_v2_76.csv\"),\n                            skip = 3)\n\ngdp_per_cap_raw\n\n# A tibble: 266 × 69\n   `Country Name` `Country Code` `Indicator Name` `Indicator Code` `1960` `1961`\n   &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n 1 Aruba          ABW            GDP per capita … NY.GDP.PCAP.CD      NA     NA \n 2 Africa Easter… AFE            GDP per capita … NY.GDP.PCAP.CD     186.   187.\n 3 Afghanistan    AFG            GDP per capita … NY.GDP.PCAP.CD      NA     NA \n 4 Africa Wester… AFW            GDP per capita … NY.GDP.PCAP.CD     122.   127.\n 5 Angola         AGO            GDP per capita … NY.GDP.PCAP.CD      NA     NA \n 6 Albania        ALB            GDP per capita … NY.GDP.PCAP.CD      NA     NA \n 7 Andorra        AND            GDP per capita … NY.GDP.PCAP.CD      NA     NA \n 8 Arab World     ARB            GDP per capita … NY.GDP.PCAP.CD      NA    213.\n 9 United Arab E… ARE            GDP per capita … NY.GDP.PCAP.CD      NA     NA \n10 Argentina      ARG            GDP per capita … NY.GDP.PCAP.CD      NA     NA \n# ℹ 256 more rows\n# ℹ 63 more variables: `1962` &lt;dbl&gt;, `1963` &lt;dbl&gt;, `1964` &lt;dbl&gt;, `1965` &lt;dbl&gt;,\n#   `1966` &lt;dbl&gt;, `1967` &lt;dbl&gt;, `1968` &lt;dbl&gt;, `1969` &lt;dbl&gt;, `1970` &lt;dbl&gt;,\n#   `1971` &lt;dbl&gt;, `1972` &lt;dbl&gt;, `1973` &lt;dbl&gt;, `1974` &lt;dbl&gt;, `1975` &lt;dbl&gt;,\n#   `1976` &lt;dbl&gt;, `1977` &lt;dbl&gt;, `1978` &lt;dbl&gt;, `1979` &lt;dbl&gt;, `1980` &lt;dbl&gt;,\n#   `1981` &lt;dbl&gt;, `1982` &lt;dbl&gt;, `1983` &lt;dbl&gt;, `1984` &lt;dbl&gt;, `1985` &lt;dbl&gt;,\n#   `1986` &lt;dbl&gt;, `1987` &lt;dbl&gt;, `1988` &lt;dbl&gt;, `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;, …\n\n\nThat looks better! We have now read in the data set itself, skipping those rows of metadata.\nThe resulting data set is a wide one. Each observation is a country or region. The first two columns provide information on the country. The third and fourth describe the indicator. All other columns provide each country’s GDP per capita in all years from 1960 to 2023.\nNow, imagine you want to compare a country’s GDP per capita across many years. For example, you want to know how Australians’ wealth has grown over time. This is very difficult to do with this current format. We would need to work with all columns from 1960 to 2023!\nTo make these data easier to work with, we will lengthen the data set. Instead of each observation (row) describing a country, we will make each observation describe a country-year.\nTo do this, we need to use pivot_longer(). We need to let it know which columns we want to transpose using the col argument. Because the end year for these data will change each year I want to pull and clean up this data set (for example, next year the last column will be 2024, not 2023), I will use ! to negate the columns I don’t want to transpose. This code will, therefore, work after each annual update to the data set.\n\ngdp_per_cap &lt;- gdp_per_cap_raw |&gt; \n  pivot_longer(cols = !c(`Country Name`:`Indicator Code`),\n               names_to = \"year\",\n               values_to = \"gdp_per_cap\")\n\ngdp_per_cap\n\n# A tibble: 17,290 × 6\n   `Country Name` `Country Code` `Indicator Name`         `Indicator Code` year \n   &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;                    &lt;chr&gt;            &lt;chr&gt;\n 1 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1960 \n 2 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1961 \n 3 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1962 \n 4 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1963 \n 5 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1964 \n 6 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1965 \n 7 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1966 \n 8 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1967 \n 9 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1968 \n10 Aruba          ABW            GDP per capita (current… NY.GDP.PCAP.CD   1969 \n# ℹ 17,280 more rows\n# ℹ 1 more variable: gdp_per_cap &lt;dbl&gt;\n\n\nI have also told pivot_longer() what to call the column containing the previous column names (using names_to) and the column containing the values (using values_to).\nYou might have noticed that I needed to include some back ticks when referencing those column names. This is because these column names do not follow the rules put in place to help you work with R. Column names must:\n\nNot include spaces\nNot start with numbers or special characters\n\nThey should also be:\n\nShort\nMeaningful\nConsistently formatted\n\nThe World Bank uses spaces in its column names. We need to remove those so we can more easily work with them in R. Happily, the very handy janitor R package is here for all of your cleaning needs!\nI use the clean_names() function to ensure names are clean and consistent:\n\ngdp_per_cap &lt;- clean_names(gdp_per_cap)\ngdp_per_cap\n\n# A tibble: 17,290 × 6\n   country_name country_code indicator_name     indicator_code year  gdp_per_cap\n   &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1960           NA\n 2 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1961           NA\n 3 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1962           NA\n 4 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1963           NA\n 5 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1964           NA\n 6 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1965           NA\n 7 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1966           NA\n 8 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1967           NA\n 9 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1968           NA\n10 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD 1969           NA\n# ℹ 17,280 more rows\n\n\nGreat! We’re almost there. Next, we need to check that R has correctly classified our data types. For example, we want to make sure that the years are coded as numbers not strings of characters.\n\nglimpse(gdp_per_cap)\n\nRows: 17,290\nColumns: 6\n$ country_name   &lt;chr&gt; \"Aruba\", \"Aruba\", \"Aruba\", \"Aruba\", \"Aruba\", \"Aruba\", \"…\n$ country_code   &lt;chr&gt; \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\",…\n$ indicator_name &lt;chr&gt; \"GDP per capita (current US$)\", \"GDP per capita (curren…\n$ indicator_code &lt;chr&gt; \"NY.GDP.PCAP.CD\", \"NY.GDP.PCAP.CD\", \"NY.GDP.PCAP.CD\", \"…\n$ year           &lt;chr&gt; \"1960\", \"1961\", \"1962\", \"1963\", \"1964\", \"1965\", \"1966\",…\n$ gdp_per_cap    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nHmm, it does appear to have read the years in as characters. Why?\n\nunique(gdp_per_cap$year)\n\n [1] \"1960\"  \"1961\"  \"1962\"  \"1963\"  \"1964\"  \"1965\"  \"1966\"  \"1967\"  \"1968\" \n[10] \"1969\"  \"1970\"  \"1971\"  \"1972\"  \"1973\"  \"1974\"  \"1975\"  \"1976\"  \"1977\" \n[19] \"1978\"  \"1979\"  \"1980\"  \"1981\"  \"1982\"  \"1983\"  \"1984\"  \"1985\"  \"1986\" \n[28] \"1987\"  \"1988\"  \"1989\"  \"1990\"  \"1991\"  \"1992\"  \"1993\"  \"1994\"  \"1995\" \n[37] \"1996\"  \"1997\"  \"1998\"  \"1999\"  \"2000\"  \"2001\"  \"2002\"  \"2003\"  \"2004\" \n[46] \"2005\"  \"2006\"  \"2007\"  \"2008\"  \"2009\"  \"2010\"  \"2011\"  \"2012\"  \"2013\" \n[55] \"2014\"  \"2015\"  \"2016\"  \"2017\"  \"2018\"  \"2019\"  \"2020\"  \"2021\"  \"2022\" \n[64] \"2023\"  \"...69\"\n\n\nLooking at all unique values included in the year column, we can see the culprit: \"...69\". It looks like the CSV includes a rouge last column with no data in it. read_csv() read that column in and coded all its values as NA:\n\ngdp_per_cap |&gt; \n  filter(year == \"...69\") |&gt; \n  distinct(gdp_per_cap)\n\n# A tibble: 1 × 1\n  gdp_per_cap\n        &lt;dbl&gt;\n1          NA\n\n\nWe can simply filter this out of our data set to get rid of it and convert the remaining values to numbers using mutate():\n\ngdp_per_cap &lt;- gdp_per_cap |&gt; \n  filter(year != \"...69\") |&gt; \n  mutate(year = as.numeric(year))\ngdp_per_cap\n\n# A tibble: 17,024 × 6\n   country_name country_code indicator_name     indicator_code  year gdp_per_cap\n   &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n 1 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1960          NA\n 2 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1961          NA\n 3 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1962          NA\n 4 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1963          NA\n 5 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1964          NA\n 6 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1965          NA\n 7 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1966          NA\n 8 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1967          NA\n 9 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1968          NA\n10 Aruba        ABW          GDP per capita (c… NY.GDP.PCAP.CD  1969          NA\n# ℹ 17,014 more rows\n\n\nWe now have a nice and clean data set that is easy to work with. Let’s take a look at Australia’s growth over time:\n\ngdp_per_cap |&gt; \n  filter(country_name == \"Australia\") |&gt; \n  ggplot(aes(x = year, y = gdp_per_cap)) + \n  geom_line() + \n  geom_point(size = 1) + \n  theme_minimal() + \n  labs(x = \"Year\",\n       y = \"GDP per capita (current US$)\") + \n  scale_y_continuous(labels = dollar)",
    "crumbs": [
      "Content",
      "Session 3",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/03-wrangling.html#widening-messy-data",
    "href": "content/03-wrangling.html#widening-messy-data",
    "title": "Data Wrangling",
    "section": "Widening messy data",
    "text": "Widening messy data\nSometimes you want to widen your data. tidyr provides a similar function, pivot_wider(), to do just this. Let’s start by getting some long data. We will again collect these data from the World Bank, but this time we will use wbstats to access it directly through the API.\nWe will add to our data set information on each country’s average life expectancy (ID: SP.DYN.LE00.IN).\n\ngapminder_raw &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  return_wide = F\n)\n\ngapminder_raw\n\n# A tibble: 27,776 × 11\n   indicator_id   indicator     iso2c iso3c country  date value unit  obs_status\n   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     \n 1 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2023  NA   &lt;NA&gt;  &lt;NA&gt;      \n 2 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2022  62.9 &lt;NA&gt;  &lt;NA&gt;      \n 3 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2021  62.0 &lt;NA&gt;  &lt;NA&gt;      \n 4 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2020  62.6 &lt;NA&gt;  &lt;NA&gt;      \n 5 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2019  63.6 &lt;NA&gt;  &lt;NA&gt;      \n 6 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2018  63.1 &lt;NA&gt;  &lt;NA&gt;      \n 7 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2017  63.0 &lt;NA&gt;  &lt;NA&gt;      \n 8 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2016  63.1 &lt;NA&gt;  &lt;NA&gt;      \n 9 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2015  62.7 &lt;NA&gt;  &lt;NA&gt;      \n10 SP.DYN.LE00.IN Life expecta… AF    AFG   Afghan…  2014  62.5 &lt;NA&gt;  &lt;NA&gt;      \n# ℹ 27,766 more rows\n# ℹ 2 more variables: footnote &lt;chr&gt;, last_updated &lt;date&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, the wonderful wb_data() function will return data to you in a clean (and wide) format. Therefore, to illustrate how to wrangle these data, I need to include the return_wide = F argument.\n\n\nWe now have data on each country-year-indicator. For example, let’s look at what we have for Afghanistan in 2018:\n\ngapminder_raw |&gt; \n  filter(country == \"Afghanistan\", date == 2018)\n\n# A tibble: 2 × 11\n  indicator_id   indicator      iso2c iso3c country  date value unit  obs_status\n  &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     \n1 SP.DYN.LE00.IN Life expectan… AF    AFG   Afghan…  2018  63.1 &lt;NA&gt;  &lt;NA&gt;      \n2 NY.GDP.PCAP.CD GDP per capit… AF    AFG   Afghan…  2018 491.  &lt;NA&gt;  &lt;NA&gt;      \n# ℹ 2 more variables: footnote &lt;chr&gt;, last_updated &lt;date&gt;\n\n\nThis is difficult to work with. For example, think about how you would plot each country’s GDP per capita against its average life expectancy for a single year.\nWe need to make these data wider. We want our observations to be country-year. We will therefore have a column for each country-year’s GDP per capita and its average life expectancy.\npivot_wider() works by creating a new column for each unique value in the column you tell it to draws names from (using the names_from argument). It will then populate that column with the corresponding value from the column you tell it to draw values from (using the values_from argument). It preserves all unique values in the other rows.\nThis is a little easier to understand in practice. Let’s step through widening our Gapminder data. I’ll start by diving straight in:\n\npivot_wider(gapminder_raw, names_from = indicator_id, values_from = value)\n\n# A tibble: 27,776 × 11\n   indicator    iso2c iso3c country  date unit  obs_status footnote last_updated\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;date&gt;      \n 1 Life expect… AF    AFG   Afghan…  2023 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 2 Life expect… AF    AFG   Afghan…  2022 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 3 Life expect… AF    AFG   Afghan…  2021 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 4 Life expect… AF    AFG   Afghan…  2020 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 5 Life expect… AF    AFG   Afghan…  2019 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 6 Life expect… AF    AFG   Afghan…  2018 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 7 Life expect… AF    AFG   Afghan…  2017 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 8 Life expect… AF    AFG   Afghan…  2016 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 9 Life expect… AF    AFG   Afghan…  2015 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n10 Life expect… AF    AFG   Afghan…  2014 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n# ℹ 27,766 more rows\n# ℹ 2 more variables: SP.DYN.LE00.IN &lt;dbl&gt;, NY.GDP.PCAP.CD &lt;dbl&gt;\n\n\nWe now have two new columns: SP.DYN.LE00.IN for average life expectancy, and NY.GDP.PCAP.CD for GDP per capita. These columns contain the corresponding values for each country-year’s average life expectancy and GDP per capita.\nHowever, something has gone wrong. Our wider data set should have fewer rows than our longer one. This is because we are hoping to have one row for each country-year containing information on its average life expectancy and GDP per capita for each year, rather than two rows for each country-year (one for each indicator). Why is this happening?\nWell, the World Bank data set includes unique information about each indicator in the indicator column. Because this is country-year-indicator level information, pivot_wider() - in an attempt to preserve that information - creates a country-year-indicator level data set. To tidy and widen our data set, we need to remove all country-year-indicator level information and then pivot it.\n\ngapminder &lt;- gapminder_raw |&gt;\n  select(!indicator) |&gt; \n  pivot_wider(names_from = indicator_id, values_from = value)\ngapminder\n\n# A tibble: 13,897 × 10\n   iso2c iso3c country      date unit  obs_status footnote last_updated\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;date&gt;      \n 1 AF    AFG   Afghanistan  2023 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 2 AF    AFG   Afghanistan  2022 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 3 AF    AFG   Afghanistan  2021 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 4 AF    AFG   Afghanistan  2020 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 5 AF    AFG   Afghanistan  2019 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 6 AF    AFG   Afghanistan  2018 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 7 AF    AFG   Afghanistan  2017 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 8 AF    AFG   Afghanistan  2016 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n 9 AF    AFG   Afghanistan  2015 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n10 AF    AFG   Afghanistan  2014 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28  \n# ℹ 13,887 more rows\n# ℹ 2 more variables: SP.DYN.LE00.IN &lt;dbl&gt;, NY.GDP.PCAP.CD &lt;dbl&gt;\n\n\nMuch better! We now have a data set at the country-year level. Each country-year has information on its average life expectancy and GDP per capita.\nThose variable names are hard to work with and not very meaningful, so we need to clean them up:\n\ngapminder &lt;- rename(gapminder, avg_life_exp = SP.DYN.LE00.IN, gdp_per_cap = NY.GDP.PCAP.CD)\ngapminder\n\n# A tibble: 13,897 × 10\n   iso2c iso3c country  date unit  obs_status footnote last_updated avg_life_exp\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;date&gt;              &lt;dbl&gt;\n 1 AF    AFG   Afghan…  2023 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           NA  \n 2 AF    AFG   Afghan…  2022 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           62.9\n 3 AF    AFG   Afghan…  2021 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           62.0\n 4 AF    AFG   Afghan…  2020 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           62.6\n 5 AF    AFG   Afghan…  2019 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           63.6\n 6 AF    AFG   Afghan…  2018 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           63.1\n 7 AF    AFG   Afghan…  2017 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           63.0\n 8 AF    AFG   Afghan…  2016 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           63.1\n 9 AF    AFG   Afghan…  2015 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           62.7\n10 AF    AFG   Afghan…  2014 &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;     2025-01-28           62.5\n# ℹ 13,887 more rows\n# ℹ 1 more variable: gdp_per_cap &lt;dbl&gt;\n\n\nThere is also a fair bit of information in this data set that I do not need, so I will remove it and reorder the columns so they are easier to view:\n\ngapminder &lt;- select(gapminder, country, iso3c, date, avg_life_exp, gdp_per_cap)\ngapminder\n\n# A tibble: 13,897 × 5\n   country     iso3c  date avg_life_exp gdp_per_cap\n   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 Afghanistan AFG    2023         NA          416.\n 2 Afghanistan AFG    2022         62.9        357.\n 3 Afghanistan AFG    2021         62.0        356.\n 4 Afghanistan AFG    2020         62.6        511.\n 5 Afghanistan AFG    2019         63.6        497.\n 6 Afghanistan AFG    2018         63.1        491.\n 7 Afghanistan AFG    2017         63.0        525.\n 8 Afghanistan AFG    2016         63.1        522.\n 9 Afghanistan AFG    2015         62.7        566.\n10 Afghanistan AFG    2014         62.5        625.\n# ℹ 13,887 more rows\n\n\nWe now have a nice, clean, and wide data set. You can easily look at the relationship between your two variables of interest over time:\n\ngapminder |&gt; \n  filter(date %in% 2017:2022) |&gt; \n  ggplot(aes(x = log(gdp_per_cap), y = avg_life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  facet_wrap(~ date) + \n  theme_minimal() + \n  labs(x = \"GDP per capita (logged current US$)\",\n       y = \"Average life expectancy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nfacet_wrap() creates a unique plot for each value in the column you specify. Here, I have used it to create six plots for each of the six years from 2017 to 2022.",
    "crumbs": [
      "Content",
      "Session 3",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/03-wrangling.html#exercises",
    "href": "content/03-wrangling.html#exercises",
    "title": "Data Wrangling",
    "section": "Exercises",
    "text": "Exercises\nName three different types of cases you can use through janitor::clean_names() to format your column names.\n\n\n\n\n\n\nHINT\n\n\n\nRead the argument descriptions in the clean_names() function documentation (by running ?clean_names in your console).\n\n\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\nHere is the full list:\nc(\"snake\", \"small_camel\", \"big_camel\",\n  \"screaming_snake\", \"parsed\", \"mixed\", \"lower_upper\", \"upper_lower\",\n  \"swap\", \"all_caps\", \"lower_camel\", \"upper_camel\", \"internal_parsing\",\n  \"none\", \"flip\", \"sentence\", \"random\", \"title\")\nI like the default (snake case), but other common ones include big camel and all caps.\n\n\n\nHow would you lengthen the gapminder data set we created earlier?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\ngapminder |&gt; \n  pivot_longer(cols = avg_life_exp:gdp_per_cap, names_to = \"indicator\")\n\n# A tibble: 27,794 × 5\n   country     iso3c  date indicator    value\n   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1 Afghanistan AFG    2023 avg_life_exp  NA  \n 2 Afghanistan AFG    2023 gdp_per_cap  416. \n 3 Afghanistan AFG    2022 avg_life_exp  62.9\n 4 Afghanistan AFG    2022 gdp_per_cap  357. \n 5 Afghanistan AFG    2021 avg_life_exp  62.0\n 6 Afghanistan AFG    2021 gdp_per_cap  356. \n 7 Afghanistan AFG    2020 avg_life_exp  62.6\n 8 Afghanistan AFG    2020 gdp_per_cap  511. \n 9 Afghanistan AFG    2019 avg_life_exp  63.6\n10 Afghanistan AFG    2019 gdp_per_cap  497. \n# ℹ 27,784 more rows",
    "crumbs": [
      "Content",
      "Session 3",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/06-probability.html",
    "href": "content/06-probability.html",
    "title": "Probability",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"janitor\", \"ggdist\", \"MetBrewer\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(polisciols)\n\nThis session, we are going to be working with randomness. If you would like to work with the same randomness I am working with, you can set your seed to the same as mine by running the following line of code:\n\nset.seed(1234)\n\n\n\n\n\n\n\nNote\n\n\n\nThe numbers produced when you set your seed are still random. They are; however, the same random numbers each time you run your code. This makes your code reproducible. For a great explanation of what setting your seed means and how R deals with randomness, I recommend this blog post.",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#set-up",
    "href": "content/06-probability.html#set-up",
    "title": "Probability",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"janitor\", \"ggdist\", \"MetBrewer\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(polisciols)\n\nThis session, we are going to be working with randomness. If you would like to work with the same randomness I am working with, you can set your seed to the same as mine by running the following line of code:\n\nset.seed(1234)\n\n\n\n\n\n\n\nNote\n\n\n\nThe numbers produced when you set your seed are still random. They are; however, the same random numbers each time you run your code. This makes your code reproducible. For a great explanation of what setting your seed means and how R deals with randomness, I recommend this blog post.",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#introduction",
    "href": "content/06-probability.html#introduction",
    "title": "Probability",
    "section": "Introduction",
    "text": "Introduction\nThus far, we have focused on identifying patterns and relationships within our data. We have learnt how to look through the noise to identify how our variables change with one another. As our independent variables change, does our dependent variable increase, decrease, or stay the same? When paired with causal theory, we are armed with tools to help us make sense of the complex world of politics and social behavior more broadly.\nWe have not, however, yet discussed what that noise all means for our confidence in the patterns and relationships we have uncovered. These final two sessions will focus on the question of uncertainty. How confident can we be that the relationships we uncover in our data are real, and not just the product of random noise?\nTo answer this question of uncertainty, you need a quick refresher or introduction to probability theory. This topic tends to be soaked in abstraction: Greek letters tend to abound. This method has never worked for me: I like to see what is going on under the hood. As such, I will introduce you to these concepts using simulations and illustrations. I will include the Greek letters so you can read the dusty old textbooks that tend to accompany more advanced statistics courses. But please keep in mind that probability can be more straightforward than the statisticians would have you believe!\nLet’s get started!",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#first-some-philosophy",
    "href": "content/06-probability.html#first-some-philosophy",
    "title": "Probability",
    "section": "First, some philosophy…",
    "text": "First, some philosophy…\nThere are two approaches to interpreting probability: frequentism and Bayesianism. The dominant approach in social science is frequentism, so that’s what we will focus on. However, it is useful to understand where both come from.\nFrequentists argue that probabilities are the proportion of times an event occurs over an infinite number of identical trials. To illustrate, consider the probability of getting heads when you flip a coin. Frequentists would argue that it is the number of times you get a head after flipping a coin an infinite number of times divided by the number of trials you ran (infinity).\nHmm… we cannot flip a coin an infinite number of times and you cannot divide something by infinity. Frequentists, therefore, need to make do with approximating probabilities. They do this by replacing the infinite number of trials with a very large number of trials. For example, they find that if you flip a coin one million times you tend to get heads half a million times. Therefore, the probability of getting heads when you flip a coin is 0.5 (or 50%).\nBayesians have a different approach. They believe probabilities represent your subjective beliefs, which are informed by your knowledge and experience of the event. These subjective beliefs are constantly being updated based on new information you receive. For example, a Bayesian may start with the belief that the probability of getting heads is 0.8 (or 80%). They believe that if you flip a coin 100 times, you will get 80 heads. You then go ahead and flip that coin 100 times and instead you get 50 heads. The Bayesian will then update their belief about the probability of flipping the coin. They will not discard their prior belief; instead they will move their belief closer to what they observed. They may split the difference and update their belief to a probability of 0.65. Every time you run your trial (flipping the coin 100 times and reporting the number of heads) they will update their beliefs based on this new information and their prior.\nEach approach has its critics. You, of course, cannot flip a coin an infinite number of times, so frequentists have defined their way out of ever having precise probabilities. Critics of Bayesianism argue that subjective beliefs should not influence empirical analysis. We will focus on frequentism moving forward, but if you are interesting in learning more about the Bayesian approach I recommend this fantastic introductory book (in R): Bayes Rules! An Introduction to Applied Bayesian Modeling by Alicia A. Johnson, Miles Q. Ott, Mine Dogucu.",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#axioms-of-probability",
    "href": "content/06-probability.html#axioms-of-probability",
    "title": "Probability",
    "section": "Axioms of probability",
    "text": "Axioms of probability\nProbability theory rests on three basic rules. First, the probability of any event, A, is non-negative. Formally:\n\\[\nP(A) \\ge 0\n\\]\nFor example, the probability of getting a head cannot be negative. It must be zero or positive.\nSecond, the probability of all possible events (the sample space, or \\(\\Omega\\)) is equal to one. Formally:\n\\[\nP(\\Omega) = 1\n\\]\nFor example, all possible events that could result from a coin flip are heads or tails. Therefore, the probability that you will get a head or a tail is equal to one.\nThird, if events cannot occur at the same time (they are mutually exclusive), then the probability that each occurs is equal to the sum of the probabilities that each individually occurs. Formally:\n\\[\nP(A\\ or\\ B) = P(A) + P(B)\n\\]\nFor example, you cannot flip a coin and get both heads and tails. Therefore, these outcomes are mutually exclusive. The probability of getting heads or tails when flipping a fair coin is:\n\\[\nP(H\\ or\\ T) = P(H) + P(T) = 0.5 + 0.5 = 1\n\\]\nFrom these three basic rules, we can derive all of probability theory! Put together, they imply that probabilities range from zero to one and that the probability of all possible outcomes from some trial must add up to one.",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#probability-distributions",
    "href": "content/06-probability.html#probability-distributions",
    "title": "Probability",
    "section": "Probability distributions",
    "text": "Probability distributions\nWe are interested in learning how likely an outcome is to occur. We can assign outcomes a random variable, which is simply a numeric representation of that event. For example, I might define a coin landing on heads a random variable of 1. I can also define a coin not landing on heads a random variable of 0.\nEach random variable has a probability distribution. This defines the likelihood that each outcome will occur. I will focus on two such distributions: the Bernoulli and the Normal distribution.\nBernoulli distribution\nThe Bernoulli distribution is the probability distribution of a binary variable. For example, here is the Bernoulli distribution of a fair coin toss:\n\ntibble(x = rbinom(1e6, 1, 0.5)) |&gt; \n  count(x) |&gt; \n  mutate(x = if_else(x == 1, \"HEAD\", \"TAIL\"),\n         prop = n / sum(n)) |&gt;\n  ggplot(aes(x = x, y = prop)) + \n  geom_col() + \n  labs(x = \"Event\",\n       y = \"Probability\") + \n  theme_minimal()\n\n\n\n\n\n\n\nHere is the Bernoulli distribution of a coin that lands on tails 75 percent of the time:\n\ntibble(x = rbinom(1e6, 1, 0.25)) |&gt; \n  count(x) |&gt; \n  mutate(x = if_else(x == 1, \"HEAD\", \"TAIL\"),\n         prop = n / sum(n)) |&gt;\n  ggplot(aes(x = x, y = prop)) + \n  geom_col() + \n  labs(x = \"Event\",\n       y = \"Probability\") + \n  theme_minimal()\n\n\n\n\n\n\n\nHow do you find an event’s Bernoulli distribution? Recall that frequentists conduct a large number of trials and tally up the number of each outcome that occurs across those trials. The probability of each outcome is just that count of occurances divided by the total number of trials they ran.\nTo demonstrate, I am going to create a coin in R. I am not going to tell you whether this coin is fair.\n\nReveal the coinflip_coin &lt;- function(n) {\n  \n  if_else(rbinom(n = n, size = 1, prob = 0.8) == 1, \"HEADS\", \"TAILS\")\n  \n}\n\n\nHidden in that code block is my coin (you can take a look at it at the end of this section). To flip the coin, we simply run the following code:\n\nflip_coin(1)\n\n[1] \"HEADS\"\n\n\n\nflip_coin(1)\n\n[1] \"HEADS\"\n\n\n\nflip_coin(1)\n\n[1] \"HEADS\"\n\n\nSo, your task is to find the Bernoulli distribution of this coin. In other words, how likely are you to flip heads and how likely are you to flip tails?\n\n\n\n\n\n\nNote\n\n\n\nBecause the probability of all possible outcomes needs to add up to one (see axiom number two) and we only have two possible outcomes, you can choose one outcome (heads or tails), record the number of times that outcome occurs, and divide it by the number of trials you run (coin flips). You can then subtract that probability from one to get the probability of the other outcome.\n\n\nWe are going to flip our coin one million times (thankfully R can do this very quickly). Each time we flip the coin, we will record its outcome (heads or tails). At the end, we will have conducted our very large number of trials and should have a good frequentist approximation of the true Bernoulli distribution of our coin.\n\nresults &lt;- tibble(trial = 1:1e6,\n                  outcome = flip_coin(1e6))\n\nresults\n\n# A tibble: 1,000,000 × 2\n   trial outcome\n   &lt;int&gt; &lt;chr&gt;  \n 1     1 HEADS  \n 2     2 HEADS  \n 3     3 TAILS  \n 4     4 HEADS  \n 5     5 HEADS  \n 6     6 HEADS  \n 7     7 HEADS  \n 8     8 HEADS  \n 9     9 HEADS  \n10    10 TAILS  \n# ℹ 999,990 more rows\n\n\nOkay, so we now have the results of our one million trials. Let’s tally them up:\n\napprox_dist &lt;- tabyl(results, outcome)\napprox_dist\n\n outcome      n  percent\n   HEADS 799748 0.799748\n   TAILS 200252 0.200252\n\n\nIn our one million trials, we got heads 799,748 times (or 80% of the time). We got tails 200,252 times (or 20% of the time). Let’s visualize this:\n\nggplot(approx_dist, aes(x = outcome, y = percent)) + \n  geom_col() + \n  labs(x = \"Event\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\nSo, do you think the coin is fair? No! It’s stacked towards heads. Let’s look at the true Bernoulli distribution of this coin:\n\nflip_coin\n\nfunction (n) \n{\n    if_else(rbinom(n = n, size = 1, prob = 0.8) == 1, \"HEADS\", \n        \"TAILS\")\n}\n&lt;bytecode: 0x129e646c0&gt;\n\n\nFocus on the rbinom(n, size = 1, prob = 0.8) part of the function. The third argument (prob) is the probability that rbinom() will return a one (the random variable I have assigned to heads). I set it to 0.8 (or 80%). So, our large number of trials approximated - with a good amount of precision - the Bernoulli distribution of this coin.\nNormal distribution\nThe Normal distribution is another common distribution that is critical to regression and modelling more broadly. We will discuss that connection in the next session.\nThe Normal distribution is symmetric, bell-shaped. Here it is visually:\n\nggplot() + \n  geom_density(aes(x = rnorm(1e6, mean = 0, sd = 1))) + \n  theme_minimal() + \n  labs(x = NULL)\n\n\n\n\n\n\n\nThe shape of this bell-curve relies on two things: first, the mean (\\(\\mu\\)) and second, the variance (\\(\\sigma^2\\)). The mean determines the center of the distribution. In the plot above, the Normal distribution is centered at its mean of zero.\nThe variance determines how spread out away from that mean the distribution is. The plot above has a standard deviation (which is the square root of the variance (I know… I also don’t know why they just stick with standard deviation…)) of one. Let’s compare this to a Normal distribution with the same mean but with a standard deviation of two:\n\nggplot() + \n  geom_density(aes(x = rnorm(1e6, mean = 0, sd = 1)), colour = \"red\") + \n  geom_density(aes(x = rnorm(1e6, mean = 0, sd = 2)), colour = \"blue\") + \n  theme_minimal() + \n  labs(x = NULL)\n\n\n\n\n\n\n\nThe Normal distribution with a larger variance (in blue) is more spread out around the center. It is also much shorter than the Normal distribution with the smaller variance. It might be helpful to think of a bucket containing a gallon of water. If you replace that bucket with a wider one, the gallon of water will fill that wider space and will be shallower in the bucket. When we increase the variance of the Normal distribution, we increase the space within which that same volume of outcomes can fall.\nJust to introduce you to the Greek letters, here is how we write the Normal distribution formally:\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]\nThe original Normal distribution plotted above is:\n\\[\nX \\sim \\mathcal{N}(0, 1^2)\n\\]\nThis Normal distribution (with a mean of zero and a variance of one) is referred to as the standard Normal distribution. It is a wonderfully handy distribution that we will discuss in more detail shortly.\nWhat does this have to do with probability? Well, it turns out that many statistics of many trials follow the Normal distribution. Let me demonstrate. Let’s flip a coin 10,000 times, recording the number of heads those coins land on (our statistic), and then repeat that process 1,000 times. We will use a fair coin this time.\n\nrun_n_trials &lt;- function(n) {\n  \n  trial &lt;- rbinom(n, 1, 0.5)\n\n  result &lt;- tabyl(trial) |&gt; \n    filter(trial == 1) |&gt; \n    pull(percent)\n  \n  return(result)\n  \n}\n\nresults &lt;- tibble(trial = 1:1000) |&gt; \n  rowwise() |&gt; \n  mutate(prop_heads = run_n_trials(10000)) |&gt; \n  ungroup()\n\nresults\n\n# A tibble: 1,000 × 2\n   trial prop_heads\n   &lt;int&gt;      &lt;dbl&gt;\n 1     1      0.495\n 2     2      0.494\n 3     3      0.501\n 4     4      0.498\n 5     5      0.496\n 6     6      0.494\n 7     7      0.506\n 8     8      0.504\n 9     9      0.495\n10    10      0.493\n# ℹ 990 more rows\n\n\nNow, let’s plot the distribution of the proportion of heads that resulted from each 10,000 coin flips in our 1,000 trials:\n\nggplot(results, aes(x = prop_heads)) + \n  geom_density() + \n  theme_minimal()\n\n\n\n\n\n\n\nIt’s (a slightly wonky) Normal distribution. Formally, we would say this is approximately Normally distributed. Also, look at that center point: it’s very close to 0.5 (the true underlying probability of flipping heads). If we did many more trials we would get increasingly close to a Normal distribution (the Central Limit Theorem) and that Normal distribution would be centered at 0.5 (the Law of Large Numbers).\n\n\n\n\n\n\nNote\n\n\n\nThe Central Limit Theorem states that the standardized sample mean of some event, \\(X\\), can increasingly be approximated by the standard Normal distribution as the sample size increases.\nThe Law of Large Numbers states that the sample mean of \\(X\\) increasingly approximates the population mean of \\(X\\) as the sample size increases.\n\n\nIt’s important to note that not all trials resulted in 50% heads. Why? We set up a fair coin (rbinom(1, 1, prob = 0.5)), and yet some trials appear to have resulted in far fewer heads than tails and some resulted in many more heads than tails. Trial number 418 only landed on heads 48% of the time. Trial number 796, on the other hand, landed on heads 51% of the time.\nThis is the result of random noise. There’s no way around this. If you were to pick up a coin and flip it a couple of times, you would probably get runs of heads and runs of tails despite that coin being fair. But, happily, the most likely outcome from our trials was (close to) the true underlying probability of 0.5. Most of our trials of 10,000 coin flips resulted in 5,000 heads or very close to 5,000 heads. Those extremes (much less or much more than 5,000 heads) were very rare.\nWhat we can take from this is that a single trial may not give us the true underlying statistic we are after. Flipping a coin 10,000 times and recording the proportion of outcomes that were heads may not provide us with the true underlying probability that the coin will land on heads. However, if we run many trials of 10,000 coin flips we will increasingly get closer to that true underlying probability.\nAlso, the most likely outcome of any single trial is the true underlying probability of our event. This gives us some hope for inferring from a single sample to our population (but I am getting ahead of myself).",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#using-probability-distributions",
    "href": "content/06-probability.html#using-probability-distributions",
    "title": "Probability",
    "section": "Using probability distributions",
    "text": "Using probability distributions\nThe probability distribution of a Bernoulli distribution is intuitive to understand. If 80 percent of the coin flips land on heads, the probability you will get heads on any given coin flip is determined to be 0.8.\nThe probability distribution of the Normal distribution, on the other hand, is a little less intuitive. It works the same functionally: for any given outcome (for example, the coin landing on heads 60 percent of the time), the probability of that outcome is the count of the number of times it occurs divided by the number of trials you conducted. But, unlike with the Bernoulli distribution, these outcomes are not discrete. The probability density function of the Normal distribution gives us the probability for any proportion of heads from 0 to 1 (or 0 to 100 percent).\nIt is, therefore, more useful to ask what the probability of some range of outcomes occurring is. Most of the time, we are interested in learning how likely some value or a more extreme value are of occurring. For example, politicians running in elections often want to know their chances of winning. This is the probability that they will get more than 50 percent of the vote. To answer this question, we can calculate the area under the curve of the Normal distribution within our range of interest.\nThis task is made abundantly more easy when we use the standard Normal distribution (which is centered at a mean of 0 and has a variance of 1). We can convert any Normal distribution to the standard Normal distribution (similarly to how you can convert degrees Celsius to Fahrenheit, or kilograms to pounds).\nWe refer to the outcomes in a standard Normal distribution as \\(z\\). So after we convert the proportion of heads that result from a very large number of large trials from a Normal distribution to a standard Normal distribution we would refer to each possible proportion of heads as \\(z\\).\nThe standard Normal distribution has two useful properties. First, because it is symmetric and centered at 0, the probability that some outcome takes a value less than or equal to \\(-z\\) is equal to the probability that it takes a value greater than or equal to \\(z\\). To illustrate, the size of the light orange shading on the left hand side of this plot (which covers all values of \\(z\\) less than or equal to -1) is precisely equal to the size of the light orange shading to the right (which covers all values of \\(z\\) greater than or equal to 1):\n\nggplot() + \n  stat_slab(\n    aes(x = rnorm(1e6, 0, 1),\n        fill_ramp = after_stat(x &lt; -1 | x &gt; 1)), \n    fill = met.brewer(\"Egypt\")[1]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\n\n\n\nSecond, we know a lot of useful cut off points of this distribution. For example, we know that 95% of \\(z\\)s fall between a \\(z\\) of -1.96 and 1.96. To illustrate:\n\nggplot() + \n  stat_slab(\n    aes(x = rnorm(1e6, 0, 1),\n        fill_ramp = after_stat(x &lt; -1.96 | x &gt; 1.96)), \n    fill = met.brewer(\"Egypt\")[1]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\n\n\n\nThe dark shaded area includes 95% of the probability distribution. Outcomes (\\(z\\)s) that fall below -1.96 and above 1.96 occur only 5% of the time.\n\n\n\n\n\n\nNote\n\n\n\nStatistical significance is often attached to a p-value of 0.05 (or 5%). THIS IS WHERE THIS VALUE COMES FROM! Again, I am jumping ahead of myself. We will talk about this next session.\n\n\nWe can easily calculate the area under the curve of the standard Normal distribution using pnorm(). For example, here is the probability of obtaining a \\(z\\) value of -1.96 or less:\n\npnorm(-1.96)\n\n[1] 0.0249979\n\n\nHere is the probability of obtaining a \\(z\\) value of 0 or less:\n\npnorm(0)\n\n[1] 0.5\n\n\nAnd here is the probability of obtaining a \\(z\\) value of 1.96 or more:\n\npnorm(1.96, lower.tail = F)\n\n[1] 0.0249979\n\n\n\n\n\n\n\n\nTip\n\n\n\npnorm() will, by default, calculate the area under the curve starting at negative infinity (the “lower tail”) and working up to the value you provide. You can ask it to start at positive infinity and work back to the value you provide by setting lower.tail to FALSE (or F for short).",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#standardizing-your-normal-random-variable",
    "href": "content/06-probability.html#standardizing-your-normal-random-variable",
    "title": "Probability",
    "section": "Standardizing your Normal random variable",
    "text": "Standardizing your Normal random variable\nSo, how do we get from our coin flips to this very useful standard Normal distribution? We need to shift our center (mean) from whatever it is to zero and truncate or distribute the spread of our values around that center so they fall within a variance of one. The function to do this is:\n\\[\nz = \\frac{X - \\mu}{\\sigma}\n\\]\nWhere \\(\\mu\\) is the mean (or center of your original Normal distribution) and \\(\\sigma\\) is its standard deviation.\nReturning to the results of our many coin flips, we can calculate the mean and standard deviation using R.\nHere are the results of our 1,000 trials of 10,000 coin flips:\n\nresults\n\n# A tibble: 1,000 × 2\n   trial prop_heads\n   &lt;int&gt;      &lt;dbl&gt;\n 1     1      0.495\n 2     2      0.494\n 3     3      0.501\n 4     4      0.498\n 5     5      0.496\n 6     6      0.494\n 7     7      0.506\n 8     8      0.504\n 9     9      0.495\n10    10      0.493\n# ℹ 990 more rows\n\n\nThe mean of the proportion of heads flipped in each of those 1,000 trials is:\n\ncoin_mean &lt;- mean(results$prop_heads)\ncoin_mean\n\n[1] 0.5000345\n\n\nThe standard deviation is:\n\ncoin_sd &lt;- sd(results$prop_heads)\ncoin_sd\n\n[1] 0.005008006\n\n\nSo we can standardize our 1,000 trial statistics using the above formula:\n\nstandard_results &lt;- results |&gt; \n  mutate(standard_prop_heads = (prop_heads - coin_mean) / coin_sd)\n\nstandard_results\n\n# A tibble: 1,000 × 3\n   trial prop_heads standard_prop_heads\n   &lt;int&gt;      &lt;dbl&gt;               &lt;dbl&gt;\n 1     1      0.495              -1.07 \n 2     2      0.494              -1.15 \n 3     3      0.501               0.253\n 4     4      0.498              -0.346\n 5     5      0.496              -0.786\n 6     6      0.494              -1.26 \n 7     7      0.506               1.21 \n 8     8      0.504               0.872\n 9     9      0.495              -1.07 \n10    10      0.493              -1.32 \n# ℹ 990 more rows\n\n\nLet’s plot those to make sure they look like a standard Normal distribution:\n\nggplot(standard_results, aes(x = standard_prop_heads)) + \n  geom_density() + \n  theme_minimal()\n\n\n\n\n\n\n\nLooking good! Remember, this is simply a translation of the proportion of heads resulting from each trial we did above (it’s the same slightly wonky shape!).\nWe can now apply all the things we know about the standard Normal distribution to our distribution of the proportion of heads. For example, we can state that 95% of all possible proportions of heads that could result from our trials lie between -1.96 and 1.96 of this standard Normal distribution.\nWe can also translate this back to our proportion of heads. For example:\n\\[\n-1.96 = \\frac{X - 0.500}{0.005}\n\\]\nTherefore:\n\\[\nX = 0.4902\n\\]\nAnd:\n\\[\n1.96 = \\frac{X - 0.500}{0.005}\n\\]\nTherefore:\n\\[\nX = 0.5098\n\\]\nSo, 95 percent of all of our trials resulted in heads between 49.02 and 50.98 percent of the time. Only five percent of our trials resulted in more extreme proportions. That’s pretty good given the true probability of heads is 0.5 (or 50%).\n\n\n\n\n\n\nCaution\n\n\n\nOur distribution of the proportion of heads resulting from each trial was only approximately Normally distributed. Later, we will work with an assumed distribution which is Normally distributed.",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-probability.html#working-with-a-single-sample-to-infer-about-the-population",
    "href": "content/06-probability.html#working-with-a-single-sample-to-infer-about-the-population",
    "title": "Probability",
    "section": "Working with a single sample to infer about the population",
    "text": "Working with a single sample to infer about the population\nLet’s bring this all together. Prior to November 2024, pollsters were very keen to learn the proportion of US voters who intended to vote for Kamala Harris in the 2024 general election. They could not ask all US voters of their intentions. Instead, they asked a sample of the US voting population and infered from that sample the population’s intentions. When we generalize from the sample statistic to the parameter we are engaging in statistical inference.\n\n\n\n\n\n\nNote\n\n\n\nThe data point of interest among the population is referred to as the parameter. Here, it is the proportion of US voters who intended to vote for Kamala Harris in the 2024 general election. We do not know this.\nThe data point of interest among the sample is referred to as the statistic. Here, it is the proportion of survey respondents who intended to vote for Kamala Harris in the 2024 general election. We do know this.\nWe aim to have a statistic that accurately represents the parameter.\n\n\nThey wanted to know how many people intended to vote for Kamala Harris. How can we be confident that their statistic represented the parameter?\nGenerally speaking, the more our sample “looks like” our population, the more confident we can be that we have a good statistic. Drawing on probability theory, our sample is increasingly likely to resemble our population with its randomness and size (also remember back to our discussion of experiments).\nYou should strive for a large pure random sample. In a pure random sample, every individual within your population is equally likely to be drawn. This is really hard to achieve! Nonetheless, you should also strive for as large a sample as you can possibly get. More is always better in terms of statistical inference (if not your research budget or time).\nSampling error\nImagine you have a large and representative sample. You are still going to have some error. This is because your sample varies in all the normal ways events with uncertainty vary. To illustrate, let’s return to our coin flips.\nWe state our possible outcomes:\n\npossible_outcomes &lt;- c(\"HEADS\", \"TAILS\")\npossible_outcomes\n\n[1] \"HEADS\" \"TAILS\"\n\n\nWe flip our coin 100 times:\n\ncoin_flip_100 &lt;- sample(possible_outcomes, 100, replace = T, prob = c(0.5, 0.5))\ncoin_flip_100\n\n  [1] \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [10] \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\"\n [19] \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [28] \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\"\n [37] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\"\n [46] \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [55] \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\"\n [64] \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [73] \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\"\n [82] \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\"\n [91] \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\"\n[100] \"TAILS\"\n\n\nWe know that the true probability of the fair coin landing on heads is 0.5. Therefore, if we flip a fair coin 100 times, we should get 50 heads. However, we also know from above that these random draws are a bit noisy: we can get proportions that do not reflect the underlying probability of 0.5. What did we get in this draw?\n\ntable(coin_flip_100)\n\ncoin_flip_100\nHEADS TAILS \n   52    48 \n\n\nThe more flips we do, the closer we will get to that true probability distribution of 0.5. To demonstrate, let’s do 1,000,000 100-coin flip trials and record the number of heads we get each time:\n\ncoin_flip &lt;- function(possible_outcomes, n) {\n  \n  outcomes &lt;- sample(possible_outcomes, size = n, replace = T, prob = c(0.5, 0.5))\n  \n  return(table(outcomes)[\"HEADS\"])\n  \n}\n\nresults &lt;- tibble(trial = 1:1e6) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 100))\n\nresults\n\n# A tibble: 1,000,000 × 2\n# Rowwise: \n   trial n_heads\n   &lt;int&gt;   &lt;int&gt;\n 1     1      51\n 2     2      51\n 3     3      43\n 4     4      44\n 5     5      46\n 6     6      46\n 7     7      49\n 8     8      57\n 9     9      55\n10    10      52\n# ℹ 999,990 more rows\n\n\nWhat are the results of these repeated trials?\n\nggplot(results, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 50) + \n  theme_minimal() + \n  labs(x = \"Percentage of heads drawn in 100 coin-flip trial\",\n       y = \"Count of trials\")\n\n\n\n\n\n\n\nThe most common outcome of these one million 100-coin-flip trials is our true underlying proportion: \\(Pr(H, T) = \\{0.5, 0.5\\}\\). The next most common outcomes are very close to that true underlying proportion. If I were to select a trial at random from those one million we just conducted, I am most likely to get one that resulted in 50% heads or very close to 50% heads. This is because there are many more trials that resulted in and around 50% heads than did not.\nWe do get some trials in which we draw many more or far fewer than our expected 50 heads. We have some as low as 25 heads and some as large as 73 heads. But again, the number of heads recorded in most of our trials are clustered around our expected 50. The average number of heads drawn in all of our trials is 49.990517 heads which is really, really close to our known parameter of 0.5 or 50%. Yay!\nSo, even with representative and large samples you will get some error. That’s okay. As long as the mean of the sampling distribution of an infinite number of identical trials would equal the true population parameter, we have an unbiased statistic that we can use to infer things about our population of interest. Here, each of the outcomes of our trials - including those ones that resulted in 25 heads and 73 heads - are unbiased statistics because the mean of infinite identical trials would be 50 heads.\nSampling distributions\nLet’s move on from coin flips. Suppose that we want to know how many Americans identify as Democrats. We will return to the American National Election Survey to answer this question. The data are stored in the polisciols R package you installed last session.\nThis survey asks respondents whether they identify as a Democrat (this binary variable is 0 if not and 1 if they do). Let’s look at these data for our first five survey respondents:\n\nnes |&gt; \n  select(respondent_id, dem) |&gt; \n  head(5)\n\n# A tibble: 5 × 2\n  respondent_id dem  \n          &lt;dbl&gt; &lt;fct&gt;\n1        200015 Other\n2        200022 Other\n3        200039 Other\n4        200046 Other\n5        200053 Other\n\n\nLet’s very cheekily pretend that this is a complete survey of the entire voting population of America. That way, we can pretend that we know the true proportion of US voters (our population of interest) who identify as Democrats (our parameter).\nWhat is that proportion?\n\ntabyl(nes, dem)\n\n      dem    n   percent\n    Other 6419 0.7752415\n Democrat 1861 0.2247585\n\n\nOkay, so let’s pretend that of all US voters identify as Democrats.\nWe can’t survey all voters, so instead we take a representative and large simple random sample from this population:\n\nnes_sample &lt;- nes |&gt; \n  select(respondent_id, dem) |&gt; \n  slice_sample(n = 3000)\n\nWe have taken a pure random sample of 3,000 individuals (or 36% of our population of 8,280 voters). Every voter (member of our population) had an equal probability of being picked for this sample.\nWhat proportion of this sample identify as Democrats?\n\ntabyl(nes_sample, dem)\n\n      dem    n   percent\n    Other 2329 0.7763333\n Democrat  671 0.2236667\n\n\n. Nice! But what if we took a different sample of 3,000?\n\nnes_sample_2 &lt;- nes |&gt; \n  select(respondent_id, dem) |&gt; \n  slice_sample(n = 3000)\n\ntabyl(nes_sample_2, dem)\n\n      dem    n   percent\n    Other 2306 0.7686667\n Democrat  694 0.2313333\n\n\nWe get a different answer: . Of course! This is just like our different coin flip trials from above. Each resulted in a different number of heads. The more flips we do, the closer we get to the true underlying probability distribution.\nLet’s take 1,000 different samples of 3,000 US voters and see what we get:\n\ndem_survey &lt;- function(df, n) {\n  \n  slice_sample(df, n = n) |&gt; \n    tabyl(dem) |&gt; \n    filter(dem == \"Democrat\") |&gt; \n    pull(percent)\n  \n}\n\nnes_samples_1000 &lt;- tibble(survey = 1:1000) |&gt; \n  rowwise() |&gt; \n  mutate(prop_dem = dem_survey(select(nes, respondent_id, dem), 3000)) |&gt; \n  ungroup()\n\nnes_samples_1000\n\n# A tibble: 1,000 × 2\n   survey prop_dem\n    &lt;int&gt;    &lt;dbl&gt;\n 1      1    0.235\n 2      2    0.217\n 3      3    0.218\n 4      4    0.219\n 5      5    0.236\n 6      6    0.228\n 7      7    0.232\n 8      8    0.230\n 9      9    0.224\n10     10    0.228\n# ℹ 990 more rows\n\n\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent), \n             colour = \"lightblue\", \n             linewidth = 2) + \n  geom_vline(xintercept = mean(nes_samples_1000$prop_dem), \n             colour = \"pink\",\n             linewidth = 2) + \n  theme_minimal() + \n  labs(x = \"Proportion of respondents who identify as Democrats\",\n       y = \"Count of trials\")\n\n\n\n\n\n\n\nOn average, 22.45% of US voters in our 1,000 samples of 3,000 US voters identified as Democrats. This is shown by the pink line in the graph above. Our (cheeky) population proportion is (shown by the blue line in the graph above).\nYay! These are very, very close to each other and we only did 1,000 samples. If we took more and more samples, the average proportion of respondents who identify as Democrats in those trials would get increasingly close to our population’s proportion of Democrats. In fact, theoretically, if we took an infinite number of samples from these 8,280 observations, the average of the proportion of individuals who identify as Democrats in each of those infinite samples would equal exactly the population’s proportion: . Further, we know that the proportions found in each of those trials would be normally distributed around that mean, with the most common outcomes sitting at or very close to the mean.\nAs long as our sample of our population is large and randomly drawn, we know enough about its shape and central point to use it to infer what is going on in the population. When your sample is large and representative (read: randomly drawn), your sampling distribution will be approximately Normally distributed. This follows from the Central Limit Theorem. The center will be at (or very, very close to) the population mean. This follows from the Law of Large Numbers.\nInferring from a single “trial”\nIn a lot of (social) science research it is not practical or, in some cases, possible to do many trials. For example, a lot of us study the onset, conduct, and termination of wars. Unlike a game of chess, you cannot reset and run a war many times in order to get your sampling distribution of your variable of interest.\nFurther, we often do not know the shape or size of our population. For example, the best guess we have of the demographics of the US population comes from the census. But this misses a lot of people. For example, if you want to study houselessness, you might need to rely on surveys of samples of people that may or may not be representative of this difficult to reach population of people.\nA lot of the time; therefore, you will have one data point: one mean, one proportion, one count. To use this one data point to infer something about our population of interest, we need to use some of lessons that we learned from above and make some pretty important assumptions.\nLet’s return to our survey work above. We took 1,000 different samples of 3,000 US voters. We asked each of those US voters whether they identified as Democrats. We then found the proportion of the 3,000 respondents who identified as Democrats in each of our 1,000 different samples. We then took the average of those 1,000 different proportions and compared it to our population average. In line with the Central Limit Theorem and Law of Large Numbers, we found that the average of our sample statistics was very, very close to our population parameter.\nOkay, now imagine that you could only run one of those trials. Let’s select one at random:\n\nnes_single &lt;- slice_sample(nes_samples_1000)\nnes_single\n\n# A tibble: 1 × 2\n  survey prop_dem\n   &lt;int&gt;    &lt;dbl&gt;\n1    276    0.219\n\n\nHow close is this single sample statistic to the population parameter of ? Pretty close! In fact, as discussed above, you are more likely to get a sample statistic close to the population parameter than not.\nRemember, when we ran multiple trials we got many sample statistics that were clustered around the population mean:\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent)) + \n  theme_minimal() + \n  labs(x = \"Proportion of respondents who identify as Democrat\",\n       y = \"Count of trials\")\n\n\n\n\n\n\n\nSo, if you were to pick one of these trials at random, you are more likely to pick one with a sample statistic that is close to the population parameter than not. Convenient!\nHow confident can we be in our statistic?\nThat being said, we could get unlucky and have drawn a large and representative sample that sits at one of those extreme values. How confident can we be that our single sample statistic is close to the population parameter?\n\nggplot() + \n  stat_slab(\n    aes(x = rnorm(n = 1e6, mean = 5, sd = 2),\n        fill_ramp = after_stat(x &lt; -1.96 | x &gt; 1.96)), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\n\n\n\nFor normally distributed data approximately 95% of the data fall within 1.96 standard deviations of the mean (the medium blue). So, if we assume that the statistic we get from our large and representative sample is our “best guess” at the population parameter, we can center our theoretical sampling distribution around this point. We know that these data are Normally distributed. We can use what we know about Normal distributions to identify the boundaries around which we are confident some proportion of all statistics from an infinite number of identically drawn samples would fall.\nLet’s make this more concrete by going back to a single random sample of 3,000 respondents from the NES survey. Let’s draw that sample:\n\nnes_sample &lt;- sample_n(nes, 3000) |&gt; \n  select(respondent_id, dem)\n\nhead(nes_sample)\n\n# A tibble: 6 × 2\n  respondent_id dem     \n          &lt;dbl&gt; &lt;fct&gt;   \n1        204130 Other   \n2        217754 Other   \n3        423955 Other   \n4        316828 Democrat\n5        212681 Democrat\n6        222824 Other   \n\n\nIn this sample, of respondents identify as Democrats. This is our best guess at our parameter of interest: the proportion of US voters who identify as Democrats.\nUsing what we know from above, what would our distribution of proportions look like if this was not the (average) proportion drawn from one trial but instead it was the average proportion drawn from an infinite number of identical trials? Well, those proportions would be Normally distributed around that center point. To fill in that blank, we need one additional piece of information: the standard deviation (or spread) of those points around that center point.\n\n\n\n\n\n\nNote\n\n\n\nWhen we are looking at the standard deviation of the sampling distribution, we refer to it as the standard error.\n\n\nThe formula for working out the standard error of a proportion (such as the proportion of a population who identify as Democrats) is:\n\\[\nse(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nWhere \\(\\hat{p}\\) is the sample proportion (here: ).\n\np_hat &lt;- tabyl(nes_sample, dem) |&gt; \n  filter(dem == \"Democrat\") |&gt; \n  pull(percent)\n\nn &lt;- 3000\n\nse &lt;- sqrt((p_hat * (1 - p_hat)) / n)\nse\n\n[1] 0.007725856\n\n\nNow, let’s use this information to draw many different hypothetical proportions of respondents who identify as Democrats in many different hypothetical samples:\n\nggplot(tibble(x = rnorm(1e6, mean = p_hat, sd = se)), aes(x = x)) + \n  stat_halfeye(.width = c(0.95, 0.68)) + \n  theme_minimal() + \n  labs(x = \"Proportion of respondents who identify as Democrats\",\n       caption = \"Median and mean shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\n\n\n\nSo, if our sample is large and random and we were to draw it many different times (here, 1,000,000 times), we would get a sampling distribution of proportions that resembles the one presented above. This variation is due to random error. There is nothing wrong with our research design or sampling method.\nWe can use what we know about Normal distributions to say that approximately 95% of these hypothetical proportions of people who identify as Democrats fall within 1.96 standard deviations of the mean, or:\n\nlower_95 &lt;- p_hat - 1.96 * se\nlower_95\n\n[1] 0.218524\n\nupper_95 &lt;- p_hat + 1.96 * se\nupper_95\n\n[1] 0.2488093\n\n\nThese boundaries are marked by the thinner black line on the graph above.\nWe are putting a lot of stead in our single sample. That’s okay as long as your sample is large and representative. So, let’s return to the problem faced by pollsters when trying to determine support for Kamala Harris prior to the election. As long as those pollsters took a large and representative sample of the population of US voters, they can assume that the statistic they derive from their survey - the proportion of those surveyed who intended to vote for Harris - is most likely to be equal to the proportion of US voters who intended to vote for Harris. They will know this with some level of confidence, based on the size of and variation within their sample.",
    "crumbs": [
      "Content",
      "Session 6",
      "Probability"
    ]
  },
  {
    "objectID": "content/06-communicating.html",
    "href": "content/06-communicating.html",
    "title": "Data Communication",
    "section": "",
    "text": "Quarto is a tool that helps you to create fully reproducible research outputs. It allows you to combine your code, results, and prose in one document. For example, this website - with all of its R code, prose, and visualizations - was created using Quarto.\nYou can use Quarto from RStudio.1 Below is a screen shot of a Quarto document (file extension .qmd) and its HTML output. You can render a Quarto document to many different types of formats, including PDF and MS Word.\nLet’s make a new Quarto document, including some R code and prose.",
    "crumbs": [
      "Content",
      "Session 6",
      "Data Communication"
    ]
  },
  {
    "objectID": "content/06-communicating.html#a-new-quarto-document",
    "href": "content/06-communicating.html#a-new-quarto-document",
    "title": "Data Communication",
    "section": "A new Quarto document",
    "text": "A new Quarto document\nOpen up a new Quarto document in RStudio:\n\nFill in the relevant fields:\n\nYour new document will have a .qmd file extension. It will also already contain some text and code. Most of this is demonstrative and can be deleted. However, the top section is very important and should be kept. This section (written in YAML) includes the metadata for your document. By default, it includes the title, author, format in which it will rendered, and the default RStudio editor.\n\n\n\n\n\n\nExercise\n\n\n\nSwitch your output (format) from HTML to PDF by changing html to pdf.\n\n\n\n\n\n\n\n\nTip\n\n\n\nA full list of the formats to which you can render your Quarto document is provided here.\n\n\n\nThere are two ways to work with and view Quarto documents. The default editor is visual, which follows a more “what-you-see-is-what-you-get” style. If you have worked a lot with MS Word documents or Google Docs, this interface will look familiar. Alternatively, you can edit in source, which looks more like a raw script. To switch between the two, you can use the Source and Visual icons in the top left hand side of the screen.\n\n\n\n\n\n\nNote\n\n\n\nI find myself switching between these two formats all the time. The visual editor is much easier to work in when writing, but it can be a bit buggy when it comes to writing code. I work in source when I am writing and running R code.",
    "crumbs": [
      "Content",
      "Session 6",
      "Data Communication"
    ]
  },
  {
    "objectID": "content/06-communicating.html#rendering-your-document",
    "href": "content/06-communicating.html#rendering-your-document",
    "title": "Data Communication",
    "section": "Rendering your document",
    "text": "Rendering your document\nTo render your document into your chosen format (in this case: HTML), you need to hit the Render icon in the document’s top bar. This will produce an HTML version of your Quarto document in the same folder in which you saved your Quarto document.\n\n\n\n\n\n\nTip\n\n\n\nYou can preview your document in RStudio by changing your settings to Preview in Viewer Pane.\n\nNow, whenever you render your document a preview of it will show up in the Viewer pane (which is in the same place as your Files, Plots, and Help panes).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you toggle on the Render on Save option, your Quarto document will render and update your viewer every time you hit save. This can be helpful when you are formatting your document.",
    "crumbs": [
      "Content",
      "Session 6",
      "Data Communication"
    ]
  },
  {
    "objectID": "content/06-communicating.html#writing-prose-in-quarto",
    "href": "content/06-communicating.html#writing-prose-in-quarto",
    "title": "Data Communication",
    "section": "Writing prose in Quarto",
    "text": "Writing prose in Quarto\nYou can write prose as you would in any other text editor in Quarto. When you are in the Visual editor model, Quarto provides you with the shortcut keys for many of the formats you use in other text editors, including MS Word and Google Docs. You can also use your usual keyboard shortcuts.\nIn the Source editor mode, you will need to use markdown. Markdown is a lightweight markup language that allows you to format plain text. It gives you a lot of control over the format of your text documents (similar to Latex).\n\n\n\n\n\n\nExercise\n\n\n\nComplete this great Markdown tutorial.",
    "crumbs": [
      "Content",
      "Session 6",
      "Data Communication"
    ]
  },
  {
    "objectID": "content/06-communicating.html#running-code-in-quarto",
    "href": "content/06-communicating.html#running-code-in-quarto",
    "title": "Data Communication",
    "section": "Running code in Quarto",
    "text": "Running code in Quarto\nYou can also run code from within your Quarto document. You can do this through a code chunk or in-line code. I will step through both options now.\n\nCode chunks\nA code chunk starts with ```{r} and ends with ```. You can then write whole “chunks” of code that will output in your rendered document.\nFor example, I will load the tidyverse R package into my current session:\n```{r}\n#| echo: true\n\nlibrary(tidyverse)\n```\nYou can specify your chunk options using #| at the start of the line. For example, above I specified that I wanted the code in the code chunk to be shown when I render my document. You can hide the code by changing the chunk option echo to false. There are many different chunk options that you can control. A full list can be found here.\nYou can set the chunk options in the individual chunks, as show above. Alternatively, you can set them universally in the YAML section at the top of your Quarto document using the execute command. For example:\n```{yaml}\nexecute:\n  echo: true\n  message: false\n  warning: false\n```\nThis will apply to all code chunks unless you overwrite it by including chunk-specific options in a code chunk.\nCode chunks are useful for running large amounts of code. Commonly, I use them to include a plot, a regression table, or to read in my data or model results. For example, you can write the code to create a ggplot directly in your document.\n\n\n\nSource: Quarto\n\n\n\n\nIn-line code\nYou will often want to reference numbers or results in your prose. For example, I may be writing up the data section of a paper and want to specify that my data set includes 100 observations. If I were to write this in normally and then go away and collect more data, I would need to come back and update this number manually to reflect my new number of observations. I may do this several times (very tedious) or I may miss a time (we are all human). In-line coding allows you to make these updates programmatically.\nYou include R code directly in your prose using the expression: r. For example:\n\nWill render as: There are r nrow(mpg) observations in our data. No need to go and update this reference if that number changes!\n\n\n\n\n\n\nTip\n\n\n\nscales is a great R package for formatting numbers.\nFor example, R will output raw numbers such as 1000000000 and 8932348920. scales allows you to format these numbers so they are easier to read: scales::comma(1000000000) gives you r scales::comma(1000000000) and scales::dollar(8932348920) gives you r scales::dollar(8932348920).\n\n\nYou can use Quarto to produce all kinds of fully reproducible documents, including journal articles and reports. You can also use it to produce very professional-looking presentations. Finally, you can even use it to produce websites. In fact, all of the resources provided to you here were produced in Quarto.",
    "crumbs": [
      "Content",
      "Session 6",
      "Data Communication"
    ]
  },
  {
    "objectID": "content/06-communicating.html#footnotes",
    "href": "content/06-communicating.html#footnotes",
    "title": "Data Communication",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use it from VS Code, Jupyter, Neovim, and Editor.↩︎",
    "crumbs": [
      "Content",
      "Session 6",
      "Data Communication"
    ]
  },
  {
    "objectID": "content/05-programming.html",
    "href": "content/05-programming.html",
    "title": "Data Programming",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"nycflights13\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Content",
      "Session 5",
      "Data Programming"
    ]
  },
  {
    "objectID": "content/05-programming.html#set-up",
    "href": "content/05-programming.html#set-up",
    "title": "Data Programming",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"nycflights13\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Content",
      "Session 5",
      "Data Programming"
    ]
  },
  {
    "objectID": "content/05-programming.html#introduction",
    "href": "content/05-programming.html#introduction",
    "title": "Data Programming",
    "section": "Introduction",
    "text": "Introduction\nYou have been introduced to a number of R functions that have been wonderfully helpful in allowing you to write concise code. However, you may sometimes find yourself wanting a new function that doesn’t yet exist. It may be because you have to repeat a task very specific to your work, or because no one else has taken the time to write a function that solves a common problem.\nThis session, I will introduce programming in R to you. You will be able to write your own R functions that will help keep your code concise and robust. I will also point you towards some resources that will help you write your own R package so you can share your code with others.\nLet’s get started!",
    "crumbs": [
      "Content",
      "Session 5",
      "Data Programming"
    ]
  },
  {
    "objectID": "content/05-programming.html#a-common-problem",
    "href": "content/05-programming.html#a-common-problem",
    "title": "Data Programming",
    "section": "A common problem",
    "text": "A common problem\nGood R code is concise and robust. But sometimes we need to perform the same task across many different data frames or numbers.\nFor example, let’s return to our data on the weather at NYC airports:\n\nweather\n\n# A tibble: 26,115 × 15\n   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4 \n 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06\n 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5 \n 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7 \n 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7 \n 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5 \n 7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0 \n 8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4 \n 9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0 \n10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8 \n# ℹ 26,105 more rows\n# ℹ 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;,\n#   visib &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThis data frame includes information about the temperature at each airport. It is recorded in degrees Fahrenheit. I - as an Australian - don’t know what to do with these huge numbers! I need to convert them to the much more sensible degrees Celsius so I can understand them better. The conversion from Fahrenheit to Celsius is done according to this formula:\n\\[\nC = \\frac{F - 32}{\\frac{9}{5}}\n\\]\nSo, to convert each temperature data point from F to C, I could do the following:\n\ntransmute(weather, origin, time_hour, temp_f = temp, temp_c = (temp - 32) / (9/5))\n\n# A tibble: 26,115 × 4\n   origin time_hour           temp_f temp_c\n   &lt;chr&gt;  &lt;dttm&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n 1 EWR    2013-01-01 01:00:00   39.0    3.9\n 2 EWR    2013-01-01 02:00:00   39.0    3.9\n 3 EWR    2013-01-01 03:00:00   39.0    3.9\n 4 EWR    2013-01-01 04:00:00   39.9    4.4\n 5 EWR    2013-01-01 05:00:00   39.0    3.9\n 6 EWR    2013-01-01 06:00:00   37.9    3.3\n 7 EWR    2013-01-01 07:00:00   39.0    3.9\n 8 EWR    2013-01-01 08:00:00   39.9    4.4\n 9 EWR    2013-01-01 09:00:00   39.9    4.4\n10 EWR    2013-01-01 10:00:00   41      5  \n# ℹ 26,105 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\ntransmute() combines mutate() and select(). It’s very handy!\n\n\nAlternatively, I could write a function that includes that formula and then apply that function to my vector of temperature values.\n\nf_to_c &lt;- function(f) {\n  \n  c &lt;- (f - 32) / (9/5)\n  \n  return(c)\n  \n}\n\nLet’s try our new function out:\n\nf_to_c(32)\n\n[1] 0\n\nf_to_c(100)\n\n[1] 37.77778\n\nf_to_c(50)\n\n[1] 10\n\n\nLooking good! Now, let’s apply it to our data:\n\ntransmute(weather, origin, time_hour, temp_f = temp, temp_c = f_to_c(temp))\n\n# A tibble: 26,115 × 4\n   origin time_hour           temp_f temp_c\n   &lt;chr&gt;  &lt;dttm&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n 1 EWR    2013-01-01 01:00:00   39.0    3.9\n 2 EWR    2013-01-01 02:00:00   39.0    3.9\n 3 EWR    2013-01-01 03:00:00   39.0    3.9\n 4 EWR    2013-01-01 04:00:00   39.9    4.4\n 5 EWR    2013-01-01 05:00:00   39.0    3.9\n 6 EWR    2013-01-01 06:00:00   37.9    3.3\n 7 EWR    2013-01-01 07:00:00   39.0    3.9\n 8 EWR    2013-01-01 08:00:00   39.9    4.4\n 9 EWR    2013-01-01 09:00:00   39.9    4.4\n10 EWR    2013-01-01 10:00:00   41      5  \n# ℹ 26,105 more rows",
    "crumbs": [
      "Content",
      "Session 5",
      "Data Programming"
    ]
  },
  {
    "objectID": "content/05-programming.html#how-to-write-a-function",
    "href": "content/05-programming.html#how-to-write-a-function",
    "title": "Data Programming",
    "section": "How to write a function",
    "text": "How to write a function\nThe function we wrote above has three important parts:\n\nA name: f_to_c. Names should be informative and easy to type.\n\nArguments: f. Arguments are the variables the user will supply to the function. Here, we allow the user to supply a temperature in Fahrenheit.\nA body. This includes the code that will run when the function is called.\n\nYou create a function by following the template:\n\nname &lt;- function(arguments) {\n  body\n}",
    "crumbs": [
      "Content",
      "Session 5",
      "Data Programming"
    ]
  },
  {
    "objectID": "content/05-programming.html#a-more-robust-function",
    "href": "content/05-programming.html#a-more-robust-function",
    "title": "Data Programming",
    "section": "A more robust function",
    "text": "A more robust function\nIt would be nice if we could allow the user to provide temperatures in either Fahrenheit or Celsius. Let’s create a more complex but useful function:\n\nconvert_temp &lt;- function(temp, type = c(\"F\", \"C\")) {\n  \n  if (type == \"F\") {\n    \n    t &lt;- (temp - 32) / (9/5)\n    \n  } else {\n    \n    t &lt;- (9/5)*temp + 32\n    \n  }\n  \n  return(t)\n  \n}\n\nLet’s try this out:\n\nconvert_temp(0, \"C\")\n\n[1] 32\n\nconvert_temp(32, \"F\")\n\n[1] 0\n\nconvert_temp(50, \"C\")\n\n[1] 122\n\nconvert_temp(122, \"F\")\n\n[1] 50\n\n\nLooks like it is working!\nWe have included some restrictions on our arguments: users can only supply one of two values: “C” or “F”. However, we have not written very robust code yet. Let’s see what happens when we include something other than “F” or “C” as the type argument:\n\nconvert_temp(0, \"Z\")\n\n[1] 32\n\nconvert_temp(50, \"bloop\")\n\n[1] 122\n\nconvert_temp(100, \"bleep\")\n\n[1] 212\n\n\nHuh… it appears that our function is accepting those values as “C”. That’s because we allowed anything other than “F” to be treated as “C”. Let’s fix that:\n\nconvert_temp &lt;- function(temp, type = c(\"F\", \"C\")) {\n  \n  if (type == \"F\") {\n    \n    t &lt;- (temp - 32) / (9/5)\n    \n  } else if (type == \"C\") {\n    \n    t &lt;- (9/5)*temp + 32\n    \n  } else {\n    \n    stop(\"You need to provide either 'F' or 'C' in the 'type' argument.\")\n    \n  }\n  \n  return(t)\n  \n}\n\nFirst, let’s check that it still works as expected:\n\nconvert_temp(0, \"C\")\n\n[1] 32\n\nconvert_temp(32, \"F\")\n\n[1] 0\n\n\nAnd then let’s check what happens when we supply a different value for type:\n\nconvert_temp(0, \"bleep\")\n\n\n\n\n\nGreat! Not only does the function refuse to provide any output, it also gives the user a helpful error message.",
    "crumbs": [
      "Content",
      "Session 5",
      "Data Programming"
    ]
  },
  {
    "objectID": "content/05-programming.html#r-packages",
    "href": "content/05-programming.html#r-packages",
    "title": "Data Programming",
    "section": "R packages",
    "text": "R packages\nYou can create functions that perform all kinds of very complex tasks. For example, you can write a function that takes entire data frames, fits models against those data frames, and provides summary statistics and various plots of those models. These more complex functions follow the same format introduced to you above. As you build your R skills, you will find more opportunities to write increasingly complex and robust functions.\nYou should not keep those functions to yourself! You should share them with others, who may also be struggling with verbose code. If you have a group of functions that perform similar tasks, you should write them up into an R package. For example, you might want to write a series of functions that perform common conversions. You can add a weight converter to the temperature converter we wrote above.\nR packages require a little bit more work. For example, you need to provide all that helpful documentation you have been referring to when using others’ functions. You will also need to run a series of robustness checks to make sure users’ code doesn’t break if they add something unusual to your functions.\nHowever, packages can be very rewarding to produce. For example, I hope you have found polisciols very helpful! I also had the opportunity to work with the UN and some amazing R coders to write comtradr, an R package that allows users to access the UN Comtrade API from R. There are many opportunities to work with organizations and others when coding in R.\nTo get started building your own R packages, you should check out the helpfully titled R Packages by Hadley Wickham (of R4DS fame) and Jennifer Bryan.",
    "crumbs": [
      "Content",
      "Session 5",
      "Data Programming"
    ]
  },
  {
    "objectID": "content/slides/01-03-plot.html#data-visualization",
    "href": "content/slides/01-03-plot.html#data-visualization",
    "title": "Plotting Your Data",
    "section": "Data visualization",
    "text": "Data visualization\nWe will use data visualization to answer the following question:\n\nDo cars with big engines use more fuel than cars with small engines?"
  },
  {
    "objectID": "content/slides/01-03-plot.html#set-up-your-plot",
    "href": "content/slides/01-03-plot.html#set-up-your-plot",
    "title": "Plotting Your Data",
    "section": "Set up your plot",
    "text": "Set up your plot\nAn empty canvas!\n\nggplot(data = mpg)"
  },
  {
    "objectID": "content/slides/01-03-plot.html#map-your-aesthetics",
    "href": "content/slides/01-03-plot.html#map-your-aesthetics",
    "title": "Plotting Your Data",
    "section": "Map your aesthetics",
    "text": "Map your aesthetics\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))"
  },
  {
    "objectID": "content/slides/01-03-plot.html#add-in-your-cars",
    "href": "content/slides/01-03-plot.html#add-in-your-cars",
    "title": "Plotting Your Data",
    "section": "Add in your cars",
    "text": "Add in your cars\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point()"
  },
  {
    "objectID": "content/slides/01-03-plot.html#look-at-the-relationship-across-all-cars",
    "href": "content/slides/01-03-plot.html#look-at-the-relationship-across-all-cars",
    "title": "Plotting Your Data",
    "section": "Look at the relationship across all cars",
    "text": "Look at the relationship across all cars\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "content/slides/01-03-plot.html#lets-look-at-groups-in-the-data",
    "href": "content/slides/01-03-plot.html#lets-look-at-groups-in-the-data",
    "title": "Plotting Your Data",
    "section": "Let’s look at groups in the data",
    "text": "Let’s look at groups in the data\n\nCan look at more than two interesting elements of our data.\nYou can use visual elements or aesthetics (aes) to communicate many dimensions in your data.\nLet’s look at a categorical variable: the class of car (SUV, 2 seater, pick up truck, etc.).\nLook for meaningfully defined groups."
  },
  {
    "objectID": "content/slides/01-03-plot.html#lets-look-at-groups-in-the-data-1",
    "href": "content/slides/01-03-plot.html#lets-look-at-groups-in-the-data-1",
    "title": "Plotting Your Data",
    "section": "Let’s look at groups in the data",
    "text": "Let’s look at groups in the data\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = class)) + \n  geom_point()"
  },
  {
    "objectID": "content/slides/01-03-plot.html#look-at-the-relationship-within-groups",
    "href": "content/slides/01-03-plot.html#look-at-the-relationship-within-groups",
    "title": "Plotting Your Data",
    "section": "Look at the relationship within groups",
    "text": "Look at the relationship within groups\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = class)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "content/slides/01-03-plot.html#aesthetics-can-be-isolated",
    "href": "content/slides/01-03-plot.html#aesthetics-can-be-isolated",
    "title": "Plotting Your Data",
    "section": "Aesthetics can be isolated",
    "text": "Aesthetics can be isolated\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(aes(colour = class)) + \n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "content/slides/00-01-intro_to_r_rstudio.html#learning-objectives-for-today",
    "href": "content/slides/00-01-intro_to_r_rstudio.html#learning-objectives-for-today",
    "title": "Introduction to R and RStudio",
    "section": "Learning objectives for today",
    "text": "Learning objectives for today\n\nIntroduction to R\nIntroduction to RStudio"
  },
  {
    "objectID": "content/slides/00-01-intro_to_r_rstudio.html#why-r",
    "href": "content/slides/00-01-intro_to_r_rstudio.html#why-r",
    "title": "Introduction to R and RStudio",
    "section": "Why R?",
    "text": "Why R?\n\nA versatile programming language\n\nAll materials used in this course were produced using R, including the website and fancy slides\n\nFree and accessible!"
  },
  {
    "objectID": "content/slides/00-01-intro_to_r_rstudio.html#r-and-rstudio",
    "href": "content/slides/00-01-intro_to_r_rstudio.html#r-and-rstudio",
    "title": "Introduction to R and RStudio",
    "section": "R and RStudio",
    "text": "R and RStudio\n\n\nR\nR is a free and open-source programming language and software environment for statistical computing and graphics.\n\nRStudio\nRStudio is an integrated development environment (IDE) for the R programming language.\n\n\n\nInstructions on how to download R and RStudio are provided in the Install or upgrade R and RStudio chapter in Jennifer Bryan’s Happy Git and GitHub for the useR."
  },
  {
    "objectID": "content/slides/00-01-intro_to_r_rstudio.html#the-r-skills-you-will-learn",
    "href": "content/slides/00-01-intro_to_r_rstudio.html#the-r-skills-you-will-learn",
    "title": "Introduction to R and RStudio",
    "section": "The R skills you will learn",
    "text": "The R skills you will learn\nThis course will introduce you to both statistics and R. Focusing on R, you will learn how to:\n\nImport your data into R\nTidy your data\nTransform it\nVisualize it\nModel patterns and relationships within it\nCommunicate your findings."
  },
  {
    "objectID": "content/slides/00-01-intro_to_r_rstudio.html#a-tour-of-rstudio",
    "href": "content/slides/00-01-intro_to_r_rstudio.html#a-tour-of-rstudio",
    "title": "Introduction to R and RStudio",
    "section": "A Tour of RStudio",
    "text": "A Tour of RStudio\n\nSource: R4DS"
  },
  {
    "objectID": "content/slides/00-01-intro_to_r_rstudio.html#summary",
    "href": "content/slides/00-01-intro_to_r_rstudio.html#summary",
    "title": "Introduction to R and RStudio",
    "section": "Summary",
    "text": "Summary\nThis session you:\n\nLearnt about new data science tools to help you conduct replicable and reproducible political science research\nSet up your data science tools"
  },
  {
    "objectID": "content/slides/00-02-intro.html#welcome",
    "href": "content/slides/00-02-intro.html#welcome",
    "title": "Welcome!",
    "section": "Welcome",
    "text": "Welcome\n\nHands-on introduction to political science data analysis\nA (re)introduce to R and statistics\nAvoid abstractions and jargon"
  },
  {
    "objectID": "content/slides/00-02-intro.html#what-you-will-learn",
    "href": "content/slides/00-02-intro.html#what-you-will-learn",
    "title": "Welcome!",
    "section": "What you will learn",
    "text": "What you will learn\n\nUse R to collect, clean, and analyze data\nDescribe important features of your outcomes of interest and the variables you think drive changes to those outcomes\nIdentify and evaluate the relationship between two variables using statistical models\nDescribe those relationships using clear and precise language\nCritically evaluate empirical claims made in political news and analysis"
  },
  {
    "objectID": "content/slides/00-02-intro.html#introduction-to-me",
    "href": "content/slides/00-02-intro.html#introduction-to-me",
    "title": "Welcome!",
    "section": "Introduction to me",
    "text": "Introduction to me\n\nCurrently: PhD student and RAND Corporation Adjunct\nPreviously: worked for the Australian government, international organizations, think tanks, and private firms\nLots of experience using statistics to uncover the things driving outcomes of interest\nHead over to ELMs to introduce yourselves to each other!"
  },
  {
    "objectID": "content/slides/00-02-intro.html#course-structure",
    "href": "content/slides/00-02-intro.html#course-structure",
    "title": "Welcome!",
    "section": "Course structure",
    "text": "Course structure\n\nEight sessions, including this introduction and a conclusion\nEach session comprises two parts:\n\nIntroduction to R\nIntroduction to a statistical concept\n\nRead through the content, watch the videos (if included), complete the practice quizzes to test your understanding\nRun the R code yourselves as you read through the content"
  },
  {
    "objectID": "content/slides/00-02-intro.html#assignments",
    "href": "content/slides/00-02-intro.html#assignments",
    "title": "Welcome!",
    "section": "Assignments",
    "text": "Assignments\n\nMultiple choice quiz at the end of every session except the conclusion\nYou need to complete these in ELMs\nA final exam"
  },
  {
    "objectID": "content/slides/00-02-intro.html#seeking-help",
    "href": "content/slides/00-02-intro.html#seeking-help",
    "title": "Welcome!",
    "section": "Seeking help",
    "text": "Seeking help\n\nI will include links to other resources that you can use if you want further clarification\nYou can ask your fellow classmates through ELMs discussion boards\nThe internet is a wonderful thing\nStackOverflow is my go-to for R-related questions\nYou can contact me via email\n\n\n\n\n\n\n\n\n\nI cannot help you with software or R-related issues. You will need to solve these yourself. R is constantly changing - being a good coder involves being good at banging your head against a wall!"
  },
  {
    "objectID": "content/slides/02-05-mutate.html#add-new-variables-with-mutate",
    "href": "content/slides/02-05-mutate.html#add-new-variables-with-mutate",
    "title": "Creating or changing columns of data",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nmutate(gapminder, gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap          gdp\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-05-mutate.html#change-existing-columns",
    "href": "content/slides/02-05-mutate.html#change-existing-columns",
    "title": "Creating or changing columns of data",
    "section": "Change existing columns",
    "text": "Change existing columns\n\nmutate(gapminder, gdpPercap = log(gdpPercap))\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      6.66\n 2 Afghanistan Asia       1957    30.3  9240934      6.71\n 3 Afghanistan Asia       1962    32.0 10267083      6.75\n 4 Afghanistan Asia       1967    34.0 11537966      6.73\n 5 Afghanistan Asia       1972    36.1 13079460      6.61\n 6 Afghanistan Asia       1977    38.4 14880372      6.67\n 7 Afghanistan Asia       1982    39.9 12881816      6.89\n 8 Afghanistan Asia       1987    40.8 13867957      6.75\n 9 Afghanistan Asia       1992    41.7 16317921      6.48\n10 Afghanistan Asia       1997    41.8 22227415      6.45\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-05-mutate.html#select-transform-and-add-new-variables-with-transmute",
    "href": "content/slides/02-05-mutate.html#select-transform-and-add-new-variables-with-transmute",
    "title": "Creating or changing columns of data",
    "section": "Select, transform, and add new variables with transmute()",
    "text": "Select, transform, and add new variables with transmute()\n\ntransmute(gapminder, country, year, gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 3\n   country      year          gdp\n   &lt;fct&gt;       &lt;int&gt;        &lt;dbl&gt;\n 1 Afghanistan  1952  6567086330.\n 2 Afghanistan  1957  7585448670.\n 3 Afghanistan  1962  8758855797.\n 4 Afghanistan  1967  9648014150.\n 5 Afghanistan  1972  9678553274.\n 6 Afghanistan  1977 11697659231.\n 7 Afghanistan  1982 12598563401.\n 8 Afghanistan  1987 11820990309.\n 9 Afghanistan  1992 10595901589.\n10 Afghanistan  1997 14121995875.\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-02-filter.html#dplyr-basics",
    "href": "content/slides/02-02-filter.html#dplyr-basics",
    "title": "Filtering Your Data",
    "section": "dplyr basics",
    "text": "dplyr basics\n\nFirst argument is always a data object (for example, a dataframe).\nSubsequent arguments typically describe which columns to operate on, using the variable names (without quotes).\nOutput is always a new data object."
  },
  {
    "objectID": "content/slides/02-02-filter.html#filter-rows-with-filter",
    "href": "content/slides/02-02-filter.html#filter-rows-with-filter",
    "title": "Filtering Your Data",
    "section": "Filter rows with filter()",
    "text": "Filter rows with filter()\n\nfilter(gapminder, country == \"Australia\", year &gt; 2000)\n\n# A tibble: 2 × 6\n  country   continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Australia Oceania    2002    80.4 19546792    30688.\n2 Australia Oceania    2007    81.2 20434176    34435."
  },
  {
    "objectID": "content/slides/02-02-filter.html#filter-rows-with-filter-1",
    "href": "content/slides/02-02-filter.html#filter-rows-with-filter-1",
    "title": "Filtering Your Data",
    "section": "Filter rows with filter()",
    "text": "Filter rows with filter()\n\nfilter(gapminder, continent %in% c(\"Asia\", \"Oceania\"))\n\n# A tibble: 420 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 410 more rows"
  },
  {
    "objectID": "content/slides/02-02-filter.html#filter-rows-with-filter-2",
    "href": "content/slides/02-02-filter.html#filter-rows-with-filter-2",
    "title": "Filtering Your Data",
    "section": "Filter rows with filter()",
    "text": "Filter rows with filter()\n\nfilter(gapminder, pop &gt; 500000 & pop &lt; 1000000)\n\n# A tibble: 88 × 6\n   country  continent  year lifeExp    pop gdpPercap\n   &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt;\n 1 Bahrain  Asia       1992    72.6 529491    19036.\n 2 Bahrain  Asia       1997    73.9 598561    20292.\n 3 Bahrain  Asia       2002    74.8 656397    23404.\n 4 Bahrain  Asia       2007    75.6 708573    29796.\n 5 Botswana Africa     1962    51.5 512764      984.\n 6 Botswana Africa     1967    53.3 553541     1215.\n 7 Botswana Africa     1972    56.0 619351     2264.\n 8 Botswana Africa     1977    59.3 781472     3215.\n 9 Botswana Africa     1982    61.5 970347     4551.\n10 Comoros  Africa     1997    60.7 527982     1174.\n# ℹ 78 more rows"
  },
  {
    "objectID": "content/slides/02-02-filter.html#filter-rows-with-filter-3",
    "href": "content/slides/02-02-filter.html#filter-rows-with-filter-3",
    "title": "Filtering Your Data",
    "section": "Filter rows with filter()",
    "text": "Filter rows with filter()\n\nfilter(gapminder, pop &gt; 500000 | pop &lt; 1000000)\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-02-filter.html#handy-operations",
    "href": "content/slides/02-02-filter.html#handy-operations",
    "title": "Filtering Your Data",
    "section": "Handy operations",
    "text": "Handy operations\n== is equal to\n\n!= is not equal to\n\n&gt;= is greater than or equal to\n\n&lt;= is less than or equal to"
  },
  {
    "objectID": "content/slides/02-02-filter.html#handy-operations-1",
    "href": "content/slides/02-02-filter.html#handy-operations-1",
    "title": "Filtering Your Data",
    "section": "Handy operations",
    "text": "Handy operations\n\n| is OR\n\n& is AND\n\n%in% is in"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#randomization",
    "href": "content/slides/02-08-randomization.html#randomization",
    "title": "ADVANCED: Randomization",
    "section": "Randomization",
    "text": "Randomization\n\nLast session, we randomly assigned 1,000 hypothetical people to two different groups\nTesting whether randomization helps us create two roughly identical groups prior to treatment\nYou now have a lot of the R code needed to replicate that analysis"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#creating-our-group-of-1000-people",
    "href": "content/slides/02-08-randomization.html#creating-our-group-of-1000-people",
    "title": "ADVANCED: Randomization",
    "section": "Creating our group of 1,000 people",
    "text": "Creating our group of 1,000 people\nImagine we have a group of 1,000 individuals. We know the following about them:\n\nHeight\nWeight\nEye colour"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#creating-our-group-of-1000-people-1",
    "href": "content/slides/02-08-randomization.html#creating-our-group-of-1000-people-1",
    "title": "ADVANCED: Randomization",
    "section": "Creating our group of 1,000 people",
    "text": "Creating our group of 1,000 people\n\ngroup_df &lt;- tibble(\n  id = 1:1000,\n  height = rnorm(1000, 170, 6),\n  weight = rnorm(1000, 80, 10),\n  eye_colour = sample(c(\"Blue\", \"Green\", \"Brown\", \"Grey\"), \n                      1000, \n                      replace = T)\n)\n\ngroup_df\n\n# A tibble: 1,000 × 4\n      id height weight eye_colour\n   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1     1   177.   66.0 Grey      \n 2     2   174.   60.1 Blue      \n 3     3   177.   80.7 Grey      \n 4     4   170.   64.1 Grey      \n 5     5   171.   67.2 Brown     \n 6     6   165.   57.1 Grey      \n 7     7   172.   80.7 Brown     \n 8     8   162.   87.2 Blue      \n 9     9   156.   76.8 Green     \n10    10   167.   74.7 Blue      \n# ℹ 990 more rows"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#the-normal-distribution",
    "href": "content/slides/02-08-randomization.html#the-normal-distribution",
    "title": "ADVANCED: Randomization",
    "section": "The Normal distribution",
    "text": "The Normal distribution\n\nggplot() + \n  geom_density(aes(x = rnorm(n = 1e6, mean = 0, sd = 1))) + \n  theme_minimal()"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#random-sampling-from-the-normal-distribution",
    "href": "content/slides/02-08-randomization.html#random-sampling-from-the-normal-distribution",
    "title": "ADVANCED: Randomization",
    "section": "Random sampling from the Normal distribution",
    "text": "Random sampling from the Normal distribution\nI can take a random sample of n values from a Normal distribution centered at some mean with a specific standard deviation.\n\nBy default, rnorm() takes a mean of 0 and a standard deviation of 1\nThe following code takes 1,000 random samples from that default Normal distribution\n\n\n\nIf this is all nonsense to you, don’t worry! I will step through this in a lot more detail later in the course."
  },
  {
    "objectID": "content/slides/02-08-randomization.html#random-sampling-from-the-normal-distribution-1",
    "href": "content/slides/02-08-randomization.html#random-sampling-from-the-normal-distribution-1",
    "title": "ADVANCED: Randomization",
    "section": "Random sampling from the Normal distribution",
    "text": "Random sampling from the Normal distribution\n\nrnorm(n = 1000, mean = 0, sd = 1)\n\n   [1]  0.071890142 -0.983347004 -0.793344941  0.364379146 -0.961072180\n   [6] -0.162500978 -0.237673394 -0.176932478 -0.926391613  0.782644433\n  [11]  0.145257978  1.058420350 -1.449583165 -0.202173778  1.583759199\n  [16]  0.584594199 -0.986021912 -0.010934975  0.816906000  1.280428695\n  [21] -0.277758455  0.575886661 -1.943294030 -0.917506731 -0.215297949\n  [26] -0.597599947 -1.591842787 -1.649688480  0.489212367 -0.011660132\n  [31]  0.234244678  1.087474342  0.891766071  0.939549957 -0.324621988\n  [36] -0.274473914  2.466303250 -0.073341849  0.668094599  1.291333157\n  [41]  0.763160382  0.464308134 -0.566102210 -1.542845867 -0.415220187\n  [46]  1.892095763  0.361849762  1.347936312  0.132764312  0.068923516\n  [51]  1.920176076 -0.150998414 -0.208639020 -0.852481295 -1.699626172\n  [56]  0.083229939 -0.048749576  1.130335072  0.556497177 -1.421512525\n  [61] -0.181071467  0.156411170 -0.677088103  0.166021619 -1.042863397\n  [66] -0.543503824 -0.759560133 -1.150536624  0.533804628 -0.283936141\n  [71]  1.091740338  1.663089192  0.480370387  1.536516129 -1.158966504\n  [76]  0.340897981 -1.251692090 -0.674042265  0.571873413  0.270916466\n  [81]  0.295881045 -0.176590088  0.103937428  1.024072692  0.230546554\n  [86]  2.734006025 -0.129980895  0.571951842  0.526039443  1.717659814\n  [91]  0.020326386  1.279974929  0.209866029 -0.596750087  0.015813680\n  [96] -0.128030842  0.873521908  0.856588213  0.485647734 -1.252259611\n [101] -1.268791224  0.525747810 -1.193497759 -0.467126447 -0.489846467\n [106]  1.276087880 -0.032709533  2.245671058  0.602627980 -3.318472950\n [111]  0.442448702  0.131567454 -0.225806395  1.216610671  2.543610080\n [116] -0.383718623  1.002948758  1.646653263 -0.935611689 -0.869921535\n [121]  0.885159596 -0.584041622 -0.357168570  0.955076812 -0.909363372\n [126]  0.293329011 -1.083429528 -0.419932138  1.363348142 -0.183868926\n [131] -1.063525889  0.237406409  0.389859479  0.877605879  1.384471714\n [136] -0.348270494 -0.707057852  0.235569241  0.060496590 -0.927448939\n [141] -1.975609973 -0.182236992 -2.020714116 -0.832259202  1.826559063\n [146]  0.112615647  1.354647460 -0.875859075 -1.782825093 -0.975055306\n [151]  0.165042315  1.414365024  0.036255519  0.600924999  0.171400777\n [156]  1.057261842  1.784348483  0.239115749 -1.386565939 -1.051449716\n [161]  0.441915364 -0.358778650 -0.193873573 -1.274328724 -1.425998536\n [166] -0.404242959  0.906310866  0.396828812 -1.418739403  2.062294292\n [171]  0.654837137 -1.028413946  0.720269592  0.487433539 -1.186582663\n [176] -1.216989303 -0.460491253 -0.818576725 -0.840137733  0.836524774\n [181]  1.563158272 -0.699108724 -1.395269831  0.525566921  2.156415817\n [186]  1.077071274 -0.812720446  1.617803566 -1.131067343 -0.292769637\n [191]  1.435648082 -0.095165940  1.517770032  1.107088824 -0.666665788\n [196]  3.197293757 -0.283739627 -1.183646548 -1.948314718  0.328553916\n [201]  0.577606819  0.203528408  0.080795670  0.551769386  0.942869760\n [206]  0.650347112 -0.432096944  2.111989201  1.153535444  0.141809463\n [211] -0.616360192  1.735828556  0.499084661 -0.850590428 -1.241488161\n [216]  0.206014308  1.961047121  0.425813918 -0.063391326 -0.613754882\n [221]  0.466012926  0.201232651  0.983030868 -2.407687762  0.508535643\n [226] -0.735351932 -0.800649647 -0.902297159 -1.170700450 -0.375361593\n [231]  0.722611452  0.309845301 -1.013445680 -1.163266552  1.033083698\n [236]  0.281836641  0.480839473 -2.509386631 -0.035151401  0.526820626\n [241]  0.790768999 -0.065306849  0.353738191 -0.598721108 -1.426138188\n [246]  1.776753397 -0.092091871 -2.025681816  0.318420578 -0.235273274\n [251]  0.754571921 -0.361771960  1.191461117 -0.907252296 -2.083779468\n [256]  1.025333007 -1.365457656 -1.151509797 -0.519012325 -0.666457619\n [261]  1.214857309  1.438311595  0.677165127 -0.959807391  1.624076956\n [266]  0.116352823 -0.251724591 -0.167401016 -2.703886071 -0.488568652\n [271] -1.201831499  0.443823281  0.382216530  0.785166876  1.164902525\n [276] -1.065417865  0.748145778  0.148731628 -1.972761128  0.160044793\n [281] -0.472474491  0.297122839 -0.841748567  1.557527677  0.268480453\n [286]  0.759629202 -0.006936384 -0.839512629  0.636707113  0.626475555\n [291]  2.285514453  0.980201410  1.067043617  1.038910169 -0.231702455\n [296] -0.599746448  0.718552438  3.136026562  0.415048501  0.967892107\n [301]  1.409487445  1.564036149 -0.405033106 -0.136184783 -0.917555894\n [306] -0.616286382  0.067923246  2.173411408  0.985152582 -1.423406242\n [311] -1.986321029 -0.111276552  0.019616161 -3.782591243 -0.531272458\n [316] -2.356040416  0.003412888  1.720815539  1.319881749 -0.441010346\n [321] -1.801254469  2.604427386  0.303939881  1.445863234  0.600694351\n [326] -1.160045361 -0.671960955  0.579202166 -0.387494385 -0.073550296\n [331]  0.786785757  0.514231579 -0.910641831  0.181266399 -0.689867211\n [336]  0.323370902 -2.187792700 -0.024583582  0.179206502  0.674845948\n [341] -1.643781569  0.453575892  2.232990452  0.443212050  0.234221574\n [346]  0.651705461 -0.124921022  0.908703057 -0.133344111  0.307603808\n [351]  2.275931033  1.442686269  0.473214992  0.699161980  0.740486601\n [356]  1.695194595  1.351951188 -0.128781212 -0.132832257 -1.055973769\n [361] -0.269115645 -0.029927191  1.679912004  0.277313543 -0.441399699\n [366] -1.010602234 -1.332776166 -0.095532012 -0.897866003  0.165273149\n [371]  0.462244582  1.294302366 -1.632492108 -0.594200131 -1.417161219\n [376]  0.742761811 -0.120220671  0.893440242  1.091914422  0.450935000\n [381]  1.416194906  0.445842549  1.428889131  1.280407374 -0.056920553\n [386]  0.886507915 -0.676872809 -2.909885274  0.968168069 -1.192506619\n [391] -1.106295440  0.936813459 -1.017666002  0.943435659  0.866959748\n [396]  0.587328918  0.461631298 -1.316852880 -0.901256292  0.285944405\n [401]  1.721408218  0.652259776 -0.492501126 -0.075057657  2.138928734\n [406] -0.117168709 -0.398134258 -1.416898859  0.929373382  0.545202961\n [411]  0.750451857 -0.711860149 -0.246510622  0.525093276  0.602555565\n [416] -0.201516229  0.863491472  0.846324299  1.092548337 -2.038970671\n [421]  0.870350980 -0.322358087 -0.535260643  2.132425750 -0.809122818\n [426]  0.914991391  0.854075158  0.244576387 -0.371129262  0.096949023\n [431] -0.012315007  0.720646514 -0.038110402  0.186061345  0.404438698\n [436]  1.943726794  1.979972466  2.313921198  0.916064183  0.534956982\n [441]  0.649002736  1.452529255  1.103870346 -0.105270003  1.502381887\n [446] -0.877643286  1.174802372 -0.122341142  2.665477932 -0.652679631\n [451] -0.193127219 -0.746273798  2.046773860 -0.321928103 -0.271141050\n [456]  1.111150075  0.001421771 -0.031034805  1.016513254  0.164642424\n [461] -0.595842177 -0.550635495 -0.568567531 -1.627348160 -0.680868053\n [466] -0.222772773 -0.475608255  0.693334467  0.129046711 -0.401381124\n [471]  0.124753645 -2.659164716  0.123052264 -0.347687344 -1.310816136\n [476] -0.586271344 -1.079486669  0.168299365  0.645418398  0.690765928\n [481] -0.272375773 -0.907141843  0.877077269 -1.820714871  0.877870477\n [486] -1.025772256 -0.630185975 -1.620739932  0.013284945  0.232013026\n [491] -0.255931202 -0.095790087 -0.508320364 -0.141665895  0.723922334\n [496] -0.636926701 -1.449477297 -0.661271051  0.186869323  0.821822560\n [501] -0.837163730 -1.024921630  1.207414436  0.726878877 -0.660689430\n [506] -0.889485377 -1.194813019 -0.359526643  0.509891738  0.707024912\n [511] -0.013544636 -0.444908699  1.031602885  1.096056627 -0.485456669\n [516] -0.057584362  0.453525790 -0.727505336 -2.185315999  1.231121509\n [521]  0.964047501  0.424887695  2.436869580 -0.242425731 -0.011250082\n [526] -0.552009672  0.316260528 -0.452901027  0.436234551 -0.506317329\n [531] -0.168520176 -0.663066237 -0.770720101 -0.168458293 -0.433090527\n [536] -0.772225666  0.190561481 -0.990838875  0.032639397  0.307761797\n [541] -1.014060408 -1.276137870  0.883790676 -2.001811955  1.386984612\n [546]  1.540862762 -2.166706888 -1.167645101 -1.436722418  1.802804211\n [551] -2.041321807  0.257428384  1.582046068 -0.584976031  0.720280409\n [556] -1.396432385  1.340080407 -0.748197905 -0.028772414  1.636941868\n [561]  0.484717714 -1.926391198 -0.941698046  0.571945925 -1.116108702\n [566] -1.074305925 -3.311494340 -0.020932508 -0.131528926 -0.229102500\n [571] -0.089164076  0.537226362  1.266165944 -0.138812483 -1.458817809\n [576] -0.255283740 -0.539840492 -1.459999105  0.495843949 -0.808763152\n [581]  0.350674990 -1.881228063 -1.703011221  1.929174403  0.096059294\n [586] -0.351174106  1.538996433  0.996307185 -1.068869347 -1.222583946\n [591]  1.342342064 -1.093659904  1.049560189 -1.269635930 -0.686385167\n [596]  0.288395291  0.973084839 -0.417287269 -0.605438832 -0.439398010\n [601] -0.556657972 -0.621841316 -0.462283431 -1.245823125 -0.475762818\n [606]  0.487782502  0.222574480  0.093462981  0.286073274  0.942385989\n [611]  1.061249058 -1.801167336  0.274907097  1.151789077 -1.098308986\n [616]  0.974882144 -0.051279077 -1.720135318  0.157036265 -0.165266306\n [621]  0.579666167  1.927404513  0.138834433  2.171810333 -0.338374972\n [626]  1.375278070  1.160164219  0.765534758 -1.712709469 -0.318261196\n [631]  1.218474533 -0.973392171  1.638662136  0.773115566  0.615237982\n [636]  0.405109135 -1.596396227 -0.151481500  0.273913949  0.255896473\n [641]  0.846684064 -0.150320014 -0.244256527  1.097716155  1.672637901\n [646]  2.001542383 -0.493850449 -1.224521370  0.683071257  0.172509116\n [651] -0.599625670 -0.460041460 -0.283202983 -1.580196762 -0.199874822\n [656] -0.386937094 -0.339057937 -1.155608164  0.847062164  0.640217910\n [661] -0.766286693  0.171913282 -0.100838685  0.021769879 -0.227865453\n [666]  0.312692954 -0.162464126 -1.267020081  0.382706839 -0.475222272\n [671] -1.578182273 -1.483631595 -0.321721040  0.432277765  1.308062754\n [676] -0.587388426  0.983362539 -2.564275469 -0.568934492 -0.450456665\n [681]  1.290790209  1.297793117 -0.705223174 -0.647204857 -1.050610971\n [686]  0.850630812  0.710180809 -0.859808704  1.032570115  0.308263169\n [691] -0.031365254 -0.258677984 -0.077514400  0.503885097 -0.493870589\n [696] -1.401061119  0.300002718 -0.970980448  0.201666873 -0.400415559\n [701] -1.455361846 -0.482802307 -0.482880689  0.812545653  0.064105548\n [706]  1.265132148 -0.303740519  0.084274986 -0.956646337 -0.529865366\n [711]  1.006734180  1.242383658 -0.472556625 -0.962748689 -1.997792559\n [716]  0.432104504 -1.705048791 -2.297499689  1.059406421  0.171142592\n [721]  1.402259263  0.507421380 -0.426425304  0.057396187  0.038557971\n [726]  1.345134523  0.133688809 -1.149927551 -0.896782507  1.402325052\n [731]  0.851977809  0.692395279  0.903672241 -0.746761480  0.504833638\n [736]  0.838303020 -1.495548852  0.841512929  0.856537202 -0.005999098\n [741] -0.649716341  2.172843816  1.178308741 -0.716572985  0.673033379\n [746]  0.682039564 -0.341018611  0.189962083 -1.187121447  0.636574296\n [751] -2.267304800 -1.125396672  0.561794257  1.219743134  0.371996554\n [756] -0.338051916 -0.191860755 -0.330186036  1.172820106  1.606822341\n [761] -1.797304695  0.458942372 -1.259046520 -0.811655658 -2.084235888\n [766] -0.944234156 -0.083577241 -0.271644288 -1.968953102 -1.257198467\n [771]  0.961311719  1.447005488  0.098960179 -0.723805112 -1.415112271\n [776] -0.634488817 -0.578251557 -1.220773456  1.181012861  0.387027841\n [781]  0.800458283  1.006766832 -0.070464016  0.781055576 -2.221282522\n [786] -0.840614616 -0.737172027 -1.427940126 -1.597683673 -0.879898679\n [791]  0.855739780  0.466062623  1.242877226  0.868830471  0.637067976\n [796] -0.126647331  0.383976098  1.560303855 -0.473955378 -0.521567840\n [801]  0.176468205  1.200447427  0.479523506 -0.484223231  0.739643359\n [806] -1.194744331  0.292442884  0.334231846  0.119738485  1.039505817\n [811]  0.428623980 -0.961107113 -0.509098521  1.046015849 -1.239310123\n [816]  0.350293729 -0.243261209 -0.698645081 -1.059846549  0.576790780\n [821] -0.571473380  0.366344987 -0.312111028 -1.676762214  2.566705599\n [826] -0.439810993 -0.413375123 -1.559657169  0.731373006 -0.980588888\n [831]  0.457422464 -0.511999274  0.561597813 -0.588654204 -2.364997978\n [836] -1.861773023 -1.189544245  0.428084342 -0.088510337 -0.199402694\n [841] -0.739450706  1.597288039 -0.632980109 -0.558995152 -0.848244519\n [846]  0.705885586  1.243860827 -1.468619680  0.002167082  1.114569430\n [851]  0.662063053 -0.926827312  2.206062076 -0.360589536 -0.366821601\n [856]  1.622878261 -0.845527185  0.652210341 -0.954027888  0.026556611\n [861] -0.326020081  0.936462742  2.176155802  1.595010376  2.188432081\n [866]  0.953114446 -0.618531419 -0.206741243 -1.555942694  1.060909140\n [871] -0.503659782 -2.296567828  1.514642047  0.825713536  0.262872199\n [876]  1.157664260  1.749401125 -1.711300951 -0.321042451  0.331390613\n [881] -1.114662919 -0.107906397  0.451676319 -0.291368061  1.109789014\n [886] -0.773287401  1.252015839  1.292941007 -0.695501933 -0.146814375\n [891] -0.066524739  2.419552333  0.276911578 -0.348797163  0.747764557\n [896] -0.468574086  1.618758455 -0.173661300 -0.705488287 -1.741159848\n [901] -1.449653189  3.393261479 -1.629689378  0.020498369  0.564702565\n [906]  0.174045884  1.001748780  1.196374517 -0.121507488  0.297790459\n [911] -0.602081332 -2.078213948 -1.195444290 -0.464958180  0.164798457\n [916]  0.356551551 -0.639356717  0.723575706  1.684189156 -0.211349822\n [921]  0.331957430 -1.038108234 -1.488088305 -0.413685157  0.396828737\n [926]  0.708072077 -0.814559100  0.388688016  1.849740789 -1.295961827\n [931] -0.628128150 -1.332480476  0.536615504 -1.855457717  1.637138762\n [936]  1.986558819  0.902663691 -1.209944068  0.565797263  0.831320130\n [941]  0.492774181  0.529794486  0.312499680 -0.380801304 -0.479489937\n [946]  0.586033250 -2.285889611  0.654568848  1.758016746  1.591775043\n [951] -0.271264533  1.551689725  0.363090919  0.343458207  0.418475468\n [956] -0.441647432  0.153860217  0.787987335 -1.312940401  0.545392591\n [961] -1.490157568 -0.903047989 -0.175829818 -0.193661951  1.347467117\n [966]  0.983802497 -2.062583246 -0.390389042 -0.683845240  0.305214300\n [971] -0.754425737  0.197553270  0.798745209  1.171097972  1.168975322\n [976]  0.236751410 -0.074569952  0.532923004 -0.188425727 -1.192008899\n [981] -1.641106663  0.193171294 -1.979914103  1.028361475  1.775711167\n [986]  0.080100619 -0.012376401  0.529158370  0.167438773 -0.761168798\n [991] -0.585740599  0.209561423  0.015908167  0.076561838 -2.567872603\n [996] -1.301143351 -0.421746121  0.079811545 -0.233641177  2.180943808"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#random-assignment-using-the-binomial-distribution",
    "href": "content/slides/02-08-randomization.html#random-assignment-using-the-binomial-distribution",
    "title": "ADVANCED: Randomization",
    "section": "Random assignment using the Binomial Distribution",
    "text": "Random assignment using the Binomial Distribution\nRemember, we then randomly assigned them to one of two groups: A or B.\n\nI used random draws from the Binomial (read: binary or two) distribution to do this.\n\n\n\nAgain, do not worry if this does not make sense! Abandon this ship and go and do something more pleasant with your afternoon! We will come back to this later."
  },
  {
    "objectID": "content/slides/02-08-randomization.html#random-assignment-using-the-binomial-distribution-1",
    "href": "content/slides/02-08-randomization.html#random-assignment-using-the-binomial-distribution-1",
    "title": "ADVANCED: Randomization",
    "section": "Random assignment using the Binomial Distribution",
    "text": "Random assignment using the Binomial Distribution\n\nrbinom(n = 1000, size = 1, prob = 0.5)\n\n   [1] 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1\n  [38] 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0\n  [75] 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0\n [112] 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0\n [149] 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0\n [186] 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n [223] 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0\n [260] 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1\n [297] 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1\n [334] 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1\n [371] 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1\n [408] 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0\n [445] 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1\n [482] 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1\n [519] 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1\n [556] 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0\n [593] 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0\n [630] 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1\n [667] 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0\n [704] 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0\n [741] 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1\n [778] 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1\n [815] 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0\n [852] 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1\n [889] 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0\n [926] 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n [963] 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0\n[1000] 0"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#the-binomial-distribution",
    "href": "content/slides/02-08-randomization.html#the-binomial-distribution",
    "title": "ADVANCED: Randomization",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\n\nggplot() + \n  geom_bar(aes(x = rbinom(n = 1e6, size = 1, prob = 0.5))) + \n  theme_minimal()"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#assigning-our-people-with-mutate",
    "href": "content/slides/02-08-randomization.html#assigning-our-people-with-mutate",
    "title": "ADVANCED: Randomization",
    "section": "Assigning our people with mutate()",
    "text": "Assigning our people with mutate()\n\nassigned_group &lt;- group_df |&gt; \n  mutate(\n    group = rbinom(1000, 1, 0.5),\n    group = factor(group, labels = c(\"A\", \"B\"))\n  )\n\nassigned_group\n\n# A tibble: 1,000 × 5\n      id height weight eye_colour group\n   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;\n 1     1   177.   66.0 Grey       B    \n 2     2   174.   60.1 Blue       A    \n 3     3   177.   80.7 Grey       B    \n 4     4   170.   64.1 Grey       A    \n 5     5   171.   67.2 Brown      B    \n 6     6   165.   57.1 Grey       A    \n 7     7   172.   80.7 Brown      A    \n 8     8   162.   87.2 Blue       A    \n 9     9   156.   76.8 Green      A    \n10    10   167.   74.7 Blue       A    \n# ℹ 990 more rows"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#comparing-our-two-groups",
    "href": "content/slides/02-08-randomization.html#comparing-our-two-groups",
    "title": "ADVANCED: Randomization",
    "section": "Comparing our two groups",
    "text": "Comparing our two groups"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#comparing-our-two-groups-1",
    "href": "content/slides/02-08-randomization.html#comparing-our-two-groups-1",
    "title": "ADVANCED: Randomization",
    "section": "Comparing our two groups",
    "text": "Comparing our two groups"
  },
  {
    "objectID": "content/slides/02-08-randomization.html#comparing-our-two-groups-2",
    "href": "content/slides/02-08-randomization.html#comparing-our-two-groups-2",
    "title": "ADVANCED: Randomization",
    "section": "Comparing our two groups",
    "text": "Comparing our two groups"
  },
  {
    "objectID": "content/slides/02-03-arrange.html#arrange-rows-with-arrange",
    "href": "content/slides/02-03-arrange.html#arrange-rows-with-arrange",
    "title": "Sorting Your Data",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\narrange(gapminder, country, year)\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-03-arrange.html#arrange-rows-with-arrange-1",
    "href": "content/slides/02-03-arrange.html#arrange-rows-with-arrange-1",
    "title": "Sorting Your Data",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\narrange(gapminder, country, desc(year))\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2007    43.8 31889923      975.\n 2 Afghanistan Asia       2002    42.1 25268405      727.\n 3 Afghanistan Asia       1997    41.8 22227415      635.\n 4 Afghanistan Asia       1992    41.7 16317921      649.\n 5 Afghanistan Asia       1987    40.8 13867957      852.\n 6 Afghanistan Asia       1982    39.9 12881816      978.\n 7 Afghanistan Asia       1977    38.4 14880372      786.\n 8 Afghanistan Asia       1972    36.1 13079460      740.\n 9 Afghanistan Asia       1967    34.0 11537966      836.\n10 Afghanistan Asia       1962    32.0 10267083      853.\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-03-arrange.html#get-the-smallest-or-largest-value-in-one-line-of-code",
    "href": "content/slides/02-03-arrange.html#get-the-smallest-or-largest-value-in-one-line-of-code",
    "title": "Sorting Your Data",
    "section": "Get the smallest or largest value in one line of code",
    "text": "Get the smallest or largest value in one line of code\n\nslice_min(gapminder, lifeExp)\n\n# A tibble: 1 × 6\n  country continent  year lifeExp     pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n1 Rwanda  Africa     1992    23.6 7290203      737.\n\n\n\n\nslice_max(gapminder, lifeExp)\n\n# A tibble: 1 × 6\n  country continent  year lifeExp       pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n1 Japan   Asia       2007    82.6 127467972    31656."
  },
  {
    "objectID": "content/slides/02-06-summarise.html#create-summaries-with-summarise",
    "href": "content/slides/02-06-summarise.html#create-summaries-with-summarise",
    "title": "Summarising your data",
    "section": "Create summaries with summarise()",
    "text": "Create summaries with summarise()\n\nsummarise(\n  gapminder, avg_pop = mean(pop), avg_gdp_per_cap = mean(gdpPercap)\n)\n\n# A tibble: 1 × 2\n    avg_pop avg_gdp_per_cap\n      &lt;dbl&gt;           &lt;dbl&gt;\n1 29601212.           7215."
  },
  {
    "objectID": "content/slides/02-06-summarise.html#create-summaries-with-summarise-1",
    "href": "content/slides/02-06-summarise.html#create-summaries-with-summarise-1",
    "title": "Summarising your data",
    "section": "Create summaries with summarise()",
    "text": "Create summaries with summarise()\n\nsummarise(\n  gapminder, \n  avg_pop = mean(pop), \n  median_pop = median(pop), \n  avg_gdp_per_cap = mean(gdpPercap),\n  median_gdp_per_cap = median(gdpPercap)\n)\n\n# A tibble: 1 × 4\n    avg_pop median_pop avg_gdp_per_cap median_gdp_per_cap\n      &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;\n1 29601212.   7023596.           7215.              3532."
  },
  {
    "objectID": "content/slides/02-06-summarise.html#creating-grouped-summaries-with-group_by-and-summarise",
    "href": "content/slides/02-06-summarise.html#creating-grouped-summaries-with-group_by-and-summarise",
    "title": "Summarising your data",
    "section": "Creating grouped summaries with group_by() and summarise()",
    "text": "Creating grouped summaries with group_by() and summarise()\n\ngapminder_continent &lt;- group_by(gapminder, continent)\n\nsummarise(\n  gapminder_continent, \n  avg_pop = mean(pop), \n  avg_gdp_per_cap = mean(gdpPercap)\n)\n\n# A tibble: 5 × 3\n  continent   avg_pop avg_gdp_per_cap\n  &lt;fct&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1 Africa     9916003.           2194.\n2 Americas  24504795.           7136.\n3 Asia      77038722.           7902.\n4 Europe    17169765.          14469.\n5 Oceania    8874672.          18622."
  },
  {
    "objectID": "content/00-introduction.html",
    "href": "content/00-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to GVPT399F: Power, Politics, and Data. This hands-on introduction to political science data analysis will (re)introduce you to R and statistics in an accessible and engaging way. We will avoid abstractions and jargon, opting instead for a hands-on, ground-up, and simulation-focused approach to understanding how these concepts and skills work. By the end of this winter, you’ll not only gain confidence in your ability to analyze and interpret data but also build in-demand skills valued in careers across government, international organizations, think tanks, and the private sector. No prior experience in statistics or coding (and definitely no ancient Greek) is needed—just bring your curiosity!",
    "crumbs": [
      "Content",
      "Session 0",
      "Introduction"
    ]
  },
  {
    "objectID": "content/00-introduction.html#what-this-course-covers",
    "href": "content/00-introduction.html#what-this-course-covers",
    "title": "Introduction",
    "section": "What this course covers",
    "text": "What this course covers\nAfter successfully completing this course you will be able to:\n\nUse R to collect, clean, and analyze data\nDescribe important features of your outcomes of interest and the variables you think drive changes to those outcomes\nIdentify and evaluate the relationship between two variables using statistical models\nDescribe those relationships using clear and precise language\nCritically evaluate empirical claims made in political news and analysis.",
    "crumbs": [
      "Content",
      "Session 0",
      "Introduction"
    ]
  },
  {
    "objectID": "content/00-introduction.html#course-structure",
    "href": "content/00-introduction.html#course-structure",
    "title": "Introduction",
    "section": "Course structure",
    "text": "Course structure\nThe course comprises six substantive sessions, this introductory session, and a session to conclude. You should complete these in order: each builds on the previous sessions.\nEach session includes some written content and a series of short recorded lectures. Each ends with a mandatory multiple-choice quiz (taken through ELMs). You will be introduced to new R code and statistical concepts in each session.\nThree substantive sessions will be released each week. You should complete them within that week. The quizzes for all three sessions released that week will be due at 11:59PM on Friday nights.\nYou will also complete a final exam at the end of the course. All material covered in all eight sessions could be tested.\nQuiz and exam schedule\n\n\n\n\n\n\n\n\nWEEK\nASSIGNMENT\nRELEASE DATE\nDUE DATE\n\n\n\n1\n0: Introduction quiz\n2 January 2025\n7 January 2025, 11:59PM\n\n\n2\n\n1: Session 1 quiz\n2: Session 2 quiz\n3: Session 3 quiz\n\n6 January 2025\n10 January 2025, 11:59PM\n\n\n3\n\n4: Session 4 quiz\n5: Session 5 quiz\n6: Session 6 quiz\n\n13 January 2025\n17 January 2025, 11:59PM\n\n\n4\n7: Final exam\n18 January 2025\n22 January 2025, 11:59PM\n\n\n\nPlease note: weeks one and four are short.",
    "crumbs": [
      "Content",
      "Session 0",
      "Introduction"
    ]
  },
  {
    "objectID": "content/00-introduction.html#course-resources",
    "href": "content/00-introduction.html#course-resources",
    "title": "Introduction",
    "section": "Course resources",
    "text": "Course resources\nEach session includes several resources written just for you. These can all be accessed from this website. They include:\n\na series of recorded lectures\ncorresponding slides with embedded R code\nsome written content.\n\nThere are no additional required readings for this course. Along the way, I may suggest other resources - book chapters, blog posts, videos, etc. - that you might find helpful. Sometimes it helps to hear these concepts explained in a couple of different ways.\nAdditional required resources\nTo complete this course, you need access to a personal computer and a stable internet connection. You also need to have access to the latest versions of R and RStudio. Here are detailed instructions on how to download or update these two free resources:\n Instructions on downloading R\n Instructions on downloading RStudio\nEverything you need to successfully complete this course is available for free.",
    "crumbs": [
      "Content",
      "Session 0",
      "Introduction"
    ]
  },
  {
    "objectID": "content/00-introduction.html#introduction-to-r-and-rstudio",
    "href": "content/00-introduction.html#introduction-to-r-and-rstudio",
    "title": "Introduction",
    "section": "Introduction to R and RStudio",
    "text": "Introduction to R and RStudio\nI will now introduce you to two tools you will use to successfully complete this course: R and RStudio. R is a versatile programming language that excels in statistical analysis. It is widely used by academics and in the private, government, and international sectors. You will certainly get a lot of use out of it going forward!\nR is also a very flexible language. You can make all kinds of very cool things with R, including websites, apps, slides, and more. In fact, all the resources I produced for this course were made using R, including this website and fancy slides.\nAnother advantage R has over other statistical programming languages is its accessibility. It is entirely free to use. There are many resources out there that will introduce you to its many uses. There is also an enthusiastic and welcoming community of R users who continue to grow R itself and the various resources you might need to expand your skills.\nR and RStudio\nSo, what is the difference between R and RStudio? R is the statistical programming language. RStudio is the platform, or integrated development environment, you will use to work with R. RStudio is free and used widely by R users.\nThe R skills you will learn\nThis course aims to provide you with two broad skills: statistical analysis and R. I will now outline what you will learn in relation to R.\nYou will learn how to import your data into R. You will learn how to load data stored in an external file, database, or online into a data frame in R.\nYou will then be introduced to methods for cleaning up those data. Oftentimes, data comes to us in a messy format, with missingness, and inconsistencies. You will need to tidy it up into a format that is easy to work with and consistent.\nOnce you have tidy data, you will then need to transform it so that it is ready for your analysis. This includes focusing your data on the observations you are interested in and creating new variables.\nNext, we will focus on visualizing your data. You can learn a lot more about your data and relationships lurking within it from a plot than you can from looking at the raw numbers.\nWe will also spend a fair chunk of time learning how to model those relationships within our data. Alongside visualization, this is where R excels.\nFinally, I will also introduce you to tools for communicating your findings in an engaging and replicable way.",
    "crumbs": [
      "Content",
      "Session 0",
      "Introduction"
    ]
  },
  {
    "objectID": "content/00-introduction.html#a-tour-of-rstudio",
    "href": "content/00-introduction.html#a-tour-of-rstudio",
    "title": "Introduction",
    "section": "A tour of RStudio",
    "text": "A tour of RStudio\n\n\nExercises\nUsing the console, find the summation of 45, 978, and 121.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nsum(45, 978, 121)\n\n[1] 1144\n\n\nOr:\n\n45 + 978 + 121\n\n[1] 1144\n\n\n\n\n\nWhat is 67 divided by 6?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\n67 / 6\n\n[1] 11.16667\n\n\n\n\n\nWhat is the square root of 894? Hint: use the sqrt() function.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nsqrt(894)\n\n[1] 29.89983",
    "crumbs": [
      "Content",
      "Session 0",
      "Introduction"
    ]
  },
  {
    "objectID": "content/00-introduction.html#quiz",
    "href": "content/00-introduction.html#quiz",
    "title": "Introduction",
    "section": "Quiz",
    "text": "Quiz\nHead over to ELMs to complete this session’s mandatory multiple-choice quiz.",
    "crumbs": [
      "Content",
      "Session 0",
      "Introduction"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html",
    "href": "content/05-linear_regression.html",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "",
    "text": "We want to understand what factors make certain outcomes more likely to occur. What makes an individual more likely to vote in an election? What makes a country on the brink of democracy more likely to slide back into authoritarianism? What makes war between two countries more likely to break out?\nWe can use empirical research to help us discover what characteristics or features of our political actors or the environment in which they operate make these outcomes more likely to occur. Does election day registration increase the likelihood an individual will vote in an election? Does a free press make democratic backsliding less likely to occur? Do trade links reduce the likelihood two countries go to war with one another?\nMany of the questions we ask cannot be answered using experiments. We are left, instead, to look back at the history of our outcome of interest (for example, global conflicts, democratic backsliding, elections), and attempt to tease out the role various factors played in shaping those outcomes. This section focuses on developing your ability to do this.",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#set-up",
    "href": "content/05-linear_regression.html#set-up",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "Set up",
    "text": "Set up\nTo complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"marginaleffects\", \"janitor\", \"ggdist\"))\n\nYou will also need to install an R package I built: polisciols. This package contains data published by some wonderful political scientists. You can read more about it in the package documentation.\nThis package is not published on CRAN1, so you will need to install it using the following code:\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"hgoers/polisciols\")\n\nRemember, you only need to do this once on your computer. Run this in the console.\n\n\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(janitor)\nlibrary(ggdist)\nlibrary(polisciols)",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#the-economic-benefits-of-justice",
    "href": "content/05-linear_regression.html#the-economic-benefits-of-justice",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "The economic benefits of justice",
    "text": "The economic benefits of justice\nAppel and Loyle (2012) (two wonderful UMD alumni) explore the determinants of foreign direct investment (FDI) flows into and out of post-conflict states. States that are emerging from civil war often have an acute need for foreign and stable sources of capital. However, multinational corporations and other foreign commercial actors are likely to view post-conflict states as high risk countries in which to invest: the risk of a return to violence and instability is often high in the immediate aftermath of a civil war. Understanding this, leaders of post-conflict states often attempt to decrease this perceived risk.\nAppel and Loyle argue that leaders can successfully do this by establishing post-conflict justice (PCJ) institutions. These institutions impose both domestic and reputational costs on post-conflict leaders. These costs allow leaders to signal their commitment to minimizing the risk of a return to violence and instability to foreign commercial actors. This is a great paper and I highly encourage you to take a look at their argument in detail.\nTheir argument focuses on leaders’ attempts to change foreign commercial actors’ perceptions of the risk of a return to violence. We cannot directly observe these commercial actors’ perceptions. However, we can observe the outcome of these perceptions: investment. Firms that have the means and desire to invest in a post-conflict country will make an assessment of the risk to their investment before committing the capital required to set up shop. If they believe the country is highly likely to return to conflict, they will not invest in it. If, on the other hand, they believe the risk of recidivism is low, they will invest.\nIf Appel and Loyle’s argument is correct, we should observe higher levels of FDI investment coming into post-conflict states that establish PCJ institutions compared to those that do not. PCJ institutions reduce the risk of a return to violence, which should - in turn - reduce firms’ beliefs about this risk. Firms invest when they believe the risk of a return to conflict to be low.\nAppel and Loyle find strong evidence of this occuring. We will replicate and modify this empirical work. In doing so, we will become more familiar with the underlying mechanics of linear regression and strengthen our ability to interpret these models.\nLet’s get started!",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#net-fdi-inflows-to-post-conflict-states",
    "href": "content/05-linear_regression.html#net-fdi-inflows-to-post-conflict-states",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "Net FDI inflows to post-conflict states",
    "text": "Net FDI inflows to post-conflict states\nAppel and Loyle provide us with data on 95 different post-conflict states. These include all states that had internal armed conflicts, including internationalized conflicts, that resulted in at least 25 battle-related deaths and were settled between 1970 and 2001.\nWe can access these data through polisciols::ebj:\n\nhead(ebj)\n\n   id ccode      country_name              pcj net_fdi_inflows gdp_per_capita\n1  71    41             Haiti  No institutions         -9.8000       1182.498\n2  71    41             Haiti  No institutions          6.6000       1088.680\n3 154    52 Trinidad & Tobago  No institutions        510.1589       7742.736\n4 102    70            Mexico  No institutions       -340.6904       6894.704\n5 102    70            Mexico  No institutions       6460.7998       7780.053\n6  67    90         Guatemala PCJ institutions        431.3800       3061.873\n           gdp gdp_per_capita_growth ex_rate_fluc cap_account_openness labor\n1   8407079981            -2.1324685     0.000000           -0.7681904  68.5\n2   8055393763           -14.8832922     1.776603           -0.0871520  67.9\n3   9542938026             1.9370972     0.000000           -1.1305820  55.2\n4 628418000000            -7.8634830     1.978028            1.1804080  59.8\n5 730752000000             5.2345648     1.129686            1.1804080  61.3\n6  31339424077             0.6277104     1.046489            1.2642760  63.1\n  f_life_exp polity2 pol_constraints conflict_duration     damage\n1   55.02233       7            0.00                 1   0.000000\n2   56.09750      -7            0.00                 1  12.855902\n3   72.61483       9            0.84                 1  -2.787351\n4   74.67104       4            0.39                 1   0.000000\n5   75.23493       6            0.39                 1  -8.949999\n6   67.46067       8            0.43                31 -43.714600\n  peace_agreement    victory      cold_war\n1    No agreement    Victory Post-Cold War\n2    No agreement    Victory Post-Cold War\n3    No agreement    Victory Post-Cold War\n4 Peace agreement No victory Post-Cold War\n5    No agreement No victory Post-Cold War\n6 Peace agreement No victory Post-Cold War\n\n\nThey focus on the 10-year period immediately following the conflict’s conclusion. This is the period in which we would expect foreign commercial actors to perceive the risk of a return to violence and instability to be greatest and, therefore, the period in which leaders’ attempts to quash these perceptions to be most relevant.\nEach row in this data set represents a single post-conflict state. The data provided are a summary of the 10-years post-conflict. For example, net_fdi_inflows provides us with the total net FDI inflows (dollar amount invested in the country minus the dollar amount divested from the country) each post-conflict state received across the whole 10-year period.\n\n\n\n\n\n\nTip\n\n\n\nYou can learn more about each variable using the following command:\n\n?ebj\n\n\n\nTheir outcome of interest is net FDI inflows. Remember, if their argument is correct we would expect post-conflict states that have established a PCJ institution to have higher net FDI inflows than than states that did not. Let’s take a look at those net inflows:\n\nsummary(ebj$net_fdi_inflows)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1858.914    -0.572    38.290   759.146   408.250 24836.787 \n\n\n\nggplot(ebj, aes(x = net_fdi_inflows)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(x = \"Net FDI inflows\",\n       y = \"Count\") + \n  scale_x_continuous(labels = scales::dollar)\n\n\n\n\n\n\n\nOn average, post-conflict countries received $759.15 million in net FDI inflow. So, countries in our sample tended to receive more investment than they lost in the 10-year period following the end of their conflict.\nLooking at the histogram above we can see that this average is not a particularly good summary of our data: the majority of countries received a smaller amount of net FDI inflows. The peak the distribution is sitting closer to $0. Looking at the five number summary printed above the histogram we can see that 50 percent of the countries in our sample received between -$0.57 and $408.25 million in net inflows.\n\n\n\n\n\n\nNote\n\n\n\nThe middle 50 percent of a vector of numbers sits between the first quartile (below which 25 percent of those numbers fall) and the third quartile (below which 75 percent of those numbers fall). To illustrate, consider the following:\n\nx &lt;- seq(0, 100)\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0      25      50      50      75     100 \n\n\n\n\nWhy is our average sitting so far above where the majority of the countries’ net FDI inflows fell? There appears to be a clear outlier in our data: Russia received a net FDI inflow of $24,836.79 million in the 10 years following the end of the First Chechen War in 1997. You can see that this is pulling the average net FDI inflows up well above the median inflow across our group of post-conflict states. Keep this in mind, because we will return to it later in the course.",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#pcj-institutions",
    "href": "content/05-linear_regression.html#pcj-institutions",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "PCJ institutions",
    "text": "PCJ institutions\nAppel and Loyle ask whether states that have PCJ institutions successfully attract greater net FDI inflows than those that do not. Therefore, their main explanatory variable is the existence of PCJ institutions.\nIn their dataset the variable pcj indicates whether the the state established a PCJ institution within five years following the end of the conflict. What proportion of states did this?\n\ntabyl(ebj, pcj)\n\n              pcj  n   percent\n  No institutions 77 0.8105263\n PCJ institutions 18 0.1894737\n\n\nThe majority of post-conflict states did not establish any PCJ institutions in the five years proceeding the end of their conflicts.",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#relationship-between-pcj-institutions-and-net-fdi-inflows",
    "href": "content/05-linear_regression.html#relationship-between-pcj-institutions-and-net-fdi-inflows",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "Relationship between PCJ institutions and net FDI inflows",
    "text": "Relationship between PCJ institutions and net FDI inflows\nOf the states that do have a PCJ institution, do they tend to receive higher net FDI inflows than their less reconciliatory counterparts?\n\nebj |&gt; \n  group_by(pcj) |&gt; \n  summarise(avg_net_fdi = mean(net_fdi_inflows))\n\n# A tibble: 2 × 2\n  pcj              avg_net_fdi\n  &lt;fct&gt;                  &lt;dbl&gt;\n1 No institutions         425.\n2 PCJ institutions       2189.\n\n\nYes! On average, states that established a PCJ institution received higher net FDI inflows than those states that did not. This difference looks large. It’s $1,763.52 million in net inflows.\nReady for something very cool? At the end of the day, OLS regression is just fancy averaging. Let’s fit a linear regression model to these data and look at the results:\n\nm &lt;- lm(net_fdi_inflows ~ pcj, data = ebj)\n\nmodelsummary(m,  \n             statistic = NULL, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\"))\n\n\n\n    \n\n      \n\n \n                (1)\n              \n\n\n(Intercept)\n                  425.006\n                \n\nPCJ institutions established\n                  1763.517\n                \n\nNum.Obs.\n                  95\n                \n\nR2\n                  0.057\n                \n\nR2 Adj.\n                  0.047\n                \n\nAIC\n                  1784.5\n                \n\nBIC\n                  1792.2\n                \n\nLog.Lik.\n                  -889.253\n                \n\nRMSE\n                  2811.91\n                \n\n\n\n\n\n\nWe get the same numbers as above! Take a minute to interpret the intercept and coefficient estimate yourself. Can you see the connection to our simple grouped averages above?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\nThe intercept is the predicted value of our outcome variable when all independent variables are equal to zero. Here, we have one independent variable which is equal to zero when the country did not establish PCJ institutions. Our model predicts that post-conflict countries without PCJ institutions will have the net FDI inflow that post-conflict countries in our sample without PCJ institutions had on average.\nThe coefficient estimate is the predicted average difference in the outcome following a one-unit change in the independent variable. Here, a one-unit change in our independent variable is a move from no PCJ institutions (where pcj = 0) to at least one PCJ institution (where pcj = 1). Our model predicts that post-conflict countries with PCJ institutions will have on average $1,763.52 million more in net FDI inflows because this is the difference between the average net inflows of countries in our sample without PCJ institutions and those with them.\n\n\n\nAccording to this model, what is the average predicted net FDI inflows for countries that did not establish PCJ institution?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPost-conflict states that did not establish a PCJ institution received, on average, net FDI inflows of $425.01 million in the 10-year period after conflict.\nTo get this, I looked at the intercept coefficient which tells us the average predicted value of our outcome variable when all explanatory variables are equal to zero or their baseline category.\n\n\n\nAccording to this model, what is the average predicted net FDI inflows for countries that did establish PCJ institution?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn contrast, states that did establish PCJ institution received, on average, net FDI inflows of $2,188.52 million.\nIn other words:\n\\[\nAverage\\ net\\ FDI\\ inflows = \\beta_0 + \\beta_1 Institution\\ established + \\epsilon\n\\]\nWhen an institution was established (i.e. \\(Institution\\ established = 1\\)):\n\\[\nAverage\\ net\\ FDI\\ inflows = 425.01 + 1763.52 * 1 + \\epsilon \\\\\n\\]\n\\[\nAverage\\ net\\ FDI\\ inflows = 2188.52\n\\]\n\n\n\nSubstantive significance\nSo, we found support for Appel and Loyal’s hypothesis: countries that established PCJ institutions within the first five-years after conflict received more net FDI inflows than those that did not establish these institutions. But, was this difference large? These institutions impose real costs on post-conflict leaders. Is the predicted boost in net FDI inflows generally worth this cost? More generally, is this difference substantively significant?\n\n\n\n\n\n\nNote\n\n\n\nSubstantive significance is different from statistical significance, which we will discuss in detail in our last session. Here, we are interested in whether the difference is large enough for our political actors to care about it.\nTo illustrate, imagine if I told you that studying this course material for an additional five hours would lead to a one-point increase in your quiz mark. I’m not sure many of you would put in that additional work for such a small reward. If, on the other hand, I told you that it would boost your mark by 15 points, you might be more willing to spend your afternoon with these materials.\nThere is no one right or wrong answer to the question of substantive significance. For example, perhaps someone who is one point away from a perfect grade would be willing to put in the extra hours to secure that one additional mark. Substantive significance rests much more on your argument for it, rather than a hard-and-fast threshold that must be reached (which is the case for statistical significance).\n\n\nI argue that this finding is substantively significant. These countries are recovering from conflict: their economies are generally very weak. Leaders are often very keen to find stable and reliable sources of funding to promote and strengthen their battered economies. This average difference of $1,763.52 million is; therefore, likely to incentivize this policy.",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#but-what-about-other-factors-that-shape-net-fdi-inflows",
    "href": "content/05-linear_regression.html#but-what-about-other-factors-that-shape-net-fdi-inflows",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "But what about other factors that shape net FDI inflows?",
    "text": "But what about other factors that shape net FDI inflows?\nOne of Appel and Loyle’s major contributions is their critique of approaches to estimating net FDI inflows that focus only on economic factors. They argue that there are several political factors that are significant determinants of other countries’ and foreign firms’ willingness to invest in these war-torn countries.\nThis critique is very valid, but it suggests that there are many different things influencing this outcome of interest, including economic factors. We have only looked at the political! The economists might turn around and accuse us of doing the very thing Appel and Loyle accused them of!\nLet’s add some of those economic factors into our model. We will start with an intuitive one: individuals’ economic wealth (measured as GDP per capita). I expect that foreign firms will be more willing to invest larger sums of money into economies with richer citizens. These citizens will be more willing and able to purchase the goods and services provided by those firms.\nTherefore, I hypothesize that the greater a state’s GDP per capita, the larger its net FDI inflows. I expect this to be the case regardless of whether the state has established a PCJ institution. Let’s test this claim:\n\nm &lt;- lm(net_fdi_inflows ~ pcj + gdp_per_capita, data = ebj)\n\nmodelsummary(m, \n             statistic = NULL, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\",\n                             \"gdp_per_capita\" = \"GDP per capita (current USD)\"))\n\n\n\n    \n\n      \n\n \n                (1)\n              \n\n\n(Intercept)\n                  -319.173\n                \n\nPCJ institutions established\n                  1673.288\n                \n\nGDP per capita (current USD)\n                  0.318\n                \n\nNum.Obs.\n                  95\n                \n\nR2\n                  0.142\n                \n\nR2 Adj.\n                  0.124\n                \n\nAIC\n                  1777.5\n                \n\nBIC\n                  1787.7\n                \n\nLog.Lik.\n                  -884.742\n                \n\nRMSE\n                  2681.52\n                \n\n\n\n\n\n\nWe continue to find a positive relationship between net FDI inflows and the establishment of a PCJ institution. In other words, countries that established PCJ institutions received, on average, larger net FDI inflows than those that did not establish these institutions. Additionally, we can now state that this is the case regardless of how rich or poor individuals within the country are.\nWe do this by holding constant the other variables in the model. This can be a little tricky to wrap your head around, so please be patient with yourselves. When we introduce multiple variables into our linear regression models we move from a line of best fit to a plane of best fit. You add an additional dimension to your plane for each additional variable you include in your model. To illustrate, look at the model we just fit (which has two variables) plotted against the observed values for whether the country had a PCJ institution, its GDP per capita, and the net FDI inflow it received:\n\nShow the code# You can produce interactive graphs in R using `plotly`. This is a bit beyond \n# the R code you have been introduced to thus far, but if you are interested in \n# learning more check out the `plotly` documentation.\n\nlibrary(plotly)\n\n# Get all plotted points for PCJ\npoints_pcj &lt;- ebj |&gt; \n  distinct(pcj) |&gt;\n  pull()\n\n# Get all plotted points for GDP per capita\npoints_gdp &lt;- seq(min(ebj$gdp_per_capita, na.rm = T),\n                  max(ebj$gdp_per_capita, na.rm = T),\n                  by = 1)\n\n# Get the predicted values for net FDI inflows from the model\nnew_df &lt;- crossing(\n  gdp_per_capita = points_gdp,\n  pcj = points_pcj\n)\n\npred_values &lt;- augment(m, newdata = new_df) |&gt; \n  pull(.fitted)\n\npred_values_matrix &lt;- matrix(pred_values, nrow = length(points_pcj), ncol = length(points_gdp))\n\n# Plot the plane\nplot_ly() |&gt; \n  add_surface(x = points_gdp, \n              y = points_pcj,\n              z = pred_values_matrix,\n              colors = \"pink\") |&gt; \n  add_markers(x = ebj$gdp_per_capita, \n              y = ebj$pcj,\n              z = ebj$net_fdi_inflows,\n              type = \"scatter3d\",\n              alpha = 0.75) |&gt; \n  layout(showlegend = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis graph is interactive. Have a play around with it.\n\n\nOur model is no longer the line of best fit. It is now the linear plane of best fit. For any given value of GDP per capita and whether the country has PCJ institutions or not, we have a predicted value of net FDI inflows. This predicted value is the value that minimizes the distance between itself and all the values of both GDP per capita and whether the country has a PCJ institution or not.\nInterpreting multiple linear regression models\nThe intercept here is not informative on its own. It tells us the estimated average net FDI inflows for countries that do not have PCJ institutions (pcj = 0) and in which citizens had a GDP per capita of $0 (gdp_per_capita = 0). Although the majority of countries did not establish a PCJ institution, there are no countries in the world that have a GDP per capita of $0.\nLet’s instead focus on the other coefficients. We find that states that established a PCJ institution received, on average, net FDI inflows of $1,673.29 million more than states that did not, holding that country’s GDP per capita constant. I have picked one value of GDP per capita and then calculated the difference between the predicted value of net FDI inflows for a countries with that GDP per capita and without PCJ institutions compared to that for a country with that GDP per capita and with PCJ institutions. The value of GDP per capita that you pick does not matter because this is a linear plane.\n\n\n\n\n\n\nNote\n\n\n\nTo illustrate, say I pick a GDP per capita of $0. The predicted net FDI inflow for a country without a PCJ institution is therefore:\n\\[\nNet\\ inflow = -319 + \\beta_{PCJ}(0) + \\beta_{GDP\\ per\\ cap}(0) \\\\\nNet\\ inflow = -319 + 1673(0) + 0.318(0) \\\\\nNet\\ inflow = -319\n\\]\nThe predicted net FDI inflow for a country with a PCJ institution (and a GDP per capita of $0) is:\n\\[\nNet\\ inflow = -319 + \\beta_{PCJ}(1) + \\beta_{GDP\\ per\\ cap}(0) \\\\\nNet\\ inflow = -319 + 1673(1) + 0.318(0)\nNet\\ inflow = 1354\n\\]\nThe difference is $1673 (the coefficient for the PCJ variable).\nI can do the same calculations for a country with a GDP per capita of $25,000. Here is the predicted net FDI inflow for a country without PCJ institutions:\n\\[\nNet\\ inflow = -319 + 1673(0) + 0.318(25000) \\\\\nNet\\ inflow = 7631\n\\]\nAnd for a country with PCJ institutions:\n\\[\nNet\\ inflow = -319 + 1673(1) + 0.318(25000) \\\\\nNet\\ inflow = 9304\n\\]\nAgain, the difference is $1673 (the coefficient for the PCJ variable).\n\n\nWe also find that an increase in the GDP per capita of a state of $1,000 is associated with an increase of $318.45 million in net FDI inflow, on average and holding whether they have PCJ institutions constant. This is consistent with our expectations that, holding all else constant, a country with a richer population is a more attractive investment destination than a country that has poorer citizens.\nUsing this richer model\nWe now have a richer understanding of the determinants of net FDI inflows to post-conflict countries. We have accounted for both economic and political determinants of those flows. Although it is often useful to look at the estimated relationship of each of those variables individually (as we did just above), we often learn more by looking at the whole model in context.\nAs usual, one of the easiest ways to communicate this is through a visualization. Let’s look at the predicted net FDI inflows for post-conflict countries that established and did not establish PCJ institutions across a range of plausible GDP per capita values:\n\nplot_predictions(m, condition = c(\"gdp_per_capita\", \"pcj\")) + \n  labs(x = \"GDP per capita (USD)\",\n       y = \"Net FDI inflows (USD million)\",\n       color = \"PCJ\",\n       fill = \"PCJ\") + \n  scale_y_continuous(labels = scales::dollar) + \n  scale_x_continuous(labels = scales::dollar) + \n  theme_minimal()\n\n\n\n\n\n\n\nWe can see that countries with PCJ institutions (represented by the blue line) are predicted to have more net FDI inflows than countries without these institutions (represented by the red line) across all values of GDP per capita. Because the model is linear, the distance between the red and blue lines is constant across all values of GDP per capita.\n\n\n\n\n\n\nNote\n\n\n\nThe coloured bands around the line showing the predicted values of net FDI inflows are the confidence intervals. We will discuss these more in later sessions.\n\n\nThe full model\nAppel and Loyle control for many more economic and political factors shaping net FDI inflows. Remember, they are arguing that the extensive literature that looks at the determinants of FDI flows to post-conflict states failed to account for this important political factor. However, that same literature did a very good job of identifying the economic factors, including countries’ economic development, size, and growth rates, that shape this outcome. They don’t dispute that these factors are also important, they just argue that we should also think about the role PCJ institutions play in shaping foreign firms’ beliefs about the risk of returning to violence.\nSo, let’s account for these other factors:\n\nm &lt;- lm(net_fdi_inflows ~ pcj + gdp_per_capita + gdp + gdp_per_capita_growth + \n          cap_account_openness + ex_rate_fluc + labor + f_life_exp + \n          pol_constraints + polity2 + damage + conflict_duration + peace_agreement + \n          victory + cold_war, \n        data = ebj)\n\nmodelsummary(m, \n             statistic = NULL, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\",\n                             \"gdp_per_capita\" = \"GDP per capita (current USD)\",\n                             \"gdp\" = \"GDP (current USD)\",\n                             \"gdp_per_capita_growth\" = \"GDP per capita growth rate (%)\",\n                             \"cap_account_openness\" = \"Capital account openness\",\n                             \"ex_rate_fluc\" = \"Exchange rate fluctuation\",\n                             \"labor\" = \"Labor force participation (%)\",\n                             \"f_life_exp\" = \"Average female life expectancy (years)\",\n                             \"pol_constraints\" = \"Political constraints\",\n                             \"polity2\" = \"Regime type (Polity score)\",\n                             \"damage\" = \"Pre-conflict GDP lost\",\n                             \"conflict_duration\" = \"Conflict duration (years)\",\n                             \"peace_agreementPeace agreement\" = \"Peace agreement\",\n                             \"victoryVictory\" = \"Decisive victory\",\n                             \"cold_warCold War\" = \"Cold War\"))\n\n\n\n    \n\n      \n\n \n                (1)\n              \n\n\n(Intercept)\n                  -1278.322\n                \n\nPCJ institutions established\n                  1960.282\n                \n\nGDP per capita (current USD)\n                  -0.111\n                \n\nGDP (current USD)\n                  0.000\n                \n\nGDP per capita growth rate (%)\n                  37.400\n                \n\nCapital account openness\n                  198.823\n                \n\nExchange rate fluctuation\n                  -42.516\n                \n\nLabor force participation (%)\n                  9.844\n                \n\nAverage female life expectancy (years)\n                  3.475\n                \n\nPolitical constraints\n                  2557.954\n                \n\nRegime type (Polity score)\n                  -90.169\n                \n\nPre-conflict GDP lost\n                  28.379\n                \n\nConflict duration (years)\n                  0.811\n                \n\nPeace agreement\n                  -1215.137\n                \n\nDecisive victory\n                  -33.969\n                \n\nCold War\n                  81.531\n                \n\nNum.Obs.\n                  95\n                \n\nR2\n                  0.514\n                \n\nR2 Adj.\n                  0.422\n                \n\nAIC\n                  1749.5\n                \n\nBIC\n                  1792.9\n                \n\nLog.Lik.\n                  -857.752\n                \n\nRMSE\n                  2018.34\n                \n\n\n\n\n\n\nEven when we account for all the political, economic, and conflict-related factors that the literature previously identified to be important, we still find that the existence of PCJ institution shapes the net FDI inflows of post-conflict states.",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#including-categorical-variables-with-multiple-categories",
    "href": "content/05-linear_regression.html#including-categorical-variables-with-multiple-categories",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "Including categorical variables with multiple categories",
    "text": "Including categorical variables with multiple categories\nAppel and Loyle look at a number of political factors driving net FDI inflows to post-conflict states. They include a measure of regime type: the country’s Polity score. This score measures a state’s regime type along a 21-point scale from -10 (perfect autocracy) to 10 (perfect democracy). Broadly speaking, political scientists have usefully broken this spectrum down into three regime types: democracies, hybrid regimes, and autocracies.\nLet’s modify their measure of regime type to reflect these broad categories, instead of treating it as a continuous variable:\n\nebj &lt;- ebj |&gt; \n  mutate(regime_type = case_when(polity2 &gt; 5 ~ \"Democracy\",\n                                 polity2 &lt; -5 ~ \"Autocracy\",\n                                 TRUE ~ \"Hybrid regime\"),\n         regime_type = factor(regime_type, levels = c(\"Autocracy\",\n                                                      \"Hybrid regime\",\n                                                      \"Democracy\")))\n\nI have a theoretical reason to do this. I suspect that there is not a clear linear relationship between investors’ confidence in a post-conflict state and its regime type when we treat regime type as a continuous spectrum moving linearly from autocracies to democracies. In other words, I don’t think that moving one Polity score away from being an autocracy to being a democracy would have a consistent effect on investor confidence (and; therefore, net FDI inflows). Appel and Loyle’s model agrees with me: the regime type variable is not statistically significant.\nRather, I suspect that strong democracies and strong autocracies provide the political stability required to comfort foreign investors. These investors believe that the strong control democrats and autocrats have over their citizens and institutions reduces the risk that the country will re-enter into conflict. However, hybrid regimes do not tend to have this level of control. Investors are; therefore, less likely to invest in post-conflict countries with hybrid regimes.\nLet’s test this!\nWe now have a categorical variable with three categories: democracy, hybrid regime, and autocracy. We have thus far largely dealt with binary categorical variables (voted or not, Southern or not, female or not). How do we use and interpret multiple categorical variables in regression analysis?\nHappily, the intuition remains the same as with our binary categorical variables. We hold one category out as our baseline category and then compare the associated effects of the other categories to this one.\nLet’s step through that using a stripped back version of our model:\n\nm &lt;- lm(net_fdi_inflows ~ pcj + regime_type, data = ebj)\n\nmodelsummary(m, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\",\n                             \"regime_typeDemocracy\" = \"Democracy\",\n                             \"regime_typeHybrid regime\" = \"Hybrid\"),\n             stars = T)\n\n\n\n    \n\n      \n\n \n                (1)\n              \n+ p \n\n\n(Intercept)\n                  214.555\n                \n\n\n                  (497.984)\n                \n\nPCJ institutions established\n                  1816.169*\n                \n\n\n                  (764.093)\n                \n\nHybrid\n                  553.893\n                \n\n\n                  (667.644)\n                \n\nDemocracy\n                  -148.124\n                \n\n\n                  (809.623)\n                \n\nNum.Obs.\n                  95\n                \n\nR2\n                  0.068\n                \n\nR2 Adj.\n                  0.037\n                \n\nAIC\n                  1787.4\n                \n\nBIC\n                  1800.1\n                \n\nLog.Lik.\n                  -888.688\n                \n\nRMSE\n                  2795.25\n                \n\n\n\n\n\n\nYou’ll note that autocracies are missing from our regression table. This is because they are being held out as our baseline category. Their effect on net FDI inflows is captured by the intercept coefficient.\n\n\n\n\n\n\nTip\n\n\n\nWe often say that the intercept coefficient represents the predicted average value of our outcome of interest when all independent variables are set to zero. It might be useful for you to think of your baseline category as taking on the value zero. For example, we can think of autocracy = 0.\n\n\nOur model suggests that autocracies (regime_type = \"Autocracy\") that have not established a commission (pcj = \"No institutions\") have a predicted average net FDI inflow of $214.55 million.\nThe coefficients on democracies and hybrid regimes need to be interpreted in relation to autocracies (their baseline category). From our model, we can see that the coefficient for democracies is negative and the coefficient for hybrid regimes is positive. That means that, on average, democracies tend to receive less net FDI inflows than autocracies and hybrid regimes tend to receive more net FDI inflows than autocracies, holding all else constant.\nPredictions with multiple categorical variables\nUsing our model, what do we predict to be the net FDI inflows for democracies, autocracies, and hybrid regimes that either have a PCJ institutions or do not?\nFirst, let’s create a table with each possible combination of these two variables of interest:\n\nnew_data &lt;- tibble(pcj = factor(c(\"No institutions\", \n                                  \"PCJ institutions\"))) |&gt; \n  cross_join(\n    tibble(regime_type = factor(c(\"Autocracy\", \"Democracy\", \"Hybrid regime\")))\n  )\n\nnew_data\n\n# A tibble: 6 × 2\n  pcj              regime_type  \n  &lt;fct&gt;            &lt;fct&gt;        \n1 No institutions  Autocracy    \n2 No institutions  Democracy    \n3 No institutions  Hybrid regime\n4 PCJ institutions Autocracy    \n5 PCJ institutions Democracy    \n6 PCJ institutions Hybrid regime\n\n\nThen we can use our model to predict what we expect a hypothetical state with each of these combinations of characteristics to receive in net FDI inflows:\n\npred &lt;- augment(m, newdata = new_data)\n\npred\n\n# A tibble: 6 × 3\n  pcj              regime_type   .fitted\n  &lt;fct&gt;            &lt;fct&gt;           &lt;dbl&gt;\n1 No institutions  Autocracy       215. \n2 No institutions  Democracy        66.4\n3 No institutions  Hybrid regime   768. \n4 PCJ institutions Autocracy      2031. \n5 PCJ institutions Democracy      1883. \n6 PCJ institutions Hybrid regime  2585. \n\n\nThat’s a bit unweildy. Let’s visualize it!\n\nggplot(pred, aes(x = .fitted, y = pcj, colour = regime_type)) + \n  geom_point(size = 5) + \n  theme_minimal() + \n  labs(x = \"Predicted net FDI inflows (USD, million)\",\n       y = NULL, \n       colour = \"Regime type\")\n\n\n\n\n\n\n\nWe can clearly see that states that established PCJ institutions received, on average, larger net FDI inflows than states that did not, no matter their regime type. Further and completely counter to my hypothesis, hybrid regimes have, on average, the highest net FDI inflows compared to democracies and autocracies even when we account for whether the state has a PCJ institution.\nI’m not too worried: none of these coefficients are anywhere close to being statistically significant. I suspect that there is a more complex relationship underlying commercial actors’ beliefs about the stability of hybrid regimes, democracies, and autocracies and its effect on net investment flows. But I hope this serves as a good illustration of how we can use multiple categorical variables in our analyses.",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/05-linear_regression.html#footnotes",
    "href": "content/05-linear_regression.html#footnotes",
    "title": "Estimating Causal Effects with Observational Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Comprehensive R Archive Network (CRAN) hosts all R packages that can be installed easily using the familiar install.packages() function. These packages have gone through a comprehensive quality assurance process. I wrote polisciols for this class and update it regularly. I, therefore, will not host it through CRAN: the quality assurance process takes too long to be practical for our schedule. Instead, you are downloading it directly from its Github repository.↩︎",
    "crumbs": [
      "Content",
      "Session 5",
      "Estimating Causal Effects with Observational Data"
    ]
  },
  {
    "objectID": "content/01-measurement.html",
    "href": "content/01-measurement.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "You have an interesting question that you want to explore. You have some data that relate to that question. Included in these data are information on your outcome of interest and information on the things that you think determine or shape that outcome. You think that one (or more) of the drivers is particularly important, but no one has yet written about it or proven its importance. Brilliant! What do you do now?\nThe first step in any empirical analysis is getting to know your data. I mean, really getting to know your data. You want to dig into it with a critical eye. You want to understand any patterns lurking beneath the surface.\nUltimately, you want to get a really good understanding of the data generation process. This process can be thought of in two different and important ways. First, you want to understand how, out there in the real world, your outcome and drivers come to be. For example, if you are interested in voting patterns, you want to know the nitty gritty process of how people actually vote. Do they have to travel long distances, stand in long queues, fill out a lot of paperwork? Are there age restrictions on their ability to vote? Are there more insidious restrictions that might suppress voting for one particular group in the electorate?\nYou can use the skills we will discuss in this section to help you answer these questions. For example, you can determine whether there are relatively few young voters compared to older voters. If so, why? In turn, your growing expertise in and understanding of the data generation process should inform your exploration of the data. You might note that people have to wait in long queues on a Tuesday to vote. Does this impact the number of workers vs. retirees who vote?\nNow, this is made slightly more tricky by the second part of this process. You need to understand how your variables are actually measured. How do we know who turns out to vote? Did you get access to the voter file, which records each individual who voted and some interesting and potentially relevant demographic information about them? Or are you relying on exit polls, that only include a portion of those who voted? Were the people included in the polls reflective of the total voting population? What or whom is missing from this survey? Of course, if your sample is not representative, you might find some patterns that appear to be very important to your outcome of interest but are, in fact, just an artifact of a poorly drawn sample. If your survey failed to get responses from young people, you may be led to falsely believe that young people don’t vote.\nThis session you will be introduced to the first part of the data analysis process: data exploration. We use descriptive statistics to summarize patterns in our data. These are powerful tools that will inform you of the shape of your variables of interest. With this knowledge, you will start to answer your important question and potentially identify new ones. You will also be able to sense-check your more complex models and pick up on odd or incorrect relationships that they may find.\nAs you make your frequency tables and histograms and very elaborate dot plots and box charts, keep in mind that these tools are useful for your interrogation of the data generation process. Be critical. Continue to ask whether your data allow you to detect true relationships between your variables of interest. Build your intuition for what is really going on and what factors are really driving your outcome of interest.\nLet’s get started.",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "content/01-measurement.html#introduction",
    "href": "content/01-measurement.html#introduction",
    "title": "Data and Measurement",
    "section": "",
    "text": "You have an interesting question that you want to explore. You have some data that relate to that question. Included in these data are information on your outcome of interest and information on the things that you think determine or shape that outcome. You think that one (or more) of the drivers is particularly important, but no one has yet written about it or proven its importance. Brilliant! What do you do now?\nThe first step in any empirical analysis is getting to know your data. I mean, really getting to know your data. You want to dig into it with a critical eye. You want to understand any patterns lurking beneath the surface.\nUltimately, you want to get a really good understanding of the data generation process. This process can be thought of in two different and important ways. First, you want to understand how, out there in the real world, your outcome and drivers come to be. For example, if you are interested in voting patterns, you want to know the nitty gritty process of how people actually vote. Do they have to travel long distances, stand in long queues, fill out a lot of paperwork? Are there age restrictions on their ability to vote? Are there more insidious restrictions that might suppress voting for one particular group in the electorate?\nYou can use the skills we will discuss in this section to help you answer these questions. For example, you can determine whether there are relatively few young voters compared to older voters. If so, why? In turn, your growing expertise in and understanding of the data generation process should inform your exploration of the data. You might note that people have to wait in long queues on a Tuesday to vote. Does this impact the number of workers vs. retirees who vote?\nNow, this is made slightly more tricky by the second part of this process. You need to understand how your variables are actually measured. How do we know who turns out to vote? Did you get access to the voter file, which records each individual who voted and some interesting and potentially relevant demographic information about them? Or are you relying on exit polls, that only include a portion of those who voted? Were the people included in the polls reflective of the total voting population? What or whom is missing from this survey? Of course, if your sample is not representative, you might find some patterns that appear to be very important to your outcome of interest but are, in fact, just an artifact of a poorly drawn sample. If your survey failed to get responses from young people, you may be led to falsely believe that young people don’t vote.\nThis session you will be introduced to the first part of the data analysis process: data exploration. We use descriptive statistics to summarize patterns in our data. These are powerful tools that will inform you of the shape of your variables of interest. With this knowledge, you will start to answer your important question and potentially identify new ones. You will also be able to sense-check your more complex models and pick up on odd or incorrect relationships that they may find.\nAs you make your frequency tables and histograms and very elaborate dot plots and box charts, keep in mind that these tools are useful for your interrogation of the data generation process. Be critical. Continue to ask whether your data allow you to detect true relationships between your variables of interest. Build your intuition for what is really going on and what factors are really driving your outcome of interest.\nLet’s get started.",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "content/01-measurement.html#set-up",
    "href": "content/01-measurement.html#set-up",
    "title": "Data and Measurement",
    "section": "Set up",
    "text": "Set up\nThroughout this course, you will need a series of data sets I have collected, cleaned, and stored in the polisciols R package. These data sets were collected and published by political scientists (including some incredible GVPT alumni). This package is not published on CRAN1, so you will need to install it using the following code:\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"hgoers/polisciols\")\n\nYou will also need access to the following R packages to complete this week’s activities:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"wbstats\", \"janitor\", \"skimr\", \"countrycode\",\n                   \"scales\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(polisciols)\nlibrary(wbstats)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(countrycode)\nlibrary(scales)\n\n\n\n\n\n\n\nWarning\n\n\n\nSome of the R code used in this and future sessions will currently be a bit advanced for you if you don’t have a background in R. You don’t need to be able to follow the code to successfully complete this session. I have left it here so you can return to it when you are further along in the course and are more comfortable with R.",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "content/01-measurement.html#describing-your-data",
    "href": "content/01-measurement.html#describing-your-data",
    "title": "Data and Measurement",
    "section": "Describing your data",
    "text": "Describing your data\nBroadly, there are two types of variables: categorical and continuous variables. Categorical variables are discrete. They can be unordered (nominal) - for example, the different colours of cars - or ordered (ordinal) - for example, whether you strongly dislike, dislike, are neutral about, like, or strongly like Taylor Swift.\n\n\n\n\n\n\nNote\n\n\n\nDichotomous (or binary) variables are a special type of categorical variable. They take on one of two values. For example: yes or no; at war or not at war; is a Swifty, or is not a Swifty.\n\n\nContinuous variables are, well, continuous. For example, your height or weight, a country’s GDP or population, or the number of fatalities in a battle.\n\n\n\n\n\n\nNote\n\n\n\nContinuous variables can be made into (usually ordered) categorical variables. This process is called binning. For example, you can take individuals’ ages and reduce them to 0 - 18 years old, 18 - 45 years old, 45 - 65 years old, or 65+ years old. You lose information in this process: you cannot go from 45 - 65 years old back to the individuals’ precise age. In other words, you cannot go from a categorical to continuous variable.\n\n\nLet’s take a look at how you can describe these different types of variables using real-world political science examples.",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "content/01-measurement.html#describing-categorical-variables",
    "href": "content/01-measurement.html#describing-categorical-variables",
    "title": "Data and Measurement",
    "section": "Describing categorical variables",
    "text": "Describing categorical variables\nGenerally, we can get a good sense of a categorical variable by looking at counts or proportions. For example, which category contains the most number of observations? Which contains the least?\n\n\n\n\n\n\nNote\n\n\n\nLater, we will ask interesting questions using these summaries. These include whether differences between the counts and/or percentages of cases that fall into each category are meaningfully (and/or statistically significantly) different from one another. This deceptively simple question serves as the foundation for a lot of empirical research.\n\n\nLet’s use the American National Election Survey to explore how to produce useful descriptive statistics for categorical variables using R. The ANES surveys individual US voters prior to and just following US Presidential Elections. It asks them about their political beliefs and behavior.\nWe can access the latest survey (from the 2020 Presidential Election) using the polisciols package:\n\npolisciols::nes\n\n\n\n\n\n\n\nExercise\n\n\n\nTake a look at the different pieces of information collected about each respondent by running ?nes in your console.\n\n\nLet’s look at US voters’ views on income inequality in the US. Specifically, we will look at whether individuals think the difference in incomes between rich and poor people in the United States today is larger, smaller, or about the same as it was 20 years ago.\nRespondents could provide one of four answers (or refuse to answer the question, which is marked as NA):\n\nCodenes |&gt; \n  distinct(income_gap) |&gt; \n  arrange(income_gap)\n\n# A tibble: 5 × 1\n  income_gap    \n  &lt;ord&gt;         \n1 Don't know    \n2 Smaller       \n3 About the same\n4 Larger        \n5 &lt;NA&gt;          \n\n\nThis is an ordinal categorical variable. It is discrete and ordered. We can take a look at the variable itself using the helpful skimr::skim() function:\n\nskim(nes$income_gap)\n\n\nData summary\n\n\nName\nnes$income_gap\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ndata\n54\n0.99\nTRUE\n4\nLar: 6117, Abo: 1683, Sma: 416, Don: 10\n\n\n\n\n\nFrom this, we learn that:\n\nWe have information on 8,280 observations (or respondents)\nThe variable type is a factor (see the R tip below)\nWe are missing 54 observations (in other words, 54 people did not answer the question)\nThis means that we have information on 99% of our observations (from complete_rate).\n\n\n\n\n\n\n\nTip\n\n\n\nRemember, there are many different types of data that R recognizes. These include characters (\"A\", \"B\", \"C\"), integers (1, 2, 3), and logical values (TRUE or FALSE). R treats categorical variables as factors.\n\n\nFrequency distribution\nWhat count and proportion of respondents provided each answer? We can take advantage of janitor::tabyl() to quickly calculate this:\n\ntabyl(nes, income_gap)\n\n     income_gap    n     percent valid_percent\n     Don't know   10 0.001207729   0.001215658\n        Smaller  416 0.050241546   0.050571359\n About the same 1683 0.203260870   0.204595186\n         Larger 6117 0.738768116   0.743617797\n           &lt;NA&gt;   54 0.006521739            NA\n\n\n\n\n\n\n\n\nTip\n\n\n\nvalid_percent provides the proportion of respondents who provided each answer with missing values removed from the denominator. For example, the ANES surveyed 8,280 respondents in 2020, but only 8,226 of them answered this question.\n6,117 responded that they believe the income gap is larger today than it was 20 years ago. Therefore, the Larger proportion (which is bounded by 0 and 1, whereas percentages are bounded by 0 and 100) is 6,117 / 8,280 and its valid proportion is 6117 / 8,226.\n\n\nVisualizing this frequency\nIt is a bit difficult to quickly determine relative counts. Which was the most popular answer? Which was the least? Are these counts very different from each other?\nVisualizing your data will give you a much better sense of it. I recommend using a bar chart to show clearly relative counts.\n\nggplot(nes, aes(y = income_gap)) + \n  geom_bar() +\n  theme_minimal() + \n  labs(\n    x = \"Count of respondents\",\n    y = NULL,\n    caption = \"Source: ANES 2020 Survey\"\n  ) + \n  scale_x_continuous(labels = scales::comma)\n\n\n\nDo you think the difference in incomes between rich people and poor people in the United States today is larger, smaller, or about the same as it was 20 years ago?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\ngeom_bar() automatically counts the number of observations in each category.\n\n\nFrom this plot we quickly learn that a large majority of respondents believe that the income gap has grown over the last 20 years. Very few people believe it has shrunk.",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "content/01-measurement.html#describing-continuous-variables",
    "href": "content/01-measurement.html#describing-continuous-variables",
    "title": "Data and Measurement",
    "section": "Describing continuous variables",
    "text": "Describing continuous variables\nWe need to treat continuous variables differently from categorical ones. Continuous variables cannot meaningfully be bound together and compared. For example, imagine making a frequency table or bar chart that counts the number of countries with each observed GDP. You would have 193 different counts of one. Not very helpful!\nWe can get a much better sense of our continuous variables by looking at how they are distributed across the range of all possible values they could take on. Phew! Let’s make sense of this using some real-world data.\nFor this section, we will look at how much each country spends on education as a proportion of its gross domestic product (GDP). We will use wbstats::wb_data() to collect these data directly from the World Bank’s data portal.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the hidden code chunk to see how to do this.\n\n\n\nCodeperc_edu &lt;- wb_data(\n  \"SE.XPD.TOTL.GD.ZS\", start_date = 2020, end_date = 2020, return_wide = F\n) |&gt; \n  transmute(\n    country, \n    region = countrycode(country, \"country.name\", \"region\"),\n    year = date,\n    value\n  )\n\nperc_edu\n\n# A tibble: 217 × 4\n   country             region                      year value\n   &lt;chr&gt;               &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt;\n 1 Afghanistan         South Asia                  2020 NA   \n 2 Albania             Europe & Central Asia       2020  3.34\n 3 Algeria             Middle East & North Africa  2020  6.19\n 4 American Samoa      East Asia & Pacific         2020 NA   \n 5 Andorra             Europe & Central Asia       2020  2.63\n 6 Angola              Sub-Saharan Africa          2020  2.67\n 7 Antigua and Barbuda Latin America & Caribbean   2020  2.99\n 8 Argentina           Latin America & Caribbean   2020  5.28\n 9 Armenia             Europe & Central Asia       2020  2.71\n10 Aruba               Latin America & Caribbean   2020 NA   \n# ℹ 207 more rows\n\n\nWe can get a good sense of how expenditure varied by country by looking at the center, spread, and shape of the distribution.\nVisualizing continuous distributions\nFirst, let’s plot each country’s spending to see how they relate to one another. There are two plot types commonly used for this: histograms and density curves.\nHistograms\nA histogram creates buckets along the range of values our variable can take (i.e. buckets of 10 between 1 and 100 would include 1 - 10, 11 - 20, 21 - 30, etc.). It then counts the number of observations that fall into each of those buckets and plots that count.\nLet’s plot our data as a histogram with a bin width of 1 percentage point:\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram(binwidth = 1) + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Number of countries\"\n  )\n\n\n\n\n\n\n\nFrom this we learn that most countries spend between three to five percent of their GDP on education. There appears to be some outliers. Some countries spend over 10 percent of their GDP on education. This is well above the proportion all other countries spent.\nIf we pick a narrower bin width, we will see more fine-grained detail about the distribution of our data:\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram(binwidth = 0.25) + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Number of countries\"\n  )\n\n\n\n\n\n\n\nFrom this we learn that most countries spent around four percent of their GDP on education. There is a small cluster of countries that spent between around 7.5 to nine percent on these services. Three countries spent an unusually large proportion of their GDP (over 10 percent of it) on education annually.\nDensity curves\nDensity curves also communicate the distribution of continuous variables. They plot the density of the data that fall at a given value on the x-axis.\nLet’s plot our data using a density plot:\n\nggplot(perc_edu, aes(x = value)) + \n  geom_density() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\nThis provides us with the same information above, but highlights the broader shape of our distribution. We again learn that most countries spend around four percent of their GDP on education. There are some that spend above 7.5 percent.\nUnderstanding distributions\nWe can use the shape of a variable’s distribution to usefully summarize it or to more easily compare it to other variables. Is the distribution symmetric or skewed? Where are the majority of observations clustered? Are there multiple distinct clusters, or high points, in the distribution?\nThere are three broad distributions that you should know: Normal, right-skewed, and left-skewed. People use these terms to summarize the shape of their continuous data.\nNormal distribution\nA Normally distributed variable includes values that fall symmetrically away from their center point, which is the peak (or most common value). Examples of Normally distributed data include the height or weight of all individuals in a large population.\n\n\n\n\n\n\nNote\n\n\n\nThis distribution is also referred to as a bell-curve.\n\n\n\nggplot() + \n  geom_density(aes(x = rnorm(n = 1e6))) + \n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nrnorm() draws a number randomly from a Normal distribution.\n\n\nRight-skewed distribution\nWith right-skewed data, the majority of data have small values with a small number of larger values. Examples of right-skewed data include countries’ GDP.\n\nggplot() + \n  geom_density(aes(x = rbeta(1e6, 2, 10))) + \n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nrbeta() draws a number randomly from a beta distribution. Beta distributions are very useful. They can take on many shapes, including this cool wave. Importantly, they are bounded between zero and one, which makes them useful for studying phenomena that are similarly bounded. For example, proportions (and percentages).\n\n\nLeft-skewed distribution\nWith left-skewed data, the majority of data have large values with a small number of small values. Examples of left-skewed data include democracies’ election turn-out rates.\n\nggplot() + \n  geom_density(aes(x = rbeta(1e6, 10, 2))) + \n  theme_void()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\nWe can also use measures of central tendency to quickly describe and compare our variables.\nMean\nThe mean is the average of all values. Formally:\n\\[\n\\bar{x} = \\frac{\\Sigma x_i}{n}\n\\]\nIn other words, add all of your values together and then divide that total by the number of values you have. We can use R to quickly calculate the mean percent of GDP spent on education by countries in 2020:\n\nmean(perc_edu$value, na.rm = T)\n\n[1] 4.509317\n\n\nOn average, countries spent 4.51% of their GDP on education in 2020.\n\n\n\n\n\n\nTip\n\n\n\nIf you do not use the argument na.rm (read “NA remove!”), you will get an NA if any exist in your vector of values. This is a good default! You should be very aware of missing data points.\n\n\nMedian\nThe median is the mid-point of all values. To calculate it, put all of your values in order from smallest to largest. Identify the value in the middle. That’s your median.\nIn R:\n\nmedian(perc_edu$value, na.rm = T)\n\n[1] 4.446204\n\n\nThe median country spent 4.45% of their GDP on education in 2020.\n\n\n\n\n\n\nTip\n\n\n\nIf you have an even number of observations, the median is the half-way-point between the two middle numbers of your ordered values (or the mean of those two middle values). For example, the median of all values from one to 10 is 5.5.\n\n\nMode\nThe mode is the most frequent of your values. To calculate it, count how many times each value occurs in your data set. The one that occurs the most is your mode.\n\n\n\n\n\n\nNote\n\n\n\nThis is usually a more useful summary statistic for categorical variables than continuous ones. For example, which colour of car is most popular? Which political party has the most members?\n\n\nWe can find the modal region in our data set using base R’s table():\n\ntable(perc_edu$region)\n\n\n       East Asia & Pacific      Europe & Central Asia \n                        37                         58 \n Latin America & Caribbean Middle East & North Africa \n                        42                         21 \n             North America                 South Asia \n                         3                          8 \n        Sub-Saharan Africa \n                        48 \n\n\nThe modal (or most common) region in our data set is Europe & Central Asia.\nUsing central tendency to describe and understand distributions\nNormally distributed values have the same mean and median.\n\n\n\n\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\n\n\n\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\n\n\n\n\n\n\n\nWhen do we care about the mean or the median? There is no simple answer to this question. Both of these values are useful summaries of our continuous data. We tend to use the average to describe our data in statistical analysis. As you will learn, most regression models are, fundamentally, just fancy averages of our data. However, this approach is not always sensible.\nAs you may have noted above, the average value is more sensitive to extreme values. If you have one very large or very small number in your vector of numbers, your average will be pulled well away from your mid-point (or median). This can lead you astray. To illustrate, let’s look at the average and median of the numbers between one and 10:\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nmean(x)\n\n[1] 5.5\n\nmedian(x)\n\n[1] 5.5\n\n\nIf we add one very large number to our vector, our average will shoot up but our median will only move up one additional number in our collection:\n\nx &lt;- c(x, 1000)\nx\n\n [1]    1    2    3    4    5    6    7    8    9   10 1000\n\n\n\nmean(x)\n\n[1] 95.90909\n\nmedian(x)\n\n[1] 6\n\n\nWhich number better summarizes our data? Here, I would suggest that the average is misleading. That one 1,000 data point is doing a lot of the work. The median better describes the majority of my data.\nWe will talk more about this (and outliers more specifically) later in the course.\nFive number summary\nAs you can see, we are attempting to summarize our continuous data to give us a meaningful but manageable sense of it. Means and medians are useful for this.\nWe can provide more context to our understanding using more summary statistics. A common approach is the five number summary. This includes:\n\nThe smallest value;\nThe 25th percentile value, or the median of the lower half of the data;\nThe median;\nThe 75th percentile value, or the median of the upper half of the data;\nThe largest value.\n\nWe can use skimr::skim() to quickly get useful information about our continuous variable.\n\nskim(perc_edu$value)\n\n\nData summary\n\n\nName\nperc_edu$value\n\n\nNumber of rows\n217\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ndata\n52\n0.76\n4.51\n1.75\n0.36\n3.3\n4.45\n5.53\n10.54\n▂▇▇▂▁\n\n\n\n\n\nWe have 217 rows (because our unit of observation is a country, we can read this as 217 countries2). We are missing education spending values for 52 of those countries (see n_missing or summary()’s NA's), giving us a complete rate of 76% (see complete_rate).\nThe country that spent the least on education as a percent of its GDP in 2020 was Nigeria, which spent only 0.4% (see p0). The country that spent the most was the Micronesia, Fed. Sts., which spent 10.5% (see p100). The average percent of GDP spent on education in 2020 was 4.5% (see mean) and the median was 4.4% (see p50).\nThis description was a bit unwieldy. As usual, to get a better sense of our data we should visualize it.\nBox plots\nBox plots (sometimes referred to as box and whisker plots) visualize the five number summary (with bonus features) nicely.\n\nggplot(perc_edu, aes(x = value)) + \n  geom_boxplot() + \n  theme_minimal() + \n  theme(\n    axis.text.y = element_blank()\n  ) + \n  labs(\n    x = \"Expenditure on education as a percentage of GDP\",\n    y = NULL\n  )\n\n\n\n\n\n\n\nThe box in the graph above displays the 25th percentile, the median, and the 75th percentile values. The tails show you all the data up to a range 1.5 times the interquartile range (IQR), or the 75th percentile minus the 25th percentile (or the upper edge of the box minus the lower edge of the box). If the smallest or largest values fall below or above (respectively) 1.5 times the IQR, the tail ends at that value. The remaining data points (if they exist) are displayed as dots shooting away from the whiskers of our box and whisker plot.\nOutliers\nNote that some countries’ expenditure are displayed as dots. The box plot above is providing you with a bit more information than the five number summary alone. If the data include values that fall outside of the IQR, they are displayed as dots. These are (very rule of thumb, take with a grain of salt, please rely on your theory and data generation process instead!) candidates for outliers.\nOutliers fall so far away from the majority of the other values that they should be examined closely and perhaps excluded from your analysis. As discussed above, they can distort your mean. They do not, however, tend to distort your median.\n\n\n\n\n\n\nNote\n\n\n\nWe will talk more about how to deal with outliers later in the course.\n\n\nMeasures of spread: range, variance, and standard deviation\nWe now have a good sense of some of the features of our data. Another useful thing to know is the shape of the distribution. Here, measures of spread are useful.\nRange\nThe range is the difference between the largest and smallest value.\n\\[\nrange = max - min\n\\]\nIn R:\n\nmax(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)\n\n[1] 10.17936\n\n\nThe difference between the country that spends the highest proportion of its GDP on education and that which spends the least is 10.18 percentage points.\nVariance\nThe variance measures how spread out your values are. On average, how far are your observations from the mean?\nThis measure can, at first, be a bit too abstract to get an immediate handle on. Let’s walk through it. Imagine we have two data sets, wide_dist and narrow_dist. Both are Normally distributed, share the same mean (0), and the same number of observations (1,000,000).\nLet’s plot them:\n\n\n\n\n\n\n\n\nDespite both having the same center point and number of observations, the data are much more spread out around that center point in the top graph (of wide_dist).\nThe data in the top graph have higher variance (are more spread out) than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance of wide_dist. To do this:\n\nCalculate the mean of your values.\nCalculate the difference between each individual value and that mean (how far from the mean is every value?).\nSquare those differences.\n\n\n\n\n\n\n\nTip\n\n\n\nWe do not care whether the value is higher or lower than the mean. We only care how far from the mean it is. Squaring a value removes its sign (positive or negative). Remember, if you multiply a negative number by a negative number, you get a positive number. This allows us to concentrate on the difference between each individual data point and the mean.\n\n\n\nAdd all of those squared differences to get a single number.\nDivide that single number by the number of observations you have minus 1.\n\nYou now have your variance!\nIn R:\n\nwide_dist_mean &lt;- mean(wide_dist$x)\n\nwide_var_calc &lt;- wide_dist |&gt; \n  mutate(\n    # Calculate the mean\n    mean = wide_dist_mean,\n    # Calculate the difference between each value and the mean\n    diff = x - mean,\n    # Square that difference\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n        x      mean   diff diff_2\n    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  1.82  -0.000715  1.82   3.31 \n 2  2.01  -0.000715  2.01   4.03 \n 3  0.804 -0.000715  0.804  0.647\n 4  1.12  -0.000715  1.12   1.26 \n 5  1.02  -0.000715  1.02   1.04 \n 6  2.95  -0.000715  2.95   8.69 \n 7  2.01  -0.000715  2.02   4.06 \n 8  2.45  -0.000715  2.45   6.02 \n 9 -1.41  -0.000715 -1.41   1.99 \n10  1.43  -0.000715  1.43   2.05 \n# ℹ 999,990 more rows\n\n\nWe the add those squared differences between each observation and the mean of our whole sample together. Finally, we divide that by one less than our number of observations.\n\nwide_var &lt;- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 3.99662\n\n\nWe can compare this to the variance for our narrower distribution:\n\nCodenarrow_var_calc &lt;- narrow_dist |&gt; \n  mutate(\n    mean = mean(narrow_dist$x),\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var &lt;- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.001166\n\n\nIt is, in fact, smaller!\nThat was painful. Happily we can use base R’s var() to do this in one step:\n\nvar(wide_dist)\n\n        x\nx 3.99662\n\n\n\nvar(narrow_dist)\n\n         x\nx 1.001166\n\n\n\nvar(wide_dist) &gt; var(narrow_dist)\n\n     x\nx TRUE\n\n\nOn average, countries spent 3.06% more or less than the average of 4.51% of their GDP on education in 2020.\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 1.999155\n\n\n\nsqrt(narrow_var)\n\n[1] 1.000583\n\n\nYou can get this directly using base R’s sd():\n\nsd(wide_dist$x)\n\n[1] 1.999155\n\n\n\nsd(narrow_dist$x)\n\n[1] 1.000583\n\n\nThe standard deviation of all countries’ percentage of their GDP that they spent on education in 2020 was 1.75%. This horrible sentence demonstrates that standard deviations are most usefully employed in contexts other than attempts to better describe your variables of interest. For example, they are very important for determining how certain we can be about the relationships between different variables we uncover using statistical models (which we will get to later in the course).",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "content/01-measurement.html#conclusion",
    "href": "content/01-measurement.html#conclusion",
    "title": "Data and Measurement",
    "section": "Conclusion",
    "text": "Conclusion\nYour empirical analysis is only as strong as its foundation. You can use the tools you learnt this session to build a very strong foundation. Always start any analysis by getting a very good sense of your data. Look at it with a critical eye. Does it match your intuition? Is something off? What can you learn about the peaks and troughs among your observations?\nNow you need to head over to ELMs and successfully complete this session’s quiz. Good luck!",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "content/01-measurement.html#footnotes",
    "href": "content/01-measurement.html#footnotes",
    "title": "Data and Measurement",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Comprehensive R Archive Network (CRAN) hosts many R packages that can be installed easily using the familiar install.packages() function. These packages have gone through a comprehensive quality assurance process. I wrote polisciols for this class and will update it regularly. I, therefore, will not host it through CRAN: the quality assurance process takes too long to be practical for our weekly schedule. Instead, you are downloading it directly from its Github repository.↩︎\nYou are right: there were not 217 countries in 2020. The World Bank collects data on some countries that are not members of the UN (and would not, traditionally, be considered to be countries).↩︎",
    "crumbs": [
      "Content",
      "Session 1",
      "Data and Measurement"
    ]
  },
  {
    "objectID": "resources/quarto.html",
    "href": "resources/quarto.html",
    "title": "Introduction to Quarto and markdown",
    "section": "",
    "text": "Quarto is a tool that helps you to create fully reproducible research outputs. It allows you to combine your code, results, and prose in one document. For example, this website - with all of its R code, prose, and visualizations - was created using Quarto.\nYou can use Quarto from RStudio.1 Below is a screen shot of a Quarto document (file extension .qmd) and its HTML output. You can render a Quarto document to many different types of formats, including PDF and MS Word.\nLet’s make a new Quarto document, including some R code and prose."
  },
  {
    "objectID": "resources/quarto.html#a-new-quarto-document",
    "href": "resources/quarto.html#a-new-quarto-document",
    "title": "Introduction to Quarto and markdown",
    "section": "A new Quarto document",
    "text": "A new Quarto document\nOpen up a new Quarto document in RStudio:\n\nFill in the relevant fields:\n\nYour new document will have a .qmd file extension. It will also already contain some text and code. Most of this is demonstrative and can be deleted. However, the top section is very important and should be kept. This section (written in YAML) includes the metadata for your document. By default, it includes the title, author, format in which it will rendered, and the default RStudio editor.\n\n\n\n\n\n\nExercise\n\n\n\nSwitch your output (format) from HTML to PDF by changing html to pdf.\n\n\n\n\n\n\n\n\nTip\n\n\n\nA full list of the formats to which you can render your Quarto document is provided here.\n\n\n\nThere are two ways to work with and view Quarto documents. The default editor is visual, which follows a more “what-you-see-is-what-you-get” style. If you have worked a lot with MS Word documents or Google Docs, this interface will look familiar. Alternatively, you can edit in source, which looks more like a raw script. To switch between the two, you can use the Source and Visual icons in the top left hand side of the screen.\n\n\n\n\n\n\nNote\n\n\n\nI find myself switching between these two formats all the time. The visual editor is much easier to work in when writing, but it can be a bit buggy when it comes to writing code. I work in source when I am writing and running R code."
  },
  {
    "objectID": "resources/quarto.html#rendering-your-document",
    "href": "resources/quarto.html#rendering-your-document",
    "title": "Introduction to Quarto and markdown",
    "section": "Rendering your document",
    "text": "Rendering your document\nTo render your document into your chosen format (in this case: HTML), you need to hit the Render icon in the document’s top bar. This will produce an HTML version of your Quarto document in the same folder in which you saved your Quarto document.\n\n\n\n\n\n\nTip\n\n\n\nYou can preview your document in RStudio by changing your settings to Preview in Viewer Pane.\n\nNow, whenever you render your document a preview of it will show up in the Viewer pane (which is in the same place as your Files, Plots, and Help panes).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you toggle on the Render on Save option, your Quarto document will render and update your viewer every time you hit save. This can be helpful when you are formatting your document."
  },
  {
    "objectID": "resources/quarto.html#writing-prose-in-quarto",
    "href": "resources/quarto.html#writing-prose-in-quarto",
    "title": "Introduction to Quarto and markdown",
    "section": "Writing prose in Quarto",
    "text": "Writing prose in Quarto\nYou can write prose as you would in any other text editor in Quarto. When you are in the Visual editor model, Quarto provides you with the shortcut keys for many of the formats you use in other text editors, including MS Word and Google Docs. You can also use your usual keyboard shortcuts.\nIn the Source editor mode, you will need to use markdown. Markdown is a lightweight markup language that allows you to format plain text. It gives you a lot of control over the format of your text documents (similar to Latex).\n\n\n\n\n\n\nExercise\n\n\n\nComplete this great Markdown tutorial."
  },
  {
    "objectID": "resources/quarto.html#running-code-in-quarto",
    "href": "resources/quarto.html#running-code-in-quarto",
    "title": "Introduction to Quarto and markdown",
    "section": "Running code in Quarto",
    "text": "Running code in Quarto\nYou can also run code from within your Quarto document. You can do this through a code chunk or in-line code. I will step through both options now.\n\nCode chunks\nA code chunk starts with ```{r} and ends with ```. You can then write whole “chunks” of code that will output in your rendered document.\nFor example, I will load the tidyverse R package into my current session:\n```{r}\n#| echo: true\n\nlibrary(tidyverse)\n```\nYou can specify your chunk options using #| at the start of the line. For example, above I specified that I wanted the code in the code chunk to be shown when I render my document. You can hide the code by changing the chunk option echo to false. There are many different chunk options that you can control. A full list can be found here.\nYou can set the chunk options in the individual chunks, as show above. Alternatively, you can set them universally in the YAML section at the top of your Quarto document using the execute command. For example:\n```{yaml}\nexecute:\n  echo: true\n  message: false\n  warning: false\n```\nThis will apply to all code chunks unless you overwrite it by including chunk-specific options in a code chunk.\nCode chunks are useful for running large amounts of code. Commonly, I use them to include a plot, a regression table, or to read in my data or model results. For example, you can write the code to create a ggplot directly in your document.\n\n\n\nSource: Quarto\n\n\n\n\nIn-line code\nYou will often want to reference numbers or results in your prose. For example, I may be writing up the data section of a paper and want to specify that my data set includes 100 observations. If I were to write this in normally and then go away and collect more data, I would need to come back and update this number manually to reflect my new number of observations. I may do this several times (very tedious) or I may miss a time (we are all human). In-line coding allows you to make these updates programmatically.\nYou include R code directly in your prose using the expression: r. For example:\n\nWill render as: There are r nrow(mpg) observations in our data. No need to go and update this reference if that number changes!\n\n\n\n\n\n\nTip\n\n\n\nscales is a great R package for formatting numbers.\nFor example, R will output raw numbers such as 1000000000 and 8932348920. scales allows you to format these numbers so they are easier to read: scales::comma(1000000000) gives you r scales::comma(1000000000) and scales::dollar(8932348920) gives you r scales::dollar(8932348920).\n\n\nYou can use Quarto to produce all kinds of fully reproducible documents, including journal articles and reports. You can also use it to produce very professional-looking presentations. Finally, you can even use it to produce websites. In fact, all of the resources provided to you here were produced in Quarto."
  },
  {
    "objectID": "resources/quarto.html#footnotes",
    "href": "resources/quarto.html#footnotes",
    "title": "Introduction to Quarto and markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use it from VS Code, Jupyter, Neovim, and Editor.↩︎"
  },
  {
    "objectID": "content/04-bivariate.html",
    "href": "content/04-bivariate.html",
    "title": "Exploring the Relationship Between Two Variables",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"countrycode\", \"broom\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\nlibrary(modelsummary)\nlibrary(scales)",
    "crumbs": [
      "Content",
      "Session 4",
      "Exploring the Relationship Between Two Variables"
    ]
  },
  {
    "objectID": "content/04-bivariate.html#set-up",
    "href": "content/04-bivariate.html#set-up",
    "title": "Exploring the Relationship Between Two Variables",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"countrycode\", \"broom\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\nlibrary(modelsummary)\nlibrary(scales)",
    "crumbs": [
      "Content",
      "Session 4",
      "Exploring the Relationship Between Two Variables"
    ]
  },
  {
    "objectID": "content/04-bivariate.html#bivariate-relationships",
    "href": "content/04-bivariate.html#bivariate-relationships",
    "title": "Exploring the Relationship Between Two Variables",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nHow do two variables move with one another? When when goes up, does the other go down, up, or not really move at all? How dramatic is this shift?\nThe type of variables we have determines how we can answer this question. To begin, we will explore the relationship between two continuous variables. Later in the section, we will look at how to explore the relationship between a continuous and categorical variable.\nWe will return to the relationship between a country’s wealth and health. This question was made popular by the Gapminder project. We were introduced to it during the Transformations section in Session One.\nCollecting our data\nFirst, we need to collect our data. Following the Gapminder project, we measure each country’s health by its average life expectancy and its wealth by its GDP per capita. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |&gt; \n  # Give these variables more friendly names\n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |&gt;\n  mutate(\n    # Create a new variable that is GDP per capita logged\n    log_gdp_per_cap = log(gdp_per_cap),\n    # Add each country's region to the data set\n    region = countrycode(country, \"country.name\", \"region\")\n  ) |&gt; \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1 AW    ABW   Aruba           Latin…  2016      27442.     75.6           10.2 \n 2 AF    AFG   Afghanistan     South…  2016        522.     63.1            6.26\n 3 AO    AGO   Angola          Sub-S…  2016       1808.     61.1            7.50\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      40130.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41326.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12700.     76.3            9.45\n 8 AM    ARM   Armenia         Europ…  2016       3524.     74.7            8.17\n 9 AS    ASM   American Samoa  East …  2016      12843.     NA              9.46\n10 AG    ATG   Antigua and Ba… Latin…  2016      16558.     78.2            9.71\n# ℹ 207 more rows\n\n\nWhat is the relationship between these two variables?\nWhat is the relationship between a country’s average life expectancy and its GDP per capita? The easiest way to determine this is to visualize these two variables:\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"GDP per capita (USD current)\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nThere seems to be a good case that there is a strong relationship between a country’s GDP per capita (wealth) and its average life expectancy (health). It appears that we expect citizens that live in countries that have larger GPD per capita to live longer, on average. But this relationship is not linear (a straight line drawn through them will not summarize this relationship very well).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can transform your data to make it easier to work with. Just remember that you now need to talk in terms of logged GDP per capita instead of GDP per capita.\n\n\nI can imagine drawing a straight line among these points that summarizes how they vary with each other. It appears that as a country’s logged GDP per capita increases, so too does the average life expectancy of its population. As wealth increases, so too does health. These increases are consistent across each interval increase in wealth.\nWell, that was easy! What is the relationship between health and wealth? They increase with each other.\nHow can we measure the strength of that relationship?\nNow we need some way of measuring the strength of the relationship. In other words, what amount of the variation in countries’ average life expectancy is associated with variation in their GDP per capita? We can measure the strength of this association using correlations. The correlation coefficient tells us how closely variables relate to one another. It tells us both the strength and direction of the association.\n\nStrength: how closely are these values tied to one another. Measured from 0 to |1|, with values closer to 0 indicating a very weak relationship and values closer to |1| indicating a very strong relationship.\n\n\n\n\n\n\n\nTip\n\n\n\nWhat are those funny looking |s? They represent the absolute value, which is shorthand for the number regardless of its sign. To demonstrate, |1| is the absolute value of 1 and -1.\n\n\n\nDirection: do both \\(X\\) and \\(Y\\) change in the same direction? Positive correlations show that when \\(X\\) increases (decreases), so does \\(Y\\). Negative correlations show that when \\(X\\) increases (decreases), \\(Y\\) decreases (increases). In other words, they move in different directions.\n\nWhat is the correlation between logged GDP per capita and life expectancy?\n\ncor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = \"complete.obs\")\n\n[1] 0.8506936\n\n\nAs expected, the relationship is positive and strong.\nBuilding a generalizable description of this relationship\nWe have very quickly gained the skills to determine whether the relationship between two variables is positive, negative, or non-existent. We have also learnt how to describe the strength of that relationship. To that end, we are now able to describe the bivariate relationship between health and wealth as a positive and strong one.\nThis is useful, but we tend to need a more concrete way of describing the relationship between two variables. For example, what if a policy-maker comes up to you and asks what you think the effect of a $1,000 increase in a country’s GDP per capita will do to its average life expectancy? We can build simple models of this relationship to provide that policy-maker with a prediction of what we might expect to happen, on average. Further, we can use the model to describe the relationship between these two variables in a generalized way. If a new country were to spring into existence, we can use our knowledge of its GDP per capita to determine how long we might expect its citizens to live.\nOLS and linear regression\nLooking back at our data, we can image a straight line running between each country’s plotted average life expectancy and GDP per capita. Let’s draw that line:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nWe can, of course, draw many different lines through these points. Each of us has probably drawn a slightly different line in our heads. Which is the best line?\nOrdinary least squares (OLS) regression provides an answer. OLS regression draws the line that minimizes the distance between itself and all of the data points. That line can take many shapes, including a straight line, an S, a frowney face, and smiley face, etc.\nLooking at our data above, it appears that a straight line is the best line to draw.\n\n\n\n\n\n\nNote\n\n\n\nOverfitting involves fitting a model (or drawing a line through our data) that misses the forest for the trees. You can draw all kinds of shapes through those data that perhaps result in a smaller distance between itself and each dot. In fact, if you draw a line that connects all of those dots there will be no difference between your line and the data points.\nHowever, this model will be too focused on the data we have at hand. Our model will have no idea what to do with any new data points we introduce. This is bad! Your aim here is to produce a generalizable model of the relationship between these two variables, not to draw a line that connects this particular constellation of dots.\n\n\nOkay, so a straight line is the best type of line to draw. But there are still many, many different straight lines that we can draw. Which straight line is best? Remember, OLS regression finds the line that minimizes the distance between itself and all of the data points. Let’s step through this. Look at the graph above.\n\nDraw a line through those dots. Pick a line, any line!\nCalculate the distance between each dot and the line.\nSum up the absolute values of those distances. Remember, we just care about the distance, so we don’t need to worry about whether or not the dots are above or below the line.\nRepeat steps 1 - 3 many, many, many times.\nPick the line with the smallest sum of distances (the results from step 3).\n\nPhew, this seems tedious. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nEstimating a linear model in R\nHow did R do this? To answer this, we will first do some review.\nRemember (back to your high school maths classes) the general equation for a line:\n\\[\ny = a + mx\n\\]\nRead this as: the value of \\(y\\) is the sum of some constant, \\(a\\), and some \\(x\\) variable that has been transformed by some slope value \\(m\\).\nRemember that the slope constant, \\(m\\), tells you how much \\(y\\) changes for every one unit increase in \\(x\\).\nSo, if:\n\\[\ny = 10 + 2x\n\\]\nThen, when \\(x = 20\\):\n\\[\ny = 10 + 2*20 = 50\n\\]\nFor many values of \\(x\\):\n\nggplot(tibble(x = 0:50, y = 10 + 2*x), aes(x = x, y = y)) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\n\nWell, let’s substitute in our variables of interest. Our \\(y\\) variable is a country’s average life expectancy and our \\(x\\) variable is that country’s logged GDP per capita.\n\\[\nlife Exp_i = \\beta_0 + \\beta_1 logGdpPerCap_i + \\epsilon\n\\]\nRead this as: country \\(i\\)’s average life expectancy is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)).\nLet’s imagine that this relationship is accurately described by the following formula:\n\\[\nlife Exp_i = 30 + 4 * logGdpPerCap_i\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe will get to that pesky error term in just a minute.\n\n\nThen, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:\n\nggplot(\n  tibble(log_gdp_per_cap = 0:20, life_exp = 30 + 4*log_gdp_per_cap), \n  aes(x = log_gdp_per_cap, y = life_exp)\n) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nA country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of $148.41) has a predicted average life expectancy of 50 years, or \\(30 + 4*5\\).\nA country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of $22,026.47) has a predicted average life expectancy of 70 years, or \\(30 + 4*10\\).\nDoes this accurately describe what we see in our data? What is the average life expectancy for countries with roughly $22,000 GDP per capita?\n\ncountries_10 &lt;- filter(gapminder_df, gdp_per_cap &gt; 21000 & gdp_per_cap &lt; 23000)\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 KN    KNA   St. Kitts and N… Latin…  2016      21388.     71.7            9.97\n2 SA    SAU   Saudi Arabia     Middl…  2016      21516.     77.1            9.98\n3 SI    SVN   Slovenia         Europ…  2016      21448.     81.2            9.97\n\n\nWe predicted 70 years, but our data suggest that these countries have closer to an average of 77 years. Why do we have this difference?\nWell, we probably haven’t produced the best model we can (this isn’t the best line!). We just picked those numbers (\\(\\beta_0\\) and \\(\\beta_1\\)) out of thin air. Let’s fit a linear OLS regression and see if we improve our ability to predict what we have seen in the wild.\nHow do we calculate the constant (\\(\\beta_0\\)) using OLS regression?\nRemember, OLS regression simply finds the line that minimizes the distance between itself and all the data points.\n\n\n\n\n\n\nNote\n\n\n\nThe constant that minimizes this distance is the mean of \\(Y\\) minus \\(\\beta_1\\) times the mean of \\(X\\). You can prove this using some calculus. We won’t do that here, but feel free to Google it if you are very interested.\n\n\nSo, the constant that best predicts a country’s average life expectancy based on its logged GDP per capita is equal to the average life expectancy across our sample (72.3 years) minus the average logged GDP per capita ($8.80, or $6,652.93 GDP per capita) transformed by \\(\\beta_1\\).\nSo…\nHow do we calculate the coefficient \\(\\beta_1\\)?\nThe regression slope is the correlation coefficient between \\(X\\) and \\(Y\\) multiplied by the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\).\nEw… Let’s step through that.\nRemember, the correlation coefficient simply measures how \\(X\\) and \\(Y\\) change together. Does \\(Y\\) increase when \\(X\\) increases? How strong is this relationship?\nThe standard deviations of \\(X\\) and \\(Y\\) just measure how spread out they are.\nBringing these together, we are interested in how much \\(X\\) and \\(Y\\) change with each other moderated by how much they change independently of each other.\nFormally:\n\\[\n\\beta_1 = (\\frac{\\Sigma(\\frac{x_i - \\bar{x}}{s_X})(\\frac{y_i - \\bar{y}}{s_Y})}{n - 1})(\\frac{s_Y}{s_X}) = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma(x_i - \\bar{x})^2}\n\\]\nHappily R does all of this for us.\nLet’s fit that model already!\n\nm &lt;- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         32.794            4.521  \n\n\nOkay, so the line of best fit describing the relationship between average life expectancy and logged GDP per capita is:\n\\[\nlife Exp_i = 32.9 + 4.5 * logGdpPerCap_i + \\epsilon\n\\]\nThat’s it! We now have a model of the relationship between a country’s average life expectancy and its logged GDP per capita. This model is informed by what we actually observed in the world. It carefully balances our need to accurately describe what we have observed and to develop something that is generalizable.\nThe above model output is difficult to read. It will not be accepted by any journal or professor. Luckily, we can use modelsummary::modelsummary() to easily generate a professionally formatted table.\n\nmodelsummary(\n  m, \n  statistic = NULL,\n  coef_rename = c(\"log_gdp_per_cap\" = \"GDP per capita (logged)\"),\n  gof_map = \"nobs\"\n)\n\n\n\n    \n\n      \n\n \n                (1)\n              \n\n\n(Intercept)\n                  32.794\n                \n\nGDP per capita (logged)\n                  4.521\n                \n\nNum.Obs.\n                  203\n                \n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways make sure to change your variable names from ones that are easily read by a computer (for example, log_gdp_per_cap to one that are easily read by your human reader (for example, GDP per capita (logged)).\n\n\nAssumptions\nNote that OLS regression, particularly linear regression, requires that you make a lot of important assumptions about the relationship between your two variables. For example, we need to assume that the best line to fit is a straight one. We also assume that the best way to generate and describe the relationship across all observations is to fit the line that minimizes the distance between itself and the observed values or dots.\nThere are other approaches to determining the “best” line. These include maximum likelihood estimation and Bayesian statistics. We won’t discuss these approaches in this class. It’s worth noting here; however, that OLS regression requires a whole bunch of assumptions that may or may not be appropriate to your research question or theory. This class prepares you to grapple with those questions and to appropriately use these tools in your own research.\nPrediction and performance\nOkay, so we now have a model that describes the relationship between our outcome of interest (health) and our independent variable of interest (wealth). We can use this to predict our outcome of interest for different values of our independent variable. For example, what do we predict to be the average life expectancy of a country with a GDP per capita of $20,000?\nbroom::tidy(m) makes this model object a lot easier (tidier) to work with.\n\nm_res &lt;- tidy(m)\nm_res\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        32.8      1.75       18.8 4.18e-46\n2 log_gdp_per_cap     4.52     0.197      22.9 4.80e-58\n\n\nFirst, let’s pull out the estimated constant (or intercept or \\(\\beta_0\\)) for our calculations.\n\nbeta_0 &lt;- m_res |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nbeta_0\n\n[1] 32.79384\n\n\nNext, let’s pull out the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 &lt;- m_res |&gt; \n  filter(term == \"log_gdp_per_cap\") |&gt; \n  pull(estimate)\n\nbeta_1\n\n[1] 4.520601\n\n\nFinally, we can plug this in to our model:\n\\[\nlife Exp_i = \\beta_0 + \\beta_1 logGdpPerCap_i\n\\]\n\nlife_exp_20000 &lt;- beta_0 + beta_1 * log(20000)\nlife_exp_20000\n\n[1] 77.56355\n\n\nOn average, countries with GDPs per capita of $20,000 are predicted to have an average life expectancy of 78 years.\nLet’s take a look back at our data. Remember, these data describe what the World Bank actually observed for each country in 2016. How close is our predicted value to our observed values?\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 KN    KNA   St. Kitts and N… Latin…  2016      21388.     71.7            9.97\n2 SA    SAU   Saudi Arabia     Middl…  2016      21516.     77.1            9.98\n3 SI    SVN   Slovenia         Europ…  2016      21448.     81.2            9.97\n\n\nAs above, our data suggest that these countries have closer to an average of 77 years. Although our model predicted an average life expectancy closer to this than our guess above (which predicted 70 years), we still have a gap. Why?\nOur model is an attempt to formalize our understanding of the general relationship between a country’s wealth and health. Plotting our model against the observed values we used to generate it illustrates this point well:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(20000)) + \n  geom_hline(yintercept = life_exp_20000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nThe world is a complicated and messy place. There are many countries that have a GDP per capita of around $20,000 (those dots sitting around the vertical black line). They have a wide range of average life expectancy: look at their various placement along that vertical line. Some are higher than others.\nAlso, there are several countries with a wide range of logged GDP per capita that have an average life expectancy of 78 years (those sitting at or around the horizontal black line). These have a wide range of logged GDP per capita: some are further to the left than others.\nOur model is our best attempt at accounting for that diversity whilst still producing a useful summary of the relationship between health and wealth for those countries and all other countries with all observed values of GDP per capita.\nA bit of noise (or error) is expected. How much error is okay? This is a complicated question that has contested answers. Let’s start with actually measuring that error. Then we can chat about whether or not it’s small enough to allow us to be confident in our model.\nMeasuring error in our model\nReturning to our question above, how close are our predicted values to our observed values? For example, how far from the observed average life expectancy of countries with a GDP per capita of or close to $20,000 is 78 years?\nStart by working out the average life expectancy predicted by our model for the logged GDP per capita of all of our countries. We can then compare this to the average life expectancy actually observed in all these countries. We can predict values from a model using broom::augment():\n\naugment(m)\n\n# A tibble: 203 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1             75.6           10.2     79.0 -3.38  0.00999   4.10 0.00347  \n 2 2             63.1            6.26    61.1  2.05  0.0192    4.10 0.00252  \n 3 3             61.1            7.50    66.7 -5.61  0.00850   4.08 0.00812  \n 4 4             78.9            8.32    70.4  8.43  0.00533   4.06 0.0114   \n 5 6             79.3           10.6     80.8 -1.51  0.0132    4.10 0.000921 \n 6 7             76.3            9.45    75.5  0.797 0.00609   4.10 0.000117 \n 7 8             74.7            8.17    69.7  4.95  0.00569   4.09 0.00421  \n 8 10            78.2            9.71    76.7  1.44  0.00712   4.10 0.000448 \n 9 11            82.4           10.8     81.7  0.753 0.0149    4.10 0.000260 \n10 12            81.6           10.7     81.2  0.406 0.0140    4.10 0.0000706\n# ℹ 193 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\nThis function is simply fitting our model (\\(life Exp_i = 32.9 + 4.5 * logGdpPerCap_i\\)) to each country’s logged GDP per capita. You can confirm this by running the model yourself:\n\ngapminder_df |&gt; \n  transmute(country, log_gdp_per_cap, .fitted = beta_0 + beta_1*log_gdp_per_cap)\n\n# A tibble: 217 × 3\n   country              log_gdp_per_cap .fitted\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Aruba                          10.2     79.0\n 2 Afghanistan                     6.26    61.1\n 3 Angola                          7.50    66.7\n 4 Albania                         8.32    70.4\n 5 Andorra                        10.6     80.7\n 6 United Arab Emirates           10.6     80.8\n 7 Argentina                       9.45    75.5\n 8 Armenia                         8.17    69.7\n 9 American Samoa                  9.46    75.6\n10 Antigua and Barbuda             9.71    76.7\n# ℹ 207 more rows\n\n\nHow did the model do? What is the difference between what it predicted and the country’s observed average life expectancy? Compare .fitted (the predicted average life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval &lt;- augment(m) |&gt; \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 203 × 3\n   life_exp .fitted   diff\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.0 -3.38 \n 2     63.1    61.1  2.05 \n 3     61.1    66.7 -5.61 \n 4     78.9    70.4  8.43 \n 5     79.3    80.8 -1.51 \n 6     76.3    75.5  0.797\n 7     74.7    69.7  4.95 \n 8     78.2    76.7  1.44 \n 9     82.4    81.7  0.753\n10     81.6    81.2  0.406\n# ℹ 193 more rows\n\n\nNote that broom::augment() already did this calculation and stored it in the .resid variable. The formal term for the difference between the predicted and observed values is the residual.\n\naugment(m) |&gt; \n  select(life_exp, .fitted, .resid)\n\n# A tibble: 203 × 3\n   life_exp .fitted .resid\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.0 -3.38 \n 2     63.1    61.1  2.05 \n 3     61.1    66.7 -5.61 \n 4     78.9    70.4  8.43 \n 5     79.3    80.8 -1.51 \n 6     76.3    75.5  0.797\n 7     74.7    69.7  4.95 \n 8     78.2    76.7  1.44 \n 9     82.4    81.7  0.753\n10     81.6    81.2  0.406\n# ℹ 193 more rows\n\n\nOkay, so there are some differences. Let’s look at those differences a bit more closely:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.\nOur model hasn’t predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country’s true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).\nCan you see for which points these large differences exist?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nWhat is causing these differences? A lot of your work as a political scientist is trying to answer this very question!\n(Random) error\nThe world is a messy and complicated place. Things often vary in random ways. That’s okay! It means that your observational data are going to move in funny and random ways. That’s okay too! As long as your model includes all of the systematic drivers of the thing you are interested in measuring (such as average life expectancy), we can accept a bit of random error.\nIn fact, we have already accounted for this. Remember that error term:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nWe run into issues when there are non-random things bundled up into the difference between what our model predicts and what we actually observe. We will discuss this more in later sessions.\nA model-wide value for error\nWe often want to understand how the model has performed as a whole, rather than how well it predicts each individual observed data point. There are many different ways we can do this.\nSum of squared residuals (deviance)\nThe sum of squared residuals measures the total error in our model. Formally:\n\\[\n\\Sigma(y_i - \\hat{y_i})^2\n\\]\nWhere \\(y_i\\) is each observed value (the country’s actual average life expectancy) and \\(\\hat{y_i}\\) is each predicted value (the model’s estimate of country’s average life expectancy). We just add those all up to get a single measure of the model’s overall performance.\nRemember that we tend to square things when we don’t care about their direction. We don’t care that the predicted value is less or more than the observed value, just about how far they are from each other.\nWe can do this ourselves:\n\naugment(m) |&gt; \n  summarise(sum(.resid^2))\n\n# A tibble: 1 × 1\n  `sum(.resid^2)`\n            &lt;dbl&gt;\n1           3366.\n\n\nOr we can use broom::glance():\n\nglance(m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.724         0.722  4.09      526. 4.80e-58     1  -573. 1152. 1162.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(m) |&gt; \n  select(deviance)\n\n# A tibble: 1 × 1\n  deviance\n     &lt;dbl&gt;\n1    3366.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhere broom::tidy() gives us information about the coefficients of our model, broom::glance() gives us information on the model overall.\n\n\nThis is useful, but it is influenced by the units by which we measure our variables. If one model includes something like GDP which is includes values in the billions, we will get a very large sum of squared residuals. If another model includes something like percentage of as state’s citizens who will vote for Donald Trump, we will get a relatively small sum of squared residuals. What if we want to compare model performance in a meaningful way?\n\\(R^2\\)\nThe \\(R^2\\) value measures the amount of variation in the dependent variable that is explained by the independent variable. In our example, it measures how much the changes in countries’ average life expectancy is explained by the changes in their (logged) GDP per capita.\n\\[\nR^2 = 1 - \\frac{Unexplained\\ variation}{Total\\ variation}\n\\]\nThe \\(R^2\\) value is useful because it does not reflect the units of measurement used in our variables. Therefore, we can compare how well different models perform.\nThe \\(R^2\\) value has three component parts.\nTotal Sum of Squares (TSS)\nTSS measures the squared sum of the differences between all predicted values of the dependent variable and the mean of the dependent variable.\nExplained Sum of Squares (ESS)\nESS measures the sum of the squares of the deviations of the predicted values from the mean value of the dependent variable.\nResidual Sum of Squares (RSS)\nRSS measures the difference between the TSS and ESS. In other words, the error not explained by the model.\nFormally, the \\(R^2\\) value is:\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\Sigma(y_i - \\hat{y_i})^2}{\\Sigma(y_i - \\hat{y})^2}\n\\]\nOr:\n\\[\nR^2 = \\frac{ESS}{TSS} = \\frac{\\Sigma(\\hat{y}_i - \\bar{y})^2}{\\Sigma(y_i - \\bar{y})^2}\n\\]\nOur model’s \\(R^2\\) can be accessed using broom::glance():\n\nglance(m) |&gt; \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.724\n\n\nAn \\(R^2\\) of one means that all changes in the dependent variable are completely explained by changes in the independent variable. Here, it would mean that all changes to a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.\nAccording to our model, 72.4% of changes in a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.",
    "crumbs": [
      "Content",
      "Session 4",
      "Exploring the Relationship Between Two Variables"
    ]
  },
  {
    "objectID": "content/04-merging.html",
    "href": "content/04-merging.html",
    "title": "Data Merging",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"nycflights13\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Content",
      "Session 4",
      "Data Merging"
    ]
  },
  {
    "objectID": "content/04-merging.html#set-up",
    "href": "content/04-merging.html#set-up",
    "title": "Data Merging",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"nycflights13\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Content",
      "Session 4",
      "Data Merging"
    ]
  },
  {
    "objectID": "content/04-merging.html#introduction",
    "href": "content/04-merging.html#introduction",
    "title": "Data Merging",
    "section": "Introduction",
    "text": "Introduction\nWe often need to append or join multiple data sets to one another. Perhaps you got data from multiple different sources and you want to run analyses across them. Or you collected your data across different time periods and want to join them all together for your research. In this session, we will learn some helpful tools for doing this common task.\nLet’s get started!",
    "crumbs": [
      "Content",
      "Session 4",
      "Data Merging"
    ]
  },
  {
    "objectID": "content/04-merging.html#keys",
    "href": "content/04-merging.html#keys",
    "title": "Data Merging",
    "section": "Keys",
    "text": "Keys\nTo join multiple tables together, you need a key. These are common values across the two data sets that allow them to be connected.\nA primary key uniquely identifies each observation within your data set. For example, each airline in the airlines data set in the nycflights13 R package can be identified by its two letter carrier code, which is stored in the carrier variable:\n\nairlines\n\n# A tibble: 16 × 2\n   carrier name                       \n   &lt;chr&gt;   &lt;chr&gt;                      \n 1 9E      Endeavor Air Inc.          \n 2 AA      American Airlines Inc.     \n 3 AS      Alaska Airlines Inc.       \n 4 B6      JetBlue Airways            \n 5 DL      Delta Air Lines Inc.       \n 6 EV      ExpressJet Airlines Inc.   \n 7 F9      Frontier Airlines Inc.     \n 8 FL      AirTran Airways Corporation\n 9 HA      Hawaiian Airlines Inc.     \n10 MQ      Envoy Air                  \n11 OO      SkyWest Airlines Inc.      \n12 UA      United Air Lines Inc.      \n13 US      US Airways Inc.            \n14 VX      Virgin America             \n15 WN      Southwest Airlines Co.     \n16 YV      Mesa Airlines Inc.         \n\n\nSometimes you need to use multiple variables to uniquely identify your observations. This is particularly common when using time-series data (for example, country-year, or leader-year). For example, observations in the weather data set, which provides information about the hourly weather across New York City’s airports, are uniquely identified by the airport (origin) and time the weather was recorded (time_hour):\n\nweather\n\n# A tibble: 26,115 × 15\n   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4 \n 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06\n 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5 \n 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7 \n 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7 \n 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5 \n 7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0 \n 8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4 \n 9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0 \n10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8 \n# ℹ 26,105 more rows\n# ℹ 5 more variables: wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;,\n#   visib &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWe can verify that each observation has a unique value as its primary key using count() and filter():\n\nairlines |&gt; \n  count(carrier) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: carrier &lt;chr&gt;, n &lt;int&gt;\n\nweather |&gt; \n  count(time_hour, origin) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: time_hour &lt;dttm&gt;, origin &lt;chr&gt;, n &lt;int&gt;\n\n\nIf you know that each row of your data set is a unique observation, you can create your own primary key using its row number:\n\nairlines |&gt; \n  mutate(id = row_number(), .before = 1)\n\n# A tibble: 16 × 3\n      id carrier name                       \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                      \n 1     1 9E      Endeavor Air Inc.          \n 2     2 AA      American Airlines Inc.     \n 3     3 AS      Alaska Airlines Inc.       \n 4     4 B6      JetBlue Airways            \n 5     5 DL      Delta Air Lines Inc.       \n 6     6 EV      ExpressJet Airlines Inc.   \n 7     7 F9      Frontier Airlines Inc.     \n 8     8 FL      AirTran Airways Corporation\n 9     9 HA      Hawaiian Airlines Inc.     \n10    10 MQ      Envoy Air                  \n11    11 OO      SkyWest Airlines Inc.      \n12    12 UA      United Air Lines Inc.      \n13    13 US      US Airways Inc.            \n14    14 VX      Virgin America             \n15    15 WN      Southwest Airlines Co.     \n16    16 YV      Mesa Airlines Inc.         \n\n\n\n\n\n\n\n\nTip\n\n\n\nBy default, if you create a new column using mutate() that column will be placed at the end of the tibble. We can use the .before argument to control where the new column goes. Here I have asked mutate() to place it before the first column (making it the first column).",
    "crumbs": [
      "Content",
      "Session 4",
      "Data Merging"
    ]
  },
  {
    "objectID": "content/04-merging.html#basic-joins",
    "href": "content/04-merging.html#basic-joins",
    "title": "Data Merging",
    "section": "Basic joins",
    "text": "Basic joins\nPrimary keys serve as your connection to foreign keys, which are the variables that allow you to uniquely identify observations within the data set you want to join to your original data set. For example, flights$carrier is a foreign key that connects to the primary key airlines$carrier.\nYou can join data sets using their primary and foreign keys using one of six join functions provided in the dplyr R package (which is loaded in with tidyverse). These are: left_join(), inner_join(), right_join(), full_join(), semi_join(), and anti_join(). All of these functions take two data frames (which I will refer to as primary and foreign) and return a data frame.\nMutating joins\nMutating joins create new columns by appending information in foreign to primary according to the key and join function you use. There are four mutating join functions: left_join(), right_join(), inner_join(), and full_join().\nleft_join() matches observations in primary and foreign by their keys, then copies all other variables in foreign to primary. The resulting data frame will always have the same number of rows as primary. For example, let’s join flights (which provides information on all flights that departed NYC airports in 2013) with airlines:\n\nflights |&gt; \n  left_join(airlines)\n\n# A tibble: 336,776 × 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;\n\n\nWhen you run this in RStudio you will see the following message printed in your console:\nJoining with `by = join_by(carrier)`\nThis is letting you know the key left_join() used to match observations in flights with those in airlines. It guessed correctly: we wanted to match these data sets by airline. However, we should not rely on R to guess correctly every time. This can lead to incorrect matches, which can mess up your analysis down the line. Instead, you should always specify the key yourself using the by argument.\n\nflights |&gt; \n  left_join(airlines, by = join_by(carrier))\n\n# A tibble: 336,776 × 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;\n\n\nHere is a visualization of these joins:\n\n\nSource: R4DS\n\nIf left_join() cannot find a match for a primary key it will fill the values in the new columns created by copying foreign over with NA. For example,\n\nflights |&gt; \n  left_join(planes, by = join_by(tailnum, year)) |&gt; \n  filter(tailnum == \"N3ALAA\") |&gt; \n  select(tailnum, year, flight, origin, type:engine)\n\n# A tibble: 63 × 11\n   tailnum  year flight origin type  manufacturer model engines seats speed\n   &lt;chr&gt;   &lt;int&gt;  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 N3ALAA   2013    301 LGA    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 2 N3ALAA   2013    353 LGA    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 3 N3ALAA   2013    301 LGA    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 4 N3ALAA   2013    359 LGA    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 5 N3ALAA   2013   1351 JFK    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 6 N3ALAA   2013    301 LGA    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 7 N3ALAA   2013    353 LGA    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 8 N3ALAA   2013   1351 JFK    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n 9 N3ALAA   2013   1205 EWR    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n10 N3ALAA   2013     84 JFK    &lt;NA&gt;  &lt;NA&gt;         &lt;NA&gt;       NA    NA    NA\n# ℹ 53 more rows\n# ℹ 1 more variable: engine &lt;chr&gt;\n\n\ninner_join(), right_join(), and full_join() all work similarly to left_join(). They differ on which rows they keep. left_join() kept the rows in primary (the first, or left-most data frame). right_join() keeps the rows in foreign (the second, or right-most data frame). For example:\n\nairports |&gt; \n  right_join(weather, by = join_by(faa == origin))\n\n# A tibble: 26,115 × 22\n   faa   name    lat   lon   alt    tz dst   tzone  year month   day  hour  temp\n   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     1  39.0\n 2 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     2  39.0\n 3 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     3  39.0\n 4 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     4  39.9\n 5 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     5  39.0\n 6 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     6  37.9\n 7 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     7  39.0\n 8 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     8  39.9\n 9 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1     9  39.9\n10 EWR   Newa…  40.7 -74.2    18    -5 A     Amer…  2013     1     1    10  41  \n# ℹ 26,105 more rows\n# ℹ 9 more variables: dewp &lt;dbl&gt;, humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;,\n#   wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;,\n#   visib &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe column containing the airport code was called faa in the airports data frame and origin in the weather data frame. We can tell the *_join() functions that these are the keys using the by argument as formatted above.\n\n\nfull_join() keeps all rows in both primary and foreign. inner_join(), on the other hand, only keeps those rows that can be matched between primary and foreign.\nFiltering joins\nFiltering joins filter rows! There are two types of these join: semi- and anti-joins.\nsemi_join() keeps all the rows in primary that have a match in foreign. For example, the airports data set contains information on all airports in the US, but the flights data set only includes information on NYC airports. We can filter the airports data set to only include information on airports included in our foreign data frame (flights) using semi_join():\n\nairports |&gt; \n  semi_join(flights, by = join_by(faa == origin))\n\n# A tibble: 3 × 8\n  faa   name                  lat   lon   alt    tz dst   tzone           \n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n\n\nHere is a visualization of this filtering process:\n\n\nSource: R4DS\n\nanti_join(), on the other hand, returns only the rows that do not have a match in foreign. For example, the following will return all airports except those included in the flights data frame:\n\nairports |&gt; \n  anti_join(flights, by = join_by(faa == origin))\n\n# A tibble: 1,455 × 8\n   faa   name                             lat    lon   alt    tz dst   tzone    \n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n 2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n 3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n 4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n 5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n 6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n 7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n 8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n 9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…\n# ℹ 1,445 more rows\n\n\nHere is a visualization of what is going on with anti_join():\n\n\nSource: R4DS",
    "crumbs": [
      "Content",
      "Session 4",
      "Data Merging"
    ]
  },
  {
    "objectID": "content/04-merging.html#cross-joins",
    "href": "content/04-merging.html#cross-joins",
    "title": "Data Merging",
    "section": "Cross joins",
    "text": "Cross joins\nAnother common joining task includes finding all combinations of variables. For example, I often need to build data sets that include information on all countries across some number of years. I like to create this “spine” independently of my data collection to make sure I am covering all country-years I expect to cover and not relying on my data source.\nFor example, I may want to look at delays across all airports in the US in the years 2018-2024. To build my spine, I would start with a list of all airports:\n\nairports\n\n# A tibble: 1,458 × 8\n   faa   name                             lat    lon   alt    tz dst   tzone    \n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n 2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n 3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n 4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n 5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n 6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n 7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n 8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n 9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…\n# ℹ 1,448 more rows\n\n\nNow, I want to create new rows for each airport and year between 2018 and 2024. For example, I want seven rows for Lansdowne Airport (Lansdowne Airport in 2018, Lansdowne Airport in 2019, etc.). To do this, I can use cross_join() to join airports to a new data frame I create with a row for each year in my scope:\n\nairports |&gt; \n  cross_join(tibble(year = 2018:2024)) |&gt; \n  relocate(year)\n\n# A tibble: 10,206 × 9\n    year faa   name                            lat   lon   alt    tz dst   tzone\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1  2018 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A     Amer…\n 2  2019 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A     Amer…\n 3  2020 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A     Amer…\n 4  2021 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A     Amer…\n 5  2022 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A     Amer…\n 6  2023 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A     Amer…\n 7  2024 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A     Amer…\n 8  2018 06A   Moton Field Municipal Airport  32.5 -85.7   264    -6 A     Amer…\n 9  2019 06A   Moton Field Municipal Airport  32.5 -85.7   264    -6 A     Amer…\n10  2020 06A   Moton Field Municipal Airport  32.5 -85.7   264    -6 A     Amer…\n# ℹ 10,196 more rows\n\n\nIf you have two vectors (not great, big data frames) that you want to find all combinations for, you can use crossing():\n\ncrossing(year = 2018:2024, faa = unique(airports$faa))\n\n# A tibble: 10,206 × 2\n    year faa  \n   &lt;int&gt; &lt;chr&gt;\n 1  2018 04G  \n 2  2018 06A  \n 3  2018 06C  \n 4  2018 06N  \n 5  2018 09J  \n 6  2018 0A9  \n 7  2018 0G6  \n 8  2018 0G7  \n 9  2018 0P2  \n10  2018 0S9  \n# ℹ 10,196 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nI will use crossing() to help build a 3D plot in the next part of this session: Estimating Causal Effects with Observational Data.",
    "crumbs": [
      "Content",
      "Session 4",
      "Data Merging"
    ]
  },
  {
    "objectID": "content/01-visualization.html",
    "href": "content/01-visualization.html",
    "title": "Introducing R Through Data Visualization",
    "section": "",
    "text": "Data visualization is one of the most powerful tools in a political scientist’s analytical toolkit. Well-designed visualizations make complex data easier to understand and help communicate findings effectively. In this chapter, we introduce ggplot2, a popular R package that follows the grammar of graphics, a structured approach to building plots. Instead of relying on pre-made chart templates, ggplot2 allows you to layer different elements to create customized and insightful visualizations.\nBy the end of this chapter, you will be able to create meaningful plots using real-world political science data. We will walk through the process of building a visualization step by step, using voter turnout data from the 2020 U.S. presidential election.\nLet’s get started!",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#introduction",
    "href": "content/01-visualization.html#introduction",
    "title": "Introducing R Through Data Visualization",
    "section": "",
    "text": "Data visualization is one of the most powerful tools in a political scientist’s analytical toolkit. Well-designed visualizations make complex data easier to understand and help communicate findings effectively. In this chapter, we introduce ggplot2, a popular R package that follows the grammar of graphics, a structured approach to building plots. Instead of relying on pre-made chart templates, ggplot2 allows you to layer different elements to create customized and insightful visualizations.\nBy the end of this chapter, you will be able to create meaningful plots using real-world political science data. We will walk through the process of building a visualization step by step, using voter turnout data from the 2020 U.S. presidential election.\nLet’s get started!",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#set-up",
    "href": "content/01-visualization.html#set-up",
    "title": "Introducing R Through Data Visualization",
    "section": "Set up",
    "text": "Set up\nBefore we begin, ensure you have installed and loaded the necessary R packages:\n\ninstall.packages(c(\"tidyverse\", \"MetBrewer\"))\n\nlibrary(tidyverse)\nlibrary(MetBrewer)\n\n\n\n\n\n\n\nTip\n\n\n\nYou only need to install an R package onto your computer once, but you need to load it (using library(\"package_name\")) every time you start a new R session.\n\n\nThe tidyverse package includes ggplot2, as well as other useful tools for working with data.",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#does-wealth-influence-voter-turnout",
    "href": "content/01-visualization.html#does-wealth-influence-voter-turnout",
    "title": "Introducing R Through Data Visualization",
    "section": "Does Wealth Influence Voter Turnout?",
    "text": "Does Wealth Influence Voter Turnout?\nIn the 2020 U.S. presidential election, voter turnout varied significantly across states. Some states saw record participation, while others lagged behind. What factors explain this variation? One common hypothesis is that wealthier citizens are more likely to vote.\nWe will investigate this by exploring the relationship between median household income and voter turnout rates across U.S. states.",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#data",
    "href": "content/01-visualization.html#data",
    "title": "Introducing R Through Data Visualization",
    "section": "Data",
    "text": "Data\nTo analyze this question, we use the turnout_df dataset, which contains information on voter turnout, median income, and regional classifications of U.S. states. To load it into your current R session, run the following:\n\nturnout_df &lt;- read_csv(\"https://gist.githubusercontent.com/hgoers/ecbe1f9659cefd38be799392e6f66a0f/raw/9666e887f0befd1f854388b29ef2cd1add6b03fe/turnout.csv\")\n\nThis data set provides us with information on all 51 US states and DC. Each row contains an observation, which is a set of information about each of our states. We have information about 8 variables, including the state’s name and its total population of citizens (in thousands).\nLet’s take a quick look at the data:\n\nglimpse(turnout_df)\n\nRows: 51\nColumns: 8\n$ state            &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"Californ…\n$ state_abb        &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\",…\n$ total_citizens   &lt;dbl&gt; 3716, 516, 5075, 2195, 25946, 4200, 2524, 722, 534, 1…\n$ total_registered &lt;dbl&gt; 2527, 383, 3878, 1361, 18001, 2993, 1850, 542, 464, 1…\n$ total_voted      &lt;dbl&gt; 2247, 330, 3649, 1186, 16893, 2837, 1681, 489, 448, 9…\n$ turnout          &lt;dbl&gt; 0.6046825, 0.6395349, 0.7190148, 0.5403189, 0.6510830…\n$ southern         &lt;chr&gt; \"Southern\", \"Not Southern\", \"Not Southern\", \"Southern…\n$ median_income    &lt;dbl&gt; 51734, 75463, 62055, 48952, 80440, 77127, 78833, 7017…\n\n\nKey variables include:\n\n\nturnout: The proportion of adult citizens in a state that voted in the 2020 election.\n\nmedian_income: Median household income in the state.\n\nsouthern: A binary variable indicating whether the state is Southern or not.",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#ultimate-goal",
    "href": "content/01-visualization.html#ultimate-goal",
    "title": "Introducing R Through Data Visualization",
    "section": "Ultimate goal",
    "text": "Ultimate goal\nBy the end of this session, you will have created the following plot:\n\n\n\n\n\n\n\n\nAlong the way, you will learn more about R packages and functions, data types, vectors, and lists.",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#creating-a-ggplot",
    "href": "content/01-visualization.html#creating-a-ggplot",
    "title": "Introducing R Through Data Visualization",
    "section": "Creating a ggplot\n",
    "text": "Creating a ggplot\n\nWe start by creating an empty plot using the function ggplot():\n\nggplot()\n\n\n\n\n\n\n\nThis is the blank canvas upon which you will append layers. The first argument ggplot() takes is the data you want it to plot. We want to provide it with the turnout_df data set.\n\nggplot(data = turnout_df)\n\n\n\n\n\n\n\nNothing has happened visually to our plot, but we can now reference these data as we add layers to the plot.\nThe next argument ggplot() takes is mapping, which takes information on how our data should be represented on the plot. This argument takes a function, aes(). This function (which is short for aesthetics) specifies which variables will be mapped to which elements of the plot. Our scatter plot has two dimensions: x and y. Let’s tell ggplot() which variables we want in each of those dimensions:\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout))\n\n\n\n\n\n\n\nggplot() has added structure to the blank canvas. It has plotted out the range of observed values of each state’s median income on the x-axis and its voter turnout on the y-axis.\nNext, we need to add the states. To do this, we add geometric objects (geoms) to our plot. We want a scatter plot, so we will use geom_point(). Throughout this course you will see many of the different geoms_* available.\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point()\n\n\n\n\n\n\n\nThis is looking good! We can see each state’s median income and voter turnout in 2020. Importantly, we can see this information within the context of all other states’ median incomes and voter turnout rates. We can, therefore, start to answer our question: do states with wealthier citizens have higher turnout rates than states with poorer citizens?\nThis appears to be the case. The states with lower median incomes (sitting to the left of the plot) appear to have low turnout rates (sitting in the lower half of the plot) when compared to states with higher median incomes. Formally, we would say that these variables appear to be positively related to one another. Higher (lower) values of one variable tend to be associated with higher (lower) values of the other.",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#adding-aesthetics-and-layers",
    "href": "content/01-visualization.html#adding-aesthetics-and-layers",
    "title": "Introducing R Through Data Visualization",
    "section": "Adding aesthetics and layers",
    "text": "Adding aesthetics and layers\nWe can add more dimensions to our plot to communicate different groups in our data. For example, we may want to explore whether the relationship between wealth and turnout rates differs for Southern states and non-Southern states. To see this, I am going to colour in the points on the scatter plot based on whether they are Southern states.\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout, colour = southern)) +\n  geom_point()\n\n\n\n\n\n\n\nThe southern variable is categorical. ggplot() automatically assigns each unique category within this variable a value (here, a colour). It also adds a useful legend to the plot informing you of these values.\nWe can now add a new layer to our scatter plot that sums up the relationship between our two variables of interest more succinctly. We can plot the line of best fit (more on this later in the course), determined by a linear model, using geom_smooth() with its method argument set to \"lm\" (for linear model).\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout, colour = southern)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nWe have added two lines of best fit to our data: one fit against the Southern states and another fit using only the non-Southern states. From this we learn that despite Southern states tending to have both less wealthy citizens and lower turnout rates than their non-Southern counterparts, the relationship between wealth and turnout rates appears the same across these two groups. These lines have very similar slopes and are almost set at the same height. We can appropriately consolidate these two groups into one when discussing the relationship between wealth and turnout rates.\n\n\n\n\n\n\nNote\n\n\n\nWe will discuss how we prove formally that these two relationships are not substantively or statistically significantly different from one another during the course.\n\n\nHow do we do this in ggplot? The aesthetics we define in the ggplot() function are global. This means that they are passed down to every subsequent geom layer of the plot. Happily, each of these layers can take their own mapping arguments. These will override the mapping argument provided to ggplot(), if one exists.\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern)) + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nBecause we asked only the geom_point() layer to consider whether a state is Southern or not, we now have one line of best fit representing the relationship between our variables of interest across all 50 states and DC.",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#cleaning-up-our-plot",
    "href": "content/01-visualization.html#cleaning-up-our-plot",
    "title": "Introducing R Through Data Visualization",
    "section": "Cleaning up our plot",
    "text": "Cleaning up our plot\nWe are missing some important information on our plot. First, we need to provide a useful title for our viewers. We should also provide a brief summary of the relationship in our own words. It is also always good to include your data source (I got these data from the US Census). Finally, you should always use informative and tidy axis labels so your viewer is not confused as to what each variable measures. Let’s add all those labels to our plot using the labs() function:\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern)) + \n  geom_smooth(method = \"lm\") + \n  labs(title = \"Median income and voter turnout in the 2020 US Presidential election by state\", \n       subtitle = \"States with wealthier citizens tended to have higher turnout rates than states with poorer citizens\",\n       x = \"Median income\",\n       y = \"Voter turnout\",\n       colour = \"Region\",\n       caption = \"Source: US Census\")\n\n\n\n\n\n\n\nThat’s better, but our plot is still very cluttered. Let’s remove some of those extraneous frills. We can do this using the theme_*() set of functions. These provide you with some aesthetic settings that the authors of ggplot thought might be useful. For example, here are three different theme_*() functions from which you can select:\n\n\n\n\n\n\n\n\nI like theme_classic(), so let’s apply that to our plot:\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern)) + \n  geom_smooth(method = \"lm\") + \n  labs(title = \"Median income and voter turnout in the 2020 US Presidential election by state\", \n       subtitle = \"States with wealthier citizens tended to have higher turnout rates than states with poorer citizens\",\n       x = \"Median income\",\n       y = \"Voter turnout\",\n       colour = \"Region\",\n       caption = \"Source: US Census\") + \n  theme_classic()\n\n\n\n\n\n\n\nYou can adjust these pre-set aethetic choices using the theme() function. For example, I want to do the following to our chart:\n\nRemove the padding between the title/subtitle and the edge of the plot\nMake the title bold\nMove the legend to under the subtitle\n\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern)) + \n  geom_smooth(method = \"lm\") + \n  labs(title = \"Median income and voter turnout in the 2020 US Presidential election by state\", \n       subtitle = \"States with wealthier citizens tended to have higher turnout rates than states with poorer citizens\",\n       x = \"Median income\",\n       y = \"Voter turnout\",\n       colour = \"Region\",\n       caption = \"Source: US Census\") + \n  theme_classic() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\"),\n        legend.position = \"top\")\n\n\n\n\n\n\n\nBecause I included theme() after theme_classic() in my script, I overwrote its default arguments for plot.title.position, plot.title, and legend.position.\nNext, I want to include more informative tick marks. It would be good to include the unit for each, so the viewers are not left wondering. To do this we can use functions from the helpful scales R package in ggplot’s scale_*_continous() functions.\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern)) + \n  geom_smooth(method = \"lm\") + \n  labs(title = \"Median income and voter turnout in the 2020 US Presidential election by state\", \n       subtitle = \"States with wealthier citizens tended to have higher turnout rates than states with poorer citizens\",\n       x = \"Median income\",\n       y = \"Voter turnout\",\n       colour = \"Region\",\n       caption = \"Source: US Census\") + \n  theme_classic() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\"),\n        legend.position = \"top\") + \n  scale_x_continuous(labels = scales::dollar) + \n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\nI used scales::percent() to convert the proportions plotted on the y-axis to percentages and scales::dollar() to convert the median income values into more readable dollar values.\n\n\n\n\n\n\nTip\n\n\n\nYou can use a function from an R package without loading the whole package into your session by including the package name and two colons before the function. This is useful when you only want to use one or two functions from a package infrequently in your script. For example, I only need these two functions from the scales package once, so I prefer to call them directly than load in the whole package.\n\n\nNext, I would like to increase the size of the points so they are easier to see. I can do this by adjusting geom_points() size argument:\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern), size = 2) + \n  geom_smooth(method = \"lm\") + \n  labs(title = \"Median income and voter turnout in the 2020 US Presidential election by state\", \n       subtitle = \"States with wealthier citizens tended to have higher turnout rates than states with poorer citizens\",\n       x = \"Median income\",\n       y = \"Voter turnout\",\n       colour = \"Region\",\n       caption = \"Source: US Census\") + \n  theme_classic() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\"),\n        legend.position = \"top\") + \n  scale_x_continuous(labels = scales::dollar) + \n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\nWe’re almost there. As a final touch, I would like to use a more appealing colour palette than the default one. First, I will make the line of best fit grey so it doesn’t dominate the plot. I can do this using geom_smooth()’s colour argument:\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern), size = 2) + \n  geom_smooth(method = \"lm\", colour = \"darkgrey\") + \n  labs(title = \"Median income and voter turnout in the 2020 US Presidential election by state\", \n       subtitle = \"States with wealthier citizens tended to have higher turnout rates than states with poorer citizens\",\n       x = \"Median income\",\n       y = \"Voter turnout\",\n       colour = \"Region\",\n       caption = \"Source: US Census\") + \n  theme_classic() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\"),\n        legend.position = \"top\") + \n  scale_x_continuous(labels = scales::dollar) + \n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\nNext, I would like to change the colours of the points. I really like the palettes included in the MetBrewer R package. This package creates palettes based on paintings hanging in the Metropolitan Museum of Art.\n\n\n\n\n\n\nNote\n\n\n\nYou can see all the colour palettes included in the package here: https://github.com/BlakeRMills/MetBrewer.\n\n\nFor example, this palette comes from Vincent van Gogh’s Cypresses (1889):\n\nmet.brewer(name = \"VanGogh1\")\n\n\n\n\n\n\n\nAnd this colourblind-friendly palette comes from Carl Morgenstern’s Jungfrau, Mönch, and Eiger (1851):\n\nmet.brewer(name = \"Morgenstern\")\n\n\n\n\n\n\n\nLet’s use the beautiful colour palette generated from William Henry Johnson’s Jitterbugs V (1941–42). This palette has highly contrasting colours, which will make it easy for our viewer to distinguish between Southern and non-Southern dots.\n\nmet.brewer(name = \"Johnson\")\n\n\n\n\n\n\n\nTo manually select our colour palette, we need to use the scale_colour_manual() function:\n\nggplot(data = turnout_df, mapping = aes(x = median_income, y = turnout)) +\n  geom_point(aes(colour = southern), size = 2) + \n  geom_smooth(method = \"lm\", colour = \"darkgrey\") + \n  labs(title = \"Median income and voter turnout in the 2020 US Presidential election by state\", \n       subtitle = \"States with wealthier citizens tended to have higher turnout rates than states with poorer citizens\",\n       x = \"Median income\",\n       y = \"Voter turnout\",\n       colour = \"Region\",\n       caption = \"Source: US Census\") + \n  theme_classic() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\"),\n        legend.position = \"top\") + \n  scale_x_continuous(labels = scales::dollar) + \n  scale_y_continuous(labels = scales::percent) + \n  scale_colour_manual(values = met.brewer(name = \"Johnson\")[c(1, 5)])\n\n\n\n\n\n\n\nHuh, what’s with those numbers and brackets? The values argument of scale_colour_manual() takes a vector of hex values representing your selected colours. A hex value is a unique value assigned to a colour. You will see the ones provided in the Johnson palette shortly.\nA vector is a collection of values of the same type. For example, here is the vector representing our colour palette:\n\n\n[1] \"#a00e00\" \"#d04e00\" \"#f6c200\" \"#0086a8\" \"#132b69\"\n\n\nThese are all characters (think: words, or letters). Other data types include integers, logical values, and factors. You will be introduced to these throughout the course.\nI want to use the two colours at opposing ends of this palette so they are easy to distinguish from each other. I, therefore, need to grab the first value, #a00e00, and the fifth value, #132b69. I can do this using those square brackets and providing the value’s position. For example, here is the code to grab the second value:\n\nmet.brewer(name = \"Johnson\")[2]\n\n[1] \"#d04e00\"\n\n\nAnd third value:\n\nmet.brewer(name = \"Johnson\")[3]\n\n[1] \"#f6c200\"\n\n\nTo grab two values, you need to provide their positions within a vector, which is created using the code c(). For example, I include the two values in my vector using the following:\n\nc(2, 3)\n\n[1] 2 3\n\n\nWhich I can then include in the brackets to get their corresponding colour values:\n\nmet.brewer(name = \"Johnson\")[c(2, 3)]\n\n[1] \"#d04e00\" \"#f6c200\"\n\n\nPutting this all together, I can supply scale_colour_manual() with the values I would like to use from the Johnson palette. Pretty cool!",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#answering-the-question",
    "href": "content/01-visualization.html#answering-the-question",
    "title": "Introducing R Through Data Visualization",
    "section": "Answering the question",
    "text": "Answering the question\nWe now have a clean and engaging visualization of the relationship between a state’s median income and its voter turnout rates. It shows that this relationship is positive: states with higher median incomes tended to have higher turnout rates than states with lower median incomes.\n\n\n\n\n\n\n\n\nCan we learn anything more from this relationship? For example, do you think that we can definitively say that states that increase their median incomes will see higher turnout rates at the next Presidential election? Or do you think something else might be going on? Perhaps individuals with higher levels of education tend to earn and vote more than those with less education. Might that be the decisive factor lurking behind this relationship? This course will provide you with the skills to use data to wade through these murky and complex relationships. Your first step should always be to visualize your data!",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/01-visualization.html#summary",
    "href": "content/01-visualization.html#summary",
    "title": "Introducing R Through Data Visualization",
    "section": "Summary",
    "text": "Summary\n\nggplot2 allows you to build custom visualizations by layering different components.\nWe explored voter turnout and income, finding an apparent positive relationship.\nAdding colors, labels, and trend lines enhances clarity and insight.\nFuture sessions will explore more advanced visualizations and data transformations.",
    "crumbs": [
      "Content",
      "Session 1",
      "Introducing R Through Data Visualization"
    ]
  },
  {
    "objectID": "content/slides/02-01-transformation_intro.html#learning-objectives",
    "href": "content/slides/02-01-transformation_intro.html#learning-objectives",
    "title": "Data Transformation",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nLearn basic operations in R\nBe introduced to dplyr\nClean up and transform your data"
  },
  {
    "objectID": "content/slides/02-01-transformation_intro.html#r-objects",
    "href": "content/slides/02-01-transformation_intro.html#r-objects",
    "title": "Data Transformation",
    "section": "R objects",
    "text": "R objects\nCreate new objects with &lt;-\n\nx &lt;- 3 * 4\n\nx\n\n[1] 12\n\n\n\n\nx &lt;- 3 * 10\n\nx\n\n[1] 30\n\n\n\n\nSome people use = instead of &lt;-. I strongly recommend against this. It makes your script difficult to read, and it can lead to syntax errors."
  },
  {
    "objectID": "content/slides/02-01-transformation_intro.html#r-functions",
    "href": "content/slides/02-01-transformation_intro.html#r-functions",
    "title": "Data Transformation",
    "section": "R functions",
    "text": "R functions\nMany functions come with R straight out of the box:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nYou can create objects using functions:\n\nx &lt;- seq(1, 10)\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "content/slides/02-01-transformation_intro.html#gapminder",
    "href": "content/slides/02-01-transformation_intro.html#gapminder",
    "title": "Data Transformation",
    "section": "Gapminder",
    "text": "Gapminder\nFirst, you need to install the gapminder package:\n\ninstall.packages(\"gapminder\")\n\nThen access the gapminder data set:\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nhead(gapminder)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786."
  },
  {
    "objectID": "content/slides/02-01-transformation_intro.html#data-types",
    "href": "content/slides/02-01-transformation_intro.html#data-types",
    "title": "Data Transformation",
    "section": "Data types",
    "text": "Data types\nIn gapminder:\n\nfctr stands for factors, which R uses to represent categorical variables with fixed possible values.\nint stands for integer.\ndbl stands for doubles (or real numbers)."
  },
  {
    "objectID": "content/slides/02-01-transformation_intro.html#data-types-1",
    "href": "content/slides/02-01-transformation_intro.html#data-types-1",
    "title": "Data Transformation",
    "section": "Data types",
    "text": "Data types\nOther types:\n\nchr stands for character vectors, or strings.\ndttm stands for date-times (a date + a time).\nlgl stands for logical, vectors that contain only TRUE or FALSE.1\n\nTRUE can be shortened to T and FALSE can be shortened to F."
  },
  {
    "objectID": "content/slides/02-01-transformation_intro.html#introducing-dplyr",
    "href": "content/slides/02-01-transformation_intro.html#introducing-dplyr",
    "title": "Data Transformation",
    "section": "Introducing dplyr",
    "text": "Introducing dplyr\nHelp you with most of your data transformation needs.\nFive basic functions:\n\nfilter()\narrange()\nselect()\nmutate()\nsummarise()"
  },
  {
    "objectID": "content/slides/02-04-select.html#select-columns-with-select",
    "href": "content/slides/02-04-select.html#select-columns-with-select",
    "title": "Selecting Relevant Columns",
    "section": "Select columns with select()",
    "text": "Select columns with select()\n\nselect(gapminder, country, year, pop)\n\n# A tibble: 1,704 × 3\n   country      year      pop\n   &lt;fct&gt;       &lt;int&gt;    &lt;int&gt;\n 1 Afghanistan  1952  8425333\n 2 Afghanistan  1957  9240934\n 3 Afghanistan  1962 10267083\n 4 Afghanistan  1967 11537966\n 5 Afghanistan  1972 13079460\n 6 Afghanistan  1977 14880372\n 7 Afghanistan  1982 12881816\n 8 Afghanistan  1987 13867957\n 9 Afghanistan  1992 16317921\n10 Afghanistan  1997 22227415\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-04-select.html#select-columns-with-select-1",
    "href": "content/slides/02-04-select.html#select-columns-with-select-1",
    "title": "Selecting Relevant Columns",
    "section": "Select columns with select()",
    "text": "Select columns with select()\n\nselect(gapminder, country:pop)\n\n# A tibble: 1,704 × 5\n   country     continent  year lifeExp      pop\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;\n 1 Afghanistan Asia       1952    28.8  8425333\n 2 Afghanistan Asia       1957    30.3  9240934\n 3 Afghanistan Asia       1962    32.0 10267083\n 4 Afghanistan Asia       1967    34.0 11537966\n 5 Afghanistan Asia       1972    36.1 13079460\n 6 Afghanistan Asia       1977    38.4 14880372\n 7 Afghanistan Asia       1982    39.9 12881816\n 8 Afghanistan Asia       1987    40.8 13867957\n 9 Afghanistan Asia       1992    41.7 16317921\n10 Afghanistan Asia       1997    41.8 22227415\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-04-select.html#select-columns-with-select-2",
    "href": "content/slides/02-04-select.html#select-columns-with-select-2",
    "title": "Selecting Relevant Columns",
    "section": "Select columns with select()",
    "text": "Select columns with select()\n\nselect(gapminder, -(lifeExp:pop))\n\n# A tibble: 1,704 × 4\n   country     continent  year gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952      779.\n 2 Afghanistan Asia       1957      821.\n 3 Afghanistan Asia       1962      853.\n 4 Afghanistan Asia       1967      836.\n 5 Afghanistan Asia       1972      740.\n 6 Afghanistan Asia       1977      786.\n 7 Afghanistan Asia       1982      978.\n 8 Afghanistan Asia       1987      852.\n 9 Afghanistan Asia       1992      649.\n10 Afghanistan Asia       1997      635.\n# ℹ 1,694 more rows"
  },
  {
    "objectID": "content/slides/02-07-pipe.html#combine-multiple-operations-with-the-pipe",
    "href": "content/slides/02-07-pipe.html#combine-multiple-operations-with-the-pipe",
    "title": "The pipe",
    "section": "Combine multiple operations with the pipe",
    "text": "Combine multiple operations with the pipe\nThat got messy!\n\nWe had a lot of different objects representing intermediate steps in our calculations.\nWe never need those objects again. Can we avoid creating them?\n\nLet’s introduce perhaps the defining feature of the tidyverse: the pipe."
  },
  {
    "objectID": "content/slides/02-07-pipe.html#combine-multiple-operations-with-the-pipe-1",
    "href": "content/slides/02-07-pipe.html#combine-multiple-operations-with-the-pipe-1",
    "title": "The pipe",
    "section": "Combine multiple operations with the pipe",
    "text": "Combine multiple operations with the pipe\nRead the pipe as:\nTake this |&gt; (and then…)      do this |&gt; (and then…)      do this\n\ngapminder |&gt; \n  group_by(continent) |&gt; \n  summarise(avg_pop = mean(pop), avg_gdp_per_cap = mean(gdpPercap)) |&gt; \n  arrange(avg_gdp_per_cap)\n\n# A tibble: 5 × 3\n  continent   avg_pop avg_gdp_per_cap\n  &lt;fct&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1 Africa     9916003.           2194.\n2 Americas  24504795.           7136.\n3 Asia      77038722.           7902.\n4 Europe    17169765.          14469.\n5 Oceania    8874672.          18622."
  },
  {
    "objectID": "content/slides/02-07-pipe.html#combine-multiple-operations-with-the-pipe-2",
    "href": "content/slides/02-07-pipe.html#combine-multiple-operations-with-the-pipe-2",
    "title": "The pipe",
    "section": "Combine multiple operations with the pipe",
    "text": "Combine multiple operations with the pipe\n\ngapminder |&gt; \n  group_by(continent) |&gt; \n  summarise(avg_pop = mean(pop), avg_gdp_per_cap = mean(gdpPercap)) |&gt; \n  ggplot(aes(x = continent, y = avg_gdp_per_cap)) + \n  geom_col() + \n  theme_minimal()"
  },
  {
    "objectID": "content/slides/02-07-pipe.html#a-note-on-the-pipe",
    "href": "content/slides/02-07-pipe.html#a-note-on-the-pipe",
    "title": "The pipe",
    "section": "A note on the pipe",
    "text": "A note on the pipe\n\n\nBase pipe:\n\n|&gt;\nCan be used without loading any packages\nRelatively new: introduced in 2021\n\n\nTidyverse pipe:\n\n%&gt;%\nMust load dplyr or magrittr to use"
  },
  {
    "objectID": "content/slides/02-07-pipe.html#summary",
    "href": "content/slides/02-07-pipe.html#summary",
    "title": "The pipe",
    "section": "Summary",
    "text": "Summary\nThis session you have:\n\nLearnt R basic syntax\nLearnt how to transform your data\nWritten concise code that is easy to follow"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#surveys",
    "href": "content/slides/02-09-surveys.html#surveys",
    "title": "From Surveys to Populations",
    "section": "Surveys",
    "text": "Surveys\n\nPopulations are very difficult to collect data on\n\nEven the census misses people!\n\nHappily, we can use surveys of a sample of our population to learn things about our population\nHowever, our ability to do this is conditional on how good our sample is\nWhat do I mean by “good”?"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#the-2024-us-presidential-election",
    "href": "content/slides/02-09-surveys.html#the-2024-us-presidential-election",
    "title": "From Surveys to Populations",
    "section": "The 2024 US Presidential Election",
    "text": "The 2024 US Presidential Election\n\nElections are preceded by a flood of surveys"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#surveys-1",
    "href": "content/slides/02-09-surveys.html#surveys-1",
    "title": "From Surveys to Populations",
    "section": "Surveys",
    "text": "Surveys\n\nSurveys are conducted on a subset (sample) of the population of interest\nOur population of interest: individuals who voted in the 2024 US Presidential Election"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#a-good-sample",
    "href": "content/slides/02-09-surveys.html#a-good-sample",
    "title": "From Surveys to Populations",
    "section": "A good sample",
    "text": "A good sample\n\nA good sample is a representative one\nHow closely does our sample reflect our population"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#parallel-worlds",
    "href": "content/slides/02-09-surveys.html#parallel-worlds",
    "title": "From Surveys to Populations",
    "section": "Parallel worlds",
    "text": "Parallel worlds\n\nRemember back to last session on experiments\nIn an ideal world, we would be able to create two parallel worlds (one with the treatment, one held as our control)\n\nOne version of the election booth run without monitors (the control)\nOne version with monitors (the treatment)\n\nThese worlds are perfectly identical to each other prior to treatment\nWe cannot do this :("
  },
  {
    "objectID": "content/slides/02-09-surveys.html#the-next-best-thing",
    "href": "content/slides/02-09-surveys.html#the-next-best-thing",
    "title": "From Surveys to Populations",
    "section": "The next best thing",
    "text": "The next best thing\n\nOur next best option is to create two groups that were as identical to one another as possible prior to treatment\nIf they are (almost) identical, differences between their group-wide outcomes can be attributed to the treatment\nOne good way of getting two (almost) identical groups is to assign individuals to those groups randomly\n\nThink back to our 1,000 hypothetical people!"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#randomization",
    "href": "content/slides/02-09-surveys.html#randomization",
    "title": "From Surveys to Populations",
    "section": "Randomization",
    "text": "Randomization\n\nRandomization continues to pop its chaotic head up\nWe can use it to create a sample that is (almost) identical to our population, on average\nDrawing randomly from our population increases our chances of ending up with a sample that reflects that population\nThis would be referred to as a representative sample"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#random-sampling",
    "href": "content/slides/02-09-surveys.html#random-sampling",
    "title": "From Surveys to Populations",
    "section": "Random sampling",
    "text": "Random sampling\n\nAll individuals in the population need to have an equal chance of being selected for the sample\n\nIf this holds, you have a pure random sample\n\nThis is really hard to do!\n\nHow likely were you to answer the pollster’s unknown number, calling you in the middle of the day?\nEven if you did answer, how likely were you to answer all their questions?"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#to-illustrate",
    "href": "content/slides/02-09-surveys.html#to-illustrate",
    "title": "From Surveys to Populations",
    "section": "To illustrate",
    "text": "To illustrate\nCountries’ GDP in 2022:"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#countries-gdp",
    "href": "content/slides/02-09-surveys.html#countries-gdp",
    "title": "From Surveys to Populations",
    "section": "Countries’ GDP",
    "text": "Countries’ GDP\nI want to estimate the average GDP across all countries in 2022.\n\nI send out a survey to all countries’ Departments of Statistics and ask for their GDP figures for 2022.\nI get 60 responses:\n\n\nsample_df &lt;- gdp_df |&gt; \n  drop_na(sample_value) |&gt; \n  sample_n(size = 60) |&gt; \n  transmute(country, gdp = sample_value)\n\nsample_df\n\n# A tibble: 60 × 2\n   country                 gdp\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Austria             4.72e11\n 2 Angola              1.04e11\n 3 Moldova             1.45e10\n 4 United States       2.60e13\n 5 Maldives            6.18e 9\n 6 St. Kitts and Nevis 9.81e 8\n 7 Kuwait              1.84e11\n 8 Channel Islands     1.13e10\n 9 Malaysia            4.08e11\n10 Brunei Darussalam   1.67e10\n# ℹ 50 more rows"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#countries-gdp-1",
    "href": "content/slides/02-09-surveys.html#countries-gdp-1",
    "title": "From Surveys to Populations",
    "section": "Countries’ GDP",
    "text": "Countries’ GDP\nI now calculate the average of these responses, which I find to be:\n\nsample_df |&gt; \n  summarise(avg_gdp = scales::dollar(mean(gdp, na.rm = T)))\n\n# A tibble: 1 × 1\n  avg_gdp         \n  &lt;chr&gt;           \n1 $702,270,176,629\n\n\nNow, imagine that we knew definitively that it was NA. Why such a large difference?"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#non-response-bias",
    "href": "content/slides/02-09-surveys.html#non-response-bias",
    "title": "From Surveys to Populations",
    "section": "Non-response bias",
    "text": "Non-response bias\nPoorer countries are far less likely to be able or willing to provide these economic data to academics or international organizations.\n\nThey tend to be underrepresented in a lot of data\n\nMy sample was biased against poorer countries.\n\nThey were not equally likely to respond to my request for data as rich countries"
  },
  {
    "objectID": "content/slides/02-09-surveys.html#large-numbers",
    "href": "content/slides/02-09-surveys.html#large-numbers",
    "title": "From Surveys to Populations",
    "section": "Large numbers",
    "text": "Large numbers\n\nRandomization isn’t enough: we also need to draw a sufficiently large sample from our population\n\nOne person pulled randomly from the class isn’t going to be very reflective of the class!"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#data-visualization",
    "href": "content/slides/01-05-personal_plots.html#data-visualization",
    "title": "Styling Your Plots",
    "section": "Data visualization",
    "text": "Data visualization\nWe will use data visualization to answer the following question:\n\nDo cars with big engines use more fuel than cars with small engines?"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#add-useful-titles-and-labels",
    "href": "content/slides/01-05-personal_plots.html#add-useful-titles-and-labels",
    "title": "Styling Your Plots",
    "section": "Add useful titles and labels",
    "text": "Add useful titles and labels\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(colour = class)) + \n  geom_smooth(method = \"lm\") + \n  labs(\n    title = \"Engine displacement and highway miles per gallon\",\n    subtitle = \"Values for seven different classes of cars\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway miles per gallon\"\n  )"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#add-useful-titles-and-labels-1",
    "href": "content/slides/01-05-personal_plots.html#add-useful-titles-and-labels-1",
    "title": "Styling Your Plots",
    "section": "Add useful titles and labels",
    "text": "Add useful titles and labels"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#flexible-visualization",
    "href": "content/slides/01-05-personal_plots.html#flexible-visualization",
    "title": "Styling Your Plots",
    "section": "Flexible visualization",
    "text": "Flexible visualization\nYou can use visual elements to communicate your findings in engaging ways.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class == \"2seater\")) + \n  geom_smooth(method = \"lm\") + \n  labs(\n    title = \"Engine displacement and highway miles per gallon\",\n    subtitle = \"Values for seven different classes of cars\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway miles per gallon\"\n  )"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#changing-the-look-of-your-plots",
    "href": "content/slides/01-05-personal_plots.html#changing-the-look-of-your-plots",
    "title": "Styling Your Plots",
    "section": "Changing the look of your plots",
    "text": "Changing the look of your plots\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(colour = \"pink\") + \n  geom_smooth(method = \"lm\") + \n  labs(\n    title = \"Engine displacement and highway miles per gallon\",\n    subtitle = \"Values for seven different classes of cars\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway miles per gallon\"\n  )"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#lets-clean-our-graph-up",
    "href": "content/slides/01-05-personal_plots.html#lets-clean-our-graph-up",
    "title": "Styling Your Plots",
    "section": "Let’s clean our graph up",
    "text": "Let’s clean our graph up\nLess is more when it comes to data visualization.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(colour = class)) + \n  geom_smooth(method = \"lm\") + \n  theme_minimal() + \n  labs(\n    title = \"Engine displacement and highway miles per gallon\",\n    subtitle = \"Values for seven different classes of cars\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway miles per gallon\"\n  ) \n\n\n\nAll pre-set ggplot themes can be found in the ggplot documentation."
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#lets-clean-this-up",
    "href": "content/slides/01-05-personal_plots.html#lets-clean-this-up",
    "title": "Styling Your Plots",
    "section": "Let’s clean this up",
    "text": "Let’s clean this up"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#creating-your-own-theme",
    "href": "content/slides/01-05-personal_plots.html#creating-your-own-theme",
    "title": "Styling Your Plots",
    "section": "Creating your own theme",
    "text": "Creating your own theme\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(colour = class)) + \n  geom_smooth(method = \"lm\") + \n  theme(\n    legend.position = \"bottom\",\n    panel.grid = element_blank(),\n    panel.background = element_blank(),\n    plot.title.position = \"plot\",\n    plot.title = element_text(face = \"bold\")\n  ) + \n  labs(\n    title = \"Engine displacement and highway miles per gallon\",\n    subtitle = \"Values for seven different classes of cars\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway miles per gallon\"\n  ) \n\n\n\nAll theme elements that you can control can be found in the ggplot documentation. There are a lot!"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#creating-your-own-theme-1",
    "href": "content/slides/01-05-personal_plots.html#creating-your-own-theme-1",
    "title": "Styling Your Plots",
    "section": "Creating your own theme",
    "text": "Creating your own theme"
  },
  {
    "objectID": "content/slides/01-05-personal_plots.html#the-before-shot",
    "href": "content/slides/01-05-personal_plots.html#the-before-shot",
    "title": "Styling Your Plots",
    "section": "The before shot",
    "text": "The before shot"
  },
  {
    "objectID": "content/slides/01-02-load_data.html#data-visualisation",
    "href": "content/slides/01-02-load_data.html#data-visualisation",
    "title": "Loading and Exploring Your Data",
    "section": "Data visualisation",
    "text": "Data visualisation\nWe will use data visualization to answer the following question:\n\nDo cars with big engines use more fuel than cars with small engines?"
  },
  {
    "objectID": "content/slides/01-02-load_data.html#load-relevant-packages",
    "href": "content/slides/01-02-load_data.html#load-relevant-packages",
    "title": "Loading and Exploring Your Data",
    "section": "Load relevant packages",
    "text": "Load relevant packages\n\n# Load relevant packages\nlibrary(tidyverse)"
  },
  {
    "objectID": "content/slides/01-02-load_data.html#load-in-relevant-data",
    "href": "content/slides/01-02-load_data.html#load-in-relevant-data",
    "title": "Loading and Exploring Your Data",
    "section": "Load in relevant data",
    "text": "Load in relevant data\n\n# Load the data\nmpg\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\n\n\n\n\naudi\na4\n1.8\n1999\n4\n\n\naudi\na4\n1.8\n1999\n4\n\n\naudi\na4\n2.0\n2008\n4\n\n\naudi\na4\n2.0\n2008\n4\n\n\naudi\na4\n2.8\n1999\n6\n\n\naudi\na4\n2.8\n1999\n6"
  },
  {
    "objectID": "content/slides/01-02-load_data.html#the-mpg-data-set",
    "href": "content/slides/01-02-load_data.html#the-mpg-data-set",
    "title": "Loading and Exploring Your Data",
    "section": "The mpg data set",
    "text": "The mpg data set\nA couple of useful variables:\n\ndispl: engine displacement, in liters\nhwy: highway miles per gallon"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#causes-and-effects",
    "href": "content/slides/01-07-experiments.html#causes-and-effects",
    "title": "Causal Effects and Experiments",
    "section": "Causes and effects",
    "text": "Causes and effects\nOur goal is to better understand what factors lead to certain outcomes of interest.\n\nDoes increasing the number of voting booths close to a potential voter make that person more likely to vote?\nDo peace treaties signed with factionalized rebel groups more often lead to a return to conflict than those signed with a single, cohesive group?\nDoes trade between two countries make war between them less likely?"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#causes-and-effects-1",
    "href": "content/slides/01-07-experiments.html#causes-and-effects-1",
    "title": "Causal Effects and Experiments",
    "section": "Causes and effects",
    "text": "Causes and effects\nThese are causal statements:\n\nMore local voting booths \\(\\rightarrow\\) More likely to vote\nMore factionalization \\(\\rightarrow\\) More likely to restart conflict\nTrade \\(\\rightarrow\\) Less likely to go to war"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#how-to-identify-causes-and-their-effects",
    "href": "content/slides/01-07-experiments.html#how-to-identify-causes-and-their-effects",
    "title": "Causal Effects and Experiments",
    "section": "How to identify causes and their effects",
    "text": "How to identify causes and their effects\nProving that changes to one factor cause changes to another is very tricky!\n\nNeed to account for all the other factors"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#the-efficacy-of-international-election-monitors",
    "href": "content/slides/01-07-experiments.html#the-efficacy-of-international-election-monitors",
    "title": "Causal Effects and Experiments",
    "section": "The efficacy of international election monitors",
    "text": "The efficacy of international election monitors\nDo international monitors cause less election-day fraud in democratic elections?\n\nNumber of elections monitored by international observers exploded throughout the 2000s\nBut do monitors increase the chances the election will be free and fair?\nIn other words, are they effective?\n\n\n\nHyde, Susan D. 2007. “The Observer Effect in International Politics: Evidence from a Natural Experiment.” World Politics 60 (1): 37–63. http://www.jstor.org/stable/40060180."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#the-efficacy-of-international-election-monitors-1",
    "href": "content/slides/01-07-experiments.html#the-efficacy-of-international-election-monitors-1",
    "title": "Causal Effects and Experiments",
    "section": "The efficacy of international election monitors",
    "text": "The efficacy of international election monitors\nDr Susan Hyde set out to answer this very question. From her article (page 39):\n\nIf the presence of international observers causes a reduction in election-day fraud, the effect of observers should be visible at the subnational level by comparing polling stations that were visited by observers with those that were not visited. More specifically, if international monitoring reduces electionday fraud directly, all else held equal, the cheating parties should gain less of their ill-gotten vote share in polling stations that were visited by international monitors."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#causal-relationships",
    "href": "content/slides/01-07-experiments.html#causal-relationships",
    "title": "Causal Effects and Experiments",
    "section": "Causal relationships",
    "text": "Causal relationships\nRefers to the directional connection between a change in one variable and a corresponding change in another.\n\nThe direction matters!\nTreatment variable: the variable causing changes to another variable.\nOutcome variable: the variable changing as a result of changes to another variable (the treatment)."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#treatment-and-outcome-variables",
    "href": "content/slides/01-07-experiments.html#treatment-and-outcome-variables",
    "title": "Causal Effects and Experiments",
    "section": "Treatment and outcome variables",
    "text": "Treatment and outcome variables\nWe want to test whether the presence of international monitors (treatment) leads to less election day fraud (outcome).\nInternational election monitors present at polling stations \\(\\rightarrow\\) Less election-day fraud at that station"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#treatment-variable",
    "href": "content/slides/01-07-experiments.html#treatment-variable",
    "title": "Causal Effects and Experiments",
    "section": "Treatment variable",
    "text": "Treatment variable\nAt any given polling station in any given election, monitors may be: 1) present, or 2) not present.\nTwo different conditions:\n\nTreatment: condition with treatment (monitors are present)\nControl: condition without treatment (monitors are not present)\n\n\n\nTreatment variables do not need to be binary."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#outcome-variable",
    "href": "content/slides/01-07-experiments.html#outcome-variable",
    "title": "Causal Effects and Experiments",
    "section": "Outcome variable",
    "text": "Outcome variable\nAt any given polling station in any given election, fraud may: 1) occur, or 2) not occur.\n\n\nOutcome variables also do not need to binary."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#observing-our-outcome",
    "href": "content/slides/01-07-experiments.html#observing-our-outcome",
    "title": "Causal Effects and Experiments",
    "section": "Observing our outcome",
    "text": "Observing our outcome\nSometimes it can be hard to observe our outcome of interest.\n\nHow might we see all election-day fraud?\nHow might we measure it?"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#observing-our-outcome-1",
    "href": "content/slides/01-07-experiments.html#observing-our-outcome-1",
    "title": "Causal Effects and Experiments",
    "section": "Observing our outcome",
    "text": "Observing our outcome\nHyde’s answer: vote-share!\nInternational election monitors present at polling stations \\(\\rightarrow\\) Less election-day fraud at that station \\(\\rightarrow\\) Lower vote-share for the cheating party"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#individual-causal-effects",
    "href": "content/slides/01-07-experiments.html#individual-causal-effects",
    "title": "Causal Effects and Experiments",
    "section": "Individual causal effects",
    "text": "Individual causal effects\nWe want to know whether the treatment causes a change in our outcome of interest.\nWhat might this look like in the real world?\n\nImagine we are looking at a specific election\nFive polling stations"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#parallel-worlds",
    "href": "content/slides/01-07-experiments.html#parallel-worlds",
    "title": "Causal Effects and Experiments",
    "section": "Parallel worlds",
    "text": "Parallel worlds"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#hypothetical-election",
    "href": "content/slides/01-07-experiments.html#hypothetical-election",
    "title": "Causal Effects and Experiments",
    "section": "Hypothetical election",
    "text": "Hypothetical election\n\n\n\n\n\nID\nMonitored vote %\nNon-monitored vote %\n\n\n\n\n1\n65.84\n91.20\n\n\n2\n34.66\n74.63\n\n\n3\n36.03\n89.75\n\n\n4\n55.74\n95.41\n\n\n5\n23.95\n93.30"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#individual-effects",
    "href": "content/slides/01-07-experiments.html#individual-effects",
    "title": "Causal Effects and Experiments",
    "section": "Individual effects",
    "text": "Individual effects\nThe only difference between these two conditions is the presence of international monitors!\n\nThe difference between vote shares under these conditions is caused by the monitors."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#individual-effects-1",
    "href": "content/slides/01-07-experiments.html#individual-effects-1",
    "title": "Causal Effects and Experiments",
    "section": "Individual effects",
    "text": "Individual effects\n\n\n\n\n\nID\nMonitored vote %\nNon-monitored vote %\nDifference (%)\n\n\n\n\n1\n65.84\n91.20\n-25.36\n\n\n2\n34.66\n74.63\n-39.97\n\n\n3\n36.03\n89.75\n-53.72\n\n\n4\n55.74\n95.41\n-39.67\n\n\n5\n23.95\n93.30\n-69.35"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#no-parallel-worlds",
    "href": "content/slides/01-07-experiments.html#no-parallel-worlds",
    "title": "Causal Effects and Experiments",
    "section": "No parallel worlds",
    "text": "No parallel worlds\nSadly for us, we cannot create parallel worlds…"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#what-now",
    "href": "content/slides/01-07-experiments.html#what-now",
    "title": "Causal Effects and Experiments",
    "section": "What now?",
    "text": "What now?\n\n\n\n\n\nID\nMonitored\nMonitored vote %\nNon-monitored vote %\nDifference\n\n\n\n\n1\n1\n65.84\nNA\nNA\n\n\n2\n1\n34.66\nNA\nNA\n\n\n3\n1\n36.03\nNA\nNA\n\n\n4\n1\n55.74\nNA\nNA\n\n\n5\n1\n23.95\nNA\nNA"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#average-causal-effects",
    "href": "content/slides/01-07-experiments.html#average-causal-effects",
    "title": "Causal Effects and Experiments",
    "section": "Average causal effects",
    "text": "Average causal effects\nWe need to move away from looking at individuals and start to look for patterns in our group."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#back-to-our-parallel-worlds",
    "href": "content/slides/01-07-experiments.html#back-to-our-parallel-worlds",
    "title": "Causal Effects and Experiments",
    "section": "Back to our parallel worlds",
    "text": "Back to our parallel worlds\n\n\n\n\n\nID\nMonitored vote %\nNon-monitored vote %\nDifference\n\n\n\n\n1\n65.84\n91.20\n-25.36\n\n\n2\n34.66\n74.63\n-39.97\n\n\n3\n36.03\n89.75\n-53.72\n\n\n4\n55.74\n95.41\n-39.67\n\n\n5\n23.95\n93.30\n-69.35"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#difference-of-averages-across-all-individuals",
    "href": "content/slides/01-07-experiments.html#difference-of-averages-across-all-individuals",
    "title": "Causal Effects and Experiments",
    "section": "Difference of averages across all individuals",
    "text": "Difference of averages across all individuals\nWhat was the average vote share received in each world?\n\n\n\n\n\nAvg. monitored vote %\nAvg. non-monitored vote %\nDifference\n\n\n\n\n43.24\n88.86\n-45.62"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#back-to-reality",
    "href": "content/slides/01-07-experiments.html#back-to-reality",
    "title": "Causal Effects and Experiments",
    "section": "Back to reality",
    "text": "Back to reality\n\n\n\n\n\nID\nMonitored\nMonitored vote %\nNon-monitored vote %\nDifference\n\n\n\n\n1\n1\n65.84\nNA\nNA\n\n\n2\n1\n34.66\nNA\nNA\n\n\n3\n1\n36.03\nNA\nNA\n\n\n4\n1\n55.74\nNA\nNA\n\n\n5\n1\n23.95\nNA\nNA"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#difference-of-means",
    "href": "content/slides/01-07-experiments.html#difference-of-means",
    "title": "Causal Effects and Experiments",
    "section": "Difference-of-means",
    "text": "Difference-of-means\nWhat was the average vote share received in each group?\n\n\n\n\n\nAvg. monitored vote %\nAvg. non-monitored vote %\nDifference\n\n\n\n\n43.24\nNaN\nNaN"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#how-on-earth-does-this-work-so-well",
    "href": "content/slides/01-07-experiments.html#how-on-earth-does-this-work-so-well",
    "title": "Causal Effects and Experiments",
    "section": "How on Earth does this work so well?",
    "text": "How on Earth does this work so well?\nRandomization!\n\nMonitors were assigned to polling stations randomly (for example, with the flip of a coin)\nThis created two groups of stations that were roughly identical on average to one another prior to treatment\nThis mimics what happens when we split our world into two (creating two literally identical groups)"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#our-goal",
    "href": "content/slides/01-07-experiments.html#our-goal",
    "title": "Causal Effects and Experiments",
    "section": "Our goal",
    "text": "Our goal\nWe want two groups that are completely identical to one another prior to treatment.\n\nThis allows us to compare their outcomes after treatment and claim that the treatment caused differences in those outcomes (if any differences exist)"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#our-goal-1",
    "href": "content/slides/01-07-experiments.html#our-goal-1",
    "title": "Causal Effects and Experiments",
    "section": "Our goal",
    "text": "Our goal"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#next-best-thing",
    "href": "content/slides/01-07-experiments.html#next-best-thing",
    "title": "Causal Effects and Experiments",
    "section": "Next best thing",
    "text": "Next best thing\nInstead, we should try to make two groups that are as similar as possible to each other prior to treatment."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#the-magic-of-randomization",
    "href": "content/slides/01-07-experiments.html#the-magic-of-randomization",
    "title": "Causal Effects and Experiments",
    "section": "The magic of randomization",
    "text": "The magic of randomization\nPerfectly random assignment does this very well!"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#dont-take-my-word-for-it",
    "href": "content/slides/01-07-experiments.html#dont-take-my-word-for-it",
    "title": "Causal Effects and Experiments",
    "section": "Don’t take my word for it",
    "text": "Don’t take my word for it\nImagine we have a group of 1,000 individuals. We know the following about them:\n\nHeight\nWeight\nEye colour"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#our-group",
    "href": "content/slides/01-07-experiments.html#our-group",
    "title": "Causal Effects and Experiments",
    "section": "Our group",
    "text": "Our group\n\n\n\n\n\nID\nHeight\nWeight\nEye colour\n\n\n\n\n1\n176.70\n73.11\nGreen\n\n\n2\n173.55\n68.74\nBlue\n\n\n3\n171.55\n92.57\nBrown\n\n\n4\n181.01\n74.95\nBlue\n\n\n5\n183.80\n97.08\nBlue\n\n\n6\n173.27\n74.24\nBlue\n\n\n7\n174.98\n77.08\nBrown\n\n\n8\n179.96\n78.62\nBlue\n\n\n9\n172.05\n77.39\nBlue\n\n\n10\n167.73\n78.41\nBrown\n\n\n11\n160.50\n84.77\nBlue\n\n\n12\n166.59\n75.98\nGrey\n\n\n13\n166.95\n74.71\nBlue\n\n\n14\n168.57\n64.72\nBlue\n\n\n15\n169.37\n75.80\nBlue\n\n\n16\n167.70\n84.09\nBrown\n\n\n17\n171.61\n94.79\nGreen\n\n\n18\n168.93\n62.46\nBrown\n\n\n19\n170.26\n101.50\nBrown\n\n\n20\n170.47\n85.83\nGrey\n\n\n21\n171.18\n62.35\nBrown\n\n\n22\n171.54\n70.39\nBlue\n\n\n23\n172.77\n93.21\nBrown\n\n\n24\n163.10\n79.40\nGrey\n\n\n25\n180.80\n67.30\nBlue\n\n\n26\n167.67\n88.59\nGrey\n\n\n27\n158.11\n75.07\nGreen\n\n\n28\n166.25\n83.27\nBlue\n\n\n29\n173.64\n78.02\nBlue\n\n\n30\n163.98\n84.68\nGrey\n\n\n31\n171.67\n104.21\nBrown\n\n\n32\n167.83\n78.15\nBrown\n\n\n33\n171.97\n73.39\nGrey\n\n\n34\n170.63\n87.91\nGreen\n\n\n35\n170.29\n86.38\nBlue\n\n\n36\n175.91\n78.67\nBrown\n\n\n37\n155.02\n76.91\nBlue\n\n\n38\n169.87\n77.31\nBlue\n\n\n39\n160.97\n83.04\nGrey\n\n\n40\n181.44\n77.12\nBrown\n\n\n41\n171.61\n80.47\nBlue\n\n\n42\n160.03\n79.89\nGrey\n\n\n43\n180.68\n86.36\nGrey\n\n\n44\n171.72\n58.37\nGrey\n\n\n45\n173.06\n72.72\nBrown\n\n\n46\n165.58\n72.96\nGreen\n\n\n47\n164.36\n71.56\nBlue\n\n\n48\n176.62\n96.55\nGreen\n\n\n49\n166.93\n72.74\nBrown\n\n\n50\n168.36\n61.42\nGrey\n\n\n51\n164.11\n73.31\nGreen\n\n\n52\n174.72\n69.63\nGreen\n\n\n53\n179.73\n96.29\nGreen\n\n\n54\n165.44\n89.68\nGreen\n\n\n55\n166.47\n65.65\nBlue\n\n\n56\n167.54\n79.92\nBrown\n\n\n57\n162.22\n68.93\nGreen\n\n\n58\n166.86\n67.93\nBrown\n\n\n59\n172.37\n88.66\nBlue\n\n\n60\n169.70\n79.99\nGrey\n\n\n61\n181.24\n88.59\nGreen\n\n\n62\n173.09\n75.79\nBrown\n\n\n63\n180.57\n88.09\nBlue\n\n\n64\n168.34\n65.53\nBlue\n\n\n65\n164.15\n77.92\nBlue\n\n\n66\n173.63\n76.08\nGrey\n\n\n67\n161.33\n50.96\nGrey\n\n\n68\n171.08\n87.73\nBlue\n\n\n69\n164.42\n89.42\nGreen\n\n\n70\n158.65\n81.30\nGrey\n\n\n71\n168.36\n77.29\nGrey\n\n\n72\n174.80\n68.38\nGrey\n\n\n73\n176.14\n98.85\nGreen\n\n\n74\n166.37\n85.02\nGrey\n\n\n75\n155.34\n96.23\nGreen\n\n\n76\n167.44\n91.24\nGreen\n\n\n77\n166.41\n82.22\nGreen\n\n\n78\n166.18\n86.13\nGreen\n\n\n79\n163.02\n75.37\nBrown\n\n\n80\n170.98\n86.68\nBlue\n\n\n81\n164.37\n73.72\nBrown\n\n\n82\n160.15\n80.81\nGrey\n\n\n83\n173.82\n89.12\nBlue\n\n\n84\n168.66\n86.82\nBrown\n\n\n85\n176.17\n73.80\nGreen\n\n\n86\n165.34\n98.34\nBrown\n\n\n87\n171.37\n59.93\nGreen\n\n\n88\n164.64\n84.63\nBrown\n\n\n89\n173.24\n77.30\nGrey\n\n\n90\n174.41\n91.50\nBlue\n\n\n91\n167.97\n81.73\nGrey\n\n\n92\n158.54\n86.14\nBlue\n\n\n93\n168.61\n75.99\nGrey\n\n\n94\n162.33\n65.08\nGreen\n\n\n95\n160.27\n89.90\nGreen\n\n\n96\n171.01\n98.48\nGreen\n\n\n97\n167.17\n83.86\nBlue\n\n\n98\n173.52\n79.00\nBlue\n\n\n99\n165.74\n88.70\nGreen\n\n\n100\n163.21\n88.79\nGreen\n\n\n101\n177.41\n83.51\nBrown\n\n\n102\n170.89\n78.99\nGrey\n\n\n103\n157.66\n85.13\nBrown\n\n\n104\n160.34\n63.07\nGreen\n\n\n105\n163.16\n74.19\nBlue\n\n\n106\n158.75\n70.31\nBrown\n\n\n107\n171.91\n89.09\nBlue\n\n\n108\n162.10\n80.92\nGreen\n\n\n109\n171.13\n85.51\nBlue\n\n\n110\n167.04\n96.91\nBlue\n\n\n111\n171.06\n67.56\nGrey\n\n\n112\n172.15\n88.39\nGreen\n\n\n113\n176.48\n84.85\nBrown\n\n\n114\n166.62\n87.99\nBlue\n\n\n115\n174.70\n66.92\nBlue\n\n\n116\n163.54\n102.28\nGrey\n\n\n117\n165.48\n89.71\nGreen\n\n\n118\n155.34\n79.88\nGreen\n\n\n119\n181.99\n87.62\nGrey\n\n\n120\n168.61\n76.69\nBrown\n\n\n121\n163.71\n70.37\nGreen\n\n\n122\n176.03\n90.43\nBrown\n\n\n123\n170.70\n68.23\nBrown\n\n\n124\n171.70\n80.07\nBlue\n\n\n125\n160.91\n57.58\nBlue\n\n\n126\n168.49\n85.05\nBlue\n\n\n127\n165.55\n68.48\nBrown\n\n\n128\n170.33\n85.95\nGreen\n\n\n129\n168.21\n79.27\nGrey\n\n\n130\n166.61\n74.71\nBrown\n\n\n131\n168.09\n79.46\nGreen\n\n\n132\n170.66\n96.20\nGreen\n\n\n133\n162.13\n68.03\nBlue\n\n\n134\n177.47\n71.34\nBrown\n\n\n135\n165.83\n80.94\nBlue\n\n\n136\n177.58\n82.21\nGreen\n\n\n137\n172.26\n78.10\nGrey\n\n\n138\n176.43\n62.73\nBrown\n\n\n139\n171.51\n102.06\nGreen\n\n\n140\n178.49\n82.57\nBlue\n\n\n141\n167.92\n67.29\nGreen\n\n\n142\n167.98\n86.95\nBlue\n\n\n143\n179.43\n84.06\nGrey\n\n\n144\n161.69\n68.49\nGreen\n\n\n145\n164.02\n87.07\nBlue\n\n\n146\n171.64\n60.86\nBlue\n\n\n147\n171.23\n91.08\nGrey\n\n\n148\n175.62\n78.79\nBrown\n\n\n149\n169.01\n97.64\nGreen\n\n\n150\n179.82\n83.21\nBrown\n\n\n151\n163.30\n68.93\nBrown\n\n\n152\n169.15\n76.73\nBlue\n\n\n153\n165.59\n77.66\nGreen\n\n\n154\n168.86\n78.91\nGrey\n\n\n155\n175.22\n91.87\nBrown\n\n\n156\n166.88\n66.71\nBlue\n\n\n157\n169.74\n101.18\nGreen\n\n\n158\n169.29\n81.76\nGreen\n\n\n159\n177.13\n61.47\nGreen\n\n\n160\n174.49\n99.84\nGreen\n\n\n161\n173.27\n87.31\nGrey\n\n\n162\n160.65\n86.83\nBrown\n\n\n163\n175.89\n73.66\nBlue\n\n\n164\n173.27\n65.95\nBrown\n\n\n165\n180.82\n76.49\nGreen\n\n\n166\n168.35\n80.63\nGrey\n\n\n167\n161.14\n70.26\nBrown\n\n\n168\n166.12\n80.28\nGrey\n\n\n169\n174.57\n71.21\nBrown\n\n\n170\n164.05\n76.54\nGreen\n\n\n171\n170.84\n60.52\nGreen\n\n\n172\n169.38\n90.74\nGrey\n\n\n173\n158.51\n85.49\nGreen\n\n\n174\n168.75\n79.91\nBrown\n\n\n175\n168.53\n76.29\nBrown\n\n\n176\n169.74\n73.68\nGrey\n\n\n177\n170.12\n82.79\nBlue\n\n\n178\n172.36\n94.37\nGreen\n\n\n179\n173.28\n65.94\nBrown\n\n\n180\n158.36\n69.75\nGrey\n\n\n181\n175.32\n74.02\nBlue\n\n\n182\n170.36\n81.16\nGreen\n\n\n183\n178.17\n83.29\nBrown\n\n\n184\n168.66\n75.14\nGreen\n\n\n185\n173.14\n77.00\nGrey\n\n\n186\n165.64\n91.08\nGreen\n\n\n187\n175.92\n78.84\nBrown\n\n\n188\n169.93\n66.59\nGrey\n\n\n189\n168.88\n86.20\nBlue\n\n\n190\n172.88\n78.38\nGreen\n\n\n191\n167.17\n82.70\nBlue\n\n\n192\n169.07\n88.74\nGreen\n\n\n193\n169.77\n79.44\nBrown\n\n\n194\n159.38\n86.55\nGreen\n\n\n195\n170.02\n84.49\nGrey\n\n\n196\n160.70\n76.15\nGreen\n\n\n197\n162.99\n89.73\nGreen\n\n\n198\n165.41\n56.54\nGreen\n\n\n199\n161.43\n71.48\nBlue\n\n\n200\n166.06\n90.44\nGrey\n\n\n201\n166.83\n70.75\nBlue\n\n\n202\n176.01\n85.24\nGrey\n\n\n203\n171.49\n65.49\nBlue\n\n\n204\n172.80\n69.30\nBrown\n\n\n205\n168.72\n74.75\nBlue\n\n\n206\n168.54\n75.65\nBrown\n\n\n207\n171.42\n48.03\nBrown\n\n\n208\n175.27\n87.44\nBlue\n\n\n209\n168.28\n90.87\nGreen\n\n\n210\n181.92\n68.40\nBlue\n\n\n211\n165.43\n87.81\nBrown\n\n\n212\n179.36\n83.75\nBrown\n\n\n213\n173.77\n72.80\nGrey\n\n\n214\n167.28\n80.26\nGrey\n\n\n215\n168.76\n93.66\nBlue\n\n\n216\n172.25\n78.19\nBlue\n\n\n217\n166.25\n74.57\nGrey\n\n\n218\n171.80\n76.14\nGreen\n\n\n219\n168.50\n73.20\nBrown\n\n\n220\n167.45\n69.53\nGreen\n\n\n221\n177.77\n91.10\nGrey\n\n\n222\n175.90\n76.32\nGreen\n\n\n223\n177.95\n83.79\nBrown\n\n\n224\n176.93\n87.98\nBrown\n\n\n225\n173.42\n83.36\nGrey\n\n\n226\n184.90\n84.10\nBrown\n\n\n227\n170.05\n55.27\nGrey\n\n\n228\n159.57\n86.02\nGreen\n\n\n229\n175.18\n88.22\nBlue\n\n\n230\n168.10\n91.42\nBrown\n\n\n231\n164.20\n66.63\nBrown\n\n\n232\n165.30\n89.41\nBlue\n\n\n233\n169.29\n70.78\nBlue\n\n\n234\n167.40\n67.80\nBlue\n\n\n235\n175.08\n90.91\nBrown\n\n\n236\n167.12\n65.75\nGreen\n\n\n237\n169.35\n68.78\nGrey\n\n\n238\n173.14\n81.73\nGreen\n\n\n239\n170.92\n82.65\nGrey\n\n\n240\n166.66\n95.30\nGreen\n\n\n241\n161.96\n57.98\nBrown\n\n\n242\n173.02\n57.41\nBlue\n\n\n243\n174.14\n57.52\nGreen\n\n\n244\n159.54\n74.67\nGreen\n\n\n245\n177.56\n66.55\nBrown\n\n\n246\n172.33\n68.01\nBlue\n\n\n247\n170.31\n77.96\nGreen\n\n\n248\n183.41\n65.97\nGreen\n\n\n249\n168.93\n75.41\nBlue\n\n\n250\n157.23\n70.82\nBlue\n\n\n251\n170.43\n90.30\nBlue\n\n\n252\n165.91\n80.87\nBrown\n\n\n253\n173.99\n71.48\nGrey\n\n\n254\n171.86\n73.99\nBlue\n\n\n255\n173.00\n71.55\nBlue\n\n\n256\n166.81\n66.82\nGreen\n\n\n257\n169.47\n83.92\nGreen\n\n\n258\n163.14\n68.18\nGreen\n\n\n259\n170.96\n73.63\nBlue\n\n\n260\n169.76\n90.32\nBlue\n\n\n261\n164.57\n79.52\nGrey\n\n\n262\n171.09\n74.96\nBrown\n\n\n263\n182.81\n84.71\nGrey\n\n\n264\n157.26\n82.23\nBlue\n\n\n265\n178.62\n77.57\nBrown\n\n\n266\n167.95\n71.93\nBlue\n\n\n267\n165.84\n75.13\nBrown\n\n\n268\n157.22\n80.20\nBrown\n\n\n269\n169.55\n84.40\nBrown\n\n\n270\n166.75\n80.27\nGrey\n\n\n271\n160.29\n94.94\nBlue\n\n\n272\n174.13\n87.81\nGreen\n\n\n273\n170.48\n91.79\nBrown\n\n\n274\n158.52\n90.16\nBlue\n\n\n275\n175.57\n70.02\nBlue\n\n\n276\n165.78\n72.29\nGrey\n\n\n277\n162.34\n62.37\nBrown\n\n\n278\n186.29\n78.78\nBrown\n\n\n279\n164.20\n81.83\nGrey\n\n\n280\n165.60\n73.44\nBlue\n\n\n281\n167.50\n61.65\nBlue\n\n\n282\n173.10\n72.15\nBrown\n\n\n283\n172.03\n82.45\nGreen\n\n\n284\n179.34\n82.94\nGrey\n\n\n285\n180.37\n64.45\nGreen\n\n\n286\n180.26\n92.08\nBlue\n\n\n287\n162.82\n80.10\nGreen\n\n\n288\n163.48\n77.86\nGrey\n\n\n289\n177.84\n90.93\nGrey\n\n\n290\n167.19\n73.73\nBrown\n\n\n291\n161.33\n66.57\nBrown\n\n\n292\n163.76\n70.94\nGreen\n\n\n293\n175.46\n82.02\nGreen\n\n\n294\n160.63\n70.61\nGreen\n\n\n295\n169.44\n62.60\nBrown\n\n\n296\n170.23\n80.45\nBlue\n\n\n297\n156.04\n84.73\nGrey\n\n\n298\n177.95\n81.07\nBlue\n\n\n299\n160.54\n75.62\nGrey\n\n\n300\n170.91\n70.18\nBrown\n\n\n301\n176.86\n94.99\nBrown\n\n\n302\n171.33\n67.79\nBlue\n\n\n303\n175.45\n75.93\nBlue\n\n\n304\n170.62\n68.55\nGreen\n\n\n305\n168.17\n83.77\nBrown\n\n\n306\n173.84\n71.72\nBrown\n\n\n307\n159.01\n73.11\nGreen\n\n\n308\n166.08\n89.02\nGreen\n\n\n309\n176.74\n64.43\nGrey\n\n\n310\n172.34\n89.83\nBrown\n\n\n311\n163.58\n87.20\nBrown\n\n\n312\n174.14\n77.71\nGreen\n\n\n313\n155.61\n89.70\nBlue\n\n\n314\n162.92\n83.94\nGrey\n\n\n315\n178.39\n71.42\nGreen\n\n\n316\n175.89\n77.52\nGrey\n\n\n317\n171.22\n89.30\nGrey\n\n\n318\n164.02\n77.52\nGreen\n\n\n319\n161.22\n90.84\nGreen\n\n\n320\n176.77\n88.15\nBrown\n\n\n321\n178.65\n72.52\nBlue\n\n\n322\n166.52\n90.45\nBlue\n\n\n323\n179.48\n91.96\nBlue\n\n\n324\n176.15\n74.59\nGrey\n\n\n325\n171.16\n83.82\nBrown\n\n\n326\n177.57\n70.86\nBlue\n\n\n327\n175.48\n77.32\nGreen\n\n\n328\n178.58\n80.32\nGreen\n\n\n329\n164.07\n67.66\nBlue\n\n\n330\n175.18\n74.10\nBrown\n\n\n331\n161.35\n55.65\nGrey\n\n\n332\n169.80\n80.75\nBlue\n\n\n333\n169.63\n93.98\nBrown\n\n\n334\n170.97\n75.78\nGrey\n\n\n335\n172.17\n86.89\nGreen\n\n\n336\n175.11\n86.04\nBlue\n\n\n337\n178.89\n76.03\nGrey\n\n\n338\n165.26\n85.85\nGrey\n\n\n339\n165.95\n82.71\nGrey\n\n\n340\n166.84\n87.37\nGrey\n\n\n341\n171.02\n72.19\nGreen\n\n\n342\n173.37\n95.15\nGreen\n\n\n343\n165.46\n83.44\nGrey\n\n\n344\n172.23\n77.78\nGreen\n\n\n345\n173.60\n77.39\nBlue\n\n\n346\n163.58\n77.77\nBrown\n\n\n347\n171.49\n82.36\nBrown\n\n\n348\n165.26\n78.44\nGrey\n\n\n349\n162.94\n91.14\nGreen\n\n\n350\n175.12\n97.98\nGreen\n\n\n351\n174.74\n74.10\nBlue\n\n\n352\n176.31\n72.38\nBrown\n\n\n353\n176.32\n71.56\nGreen\n\n\n354\n166.29\n62.28\nBlue\n\n\n355\n171.00\n85.88\nGreen\n\n\n356\n172.94\n85.40\nBrown\n\n\n357\n177.44\n68.67\nBlue\n\n\n358\n160.64\n83.10\nBrown\n\n\n359\n176.62\n73.81\nBlue\n\n\n360\n173.02\n87.66\nBlue\n\n\n361\n163.30\n86.61\nBlue\n\n\n362\n174.35\n73.65\nBrown\n\n\n363\n179.27\n66.98\nBrown\n\n\n364\n169.05\n73.35\nBrown\n\n\n365\n171.59\n98.92\nBlue\n\n\n366\n166.86\n81.15\nGreen\n\n\n367\n164.47\n80.63\nBlue\n\n\n368\n174.85\n88.93\nGrey\n\n\n369\n169.17\n68.39\nGrey\n\n\n370\n174.99\n71.34\nBrown\n\n\n371\n174.55\n82.42\nGreen\n\n\n372\n169.69\n84.97\nBrown\n\n\n373\n164.98\n98.79\nBrown\n\n\n374\n171.01\n75.99\nBrown\n\n\n375\n168.87\n83.22\nBlue\n\n\n376\n168.98\n80.16\nGreen\n\n\n377\n169.61\n75.51\nGrey\n\n\n378\n168.95\n98.18\nBlue\n\n\n379\n170.04\n75.47\nGreen\n\n\n380\n178.15\n80.74\nBrown\n\n\n381\n170.86\n76.62\nGrey\n\n\n382\n168.63\n69.44\nGrey\n\n\n383\n169.77\n93.70\nBrown\n\n\n384\n155.82\n85.51\nGrey\n\n\n385\n158.99\n86.62\nGreen\n\n\n386\n163.80\n86.46\nGrey\n\n\n387\n164.59\n81.46\nBrown\n\n\n388\n165.72\n92.99\nGrey\n\n\n389\n161.43\n88.09\nGrey\n\n\n390\n170.34\n86.97\nBrown\n\n\n391\n163.21\n72.31\nBlue\n\n\n392\n162.23\n88.33\nGreen\n\n\n393\n180.83\n84.00\nBrown\n\n\n394\n171.08\n80.97\nGreen\n\n\n395\n181.84\n96.79\nGrey\n\n\n396\n179.67\n95.43\nGreen\n\n\n397\n169.64\n80.95\nBrown\n\n\n398\n163.42\n91.17\nGreen\n\n\n399\n164.58\n81.44\nBlue\n\n\n400\n162.01\n94.56\nGreen\n\n\n401\n166.94\n105.58\nGreen\n\n\n402\n170.64\n79.27\nBrown\n\n\n403\n181.23\n79.44\nGrey\n\n\n404\n162.34\n75.26\nGrey\n\n\n405\n167.55\n66.60\nBrown\n\n\n406\n176.62\n76.30\nBlue\n\n\n407\n159.02\n66.77\nBlue\n\n\n408\n169.57\n77.50\nGrey\n\n\n409\n173.24\n63.78\nBrown\n\n\n410\n169.17\n82.77\nGreen\n\n\n411\n171.18\n69.27\nBrown\n\n\n412\n174.43\n58.19\nGreen\n\n\n413\n166.06\n76.84\nBlue\n\n\n414\n165.87\n74.95\nBrown\n\n\n415\n170.64\n84.10\nGrey\n\n\n416\n182.75\n80.60\nBrown\n\n\n417\n168.04\n76.03\nGrey\n\n\n418\n164.45\n100.22\nBrown\n\n\n419\n162.04\n67.72\nBrown\n\n\n420\n169.99\n84.94\nGreen\n\n\n421\n176.92\n87.04\nGrey\n\n\n422\n170.59\n79.15\nGreen\n\n\n423\n160.99\n71.60\nBrown\n\n\n424\n163.96\n86.42\nBrown\n\n\n425\n162.62\n85.93\nGrey\n\n\n426\n176.67\n58.55\nBrown\n\n\n427\n163.91\n83.32\nGreen\n\n\n428\n171.31\n73.43\nGreen\n\n\n429\n166.62\n99.34\nBlue\n\n\n430\n178.65\n62.36\nBlue\n\n\n431\n171.67\n79.68\nBrown\n\n\n432\n169.91\n84.26\nGrey\n\n\n433\n166.36\n66.86\nGrey\n\n\n434\n171.39\n88.02\nGrey\n\n\n435\n169.66\n71.90\nGreen\n\n\n436\n180.01\n60.59\nGrey\n\n\n437\n171.27\n69.92\nGreen\n\n\n438\n166.37\n75.06\nBrown\n\n\n439\n172.52\n88.86\nBlue\n\n\n440\n169.89\n86.78\nGreen\n\n\n441\n171.11\n81.62\nGreen\n\n\n442\n157.74\n84.24\nGrey\n\n\n443\n158.42\n86.95\nBrown\n\n\n444\n169.86\n82.49\nGreen\n\n\n445\n180.74\n66.95\nGrey\n\n\n446\n178.86\n86.22\nGrey\n\n\n447\n181.83\n80.26\nGreen\n\n\n448\n167.92\n86.38\nGrey\n\n\n449\n167.20\n60.45\nBrown\n\n\n450\n157.00\n109.69\nGreen\n\n\n451\n175.13\n85.79\nGrey\n\n\n452\n172.68\n97.45\nBrown\n\n\n453\n169.39\n70.13\nGreen\n\n\n454\n164.87\n85.52\nBlue\n\n\n455\n174.14\n69.48\nGrey\n\n\n456\n178.85\n62.86\nBrown\n\n\n457\n161.78\n85.78\nBlue\n\n\n458\n170.88\n78.04\nBrown\n\n\n459\n169.38\n69.96\nBrown\n\n\n460\n162.51\n83.56\nBlue\n\n\n461\n164.87\n67.12\nGreen\n\n\n462\n174.35\n69.83\nBrown\n\n\n463\n172.82\n81.86\nBlue\n\n\n464\n167.08\n78.35\nBrown\n\n\n465\n174.23\n95.48\nGrey\n\n\n466\n171.22\n90.52\nGreen\n\n\n467\n168.46\n84.97\nBrown\n\n\n468\n189.57\n77.43\nGreen\n\n\n469\n174.85\n88.70\nBrown\n\n\n470\n165.29\n80.47\nBlue\n\n\n471\n171.87\n89.64\nGrey\n\n\n472\n169.52\n79.42\nGreen\n\n\n473\n166.05\n90.97\nGreen\n\n\n474\n168.86\n66.72\nGrey\n\n\n475\n185.59\n75.30\nBrown\n\n\n476\n174.81\n72.62\nBlue\n\n\n477\n171.50\n91.48\nGreen\n\n\n478\n181.42\n70.84\nGreen\n\n\n479\n172.63\n61.30\nBrown\n\n\n480\n164.68\n86.60\nBlue\n\n\n481\n170.60\n70.49\nBrown\n\n\n482\n161.53\n90.85\nBlue\n\n\n483\n165.57\n96.40\nBlue\n\n\n484\n172.50\n69.94\nBrown\n\n\n485\n161.80\n87.26\nBrown\n\n\n486\n168.51\n89.39\nBrown\n\n\n487\n166.50\n90.21\nBlue\n\n\n488\n165.63\n77.65\nBrown\n\n\n489\n165.10\n74.46\nBlue\n\n\n490\n180.50\n76.57\nGrey\n\n\n491\n174.84\n66.39\nBrown\n\n\n492\n174.10\n85.40\nBlue\n\n\n493\n169.37\n65.70\nBrown\n\n\n494\n165.84\n75.62\nGreen\n\n\n495\n174.03\n91.78\nBrown\n\n\n496\n161.91\n76.69\nGreen\n\n\n497\n165.82\n84.49\nBrown\n\n\n498\n176.45\n77.73\nGreen\n\n\n499\n163.76\n84.89\nGreen\n\n\n500\n175.34\n74.79\nGrey\n\n\n501\n176.19\n79.02\nGreen\n\n\n502\n177.80\n68.79\nBrown\n\n\n503\n166.40\n74.26\nBrown\n\n\n504\n171.40\n64.85\nBrown\n\n\n505\n172.94\n89.82\nBrown\n\n\n506\n158.85\n95.25\nBlue\n\n\n507\n178.19\n95.69\nBlue\n\n\n508\n174.64\n71.65\nBrown\n\n\n509\n173.31\n61.63\nGreen\n\n\n510\n168.41\n89.21\nGrey\n\n\n511\n167.37\n83.13\nBrown\n\n\n512\n163.14\n77.23\nBrown\n\n\n513\n154.94\n78.91\nBrown\n\n\n514\n177.54\n84.86\nGrey\n\n\n515\n168.35\n80.72\nGrey\n\n\n516\n165.30\n71.59\nGreen\n\n\n517\n171.38\n90.15\nGrey\n\n\n518\n178.61\n78.10\nBrown\n\n\n519\n169.61\n86.23\nGreen\n\n\n520\n173.33\n71.01\nGrey\n\n\n521\n167.47\n72.63\nGrey\n\n\n522\n163.41\n98.54\nBlue\n\n\n523\n161.36\n86.59\nGreen\n\n\n524\n168.90\n88.02\nBrown\n\n\n525\n170.51\n79.95\nGrey\n\n\n526\n170.22\n80.82\nBrown\n\n\n527\n176.16\n76.19\nBlue\n\n\n528\n185.36\n82.77\nGrey\n\n\n529\n180.28\n92.57\nGrey\n\n\n530\n166.17\n91.87\nBrown\n\n\n531\n170.42\n74.92\nBlue\n\n\n532\n166.80\n99.50\nGreen\n\n\n533\n162.70\n91.72\nBrown\n\n\n534\n179.32\n87.12\nGreen\n\n\n535\n171.36\n74.33\nGreen\n\n\n536\n164.12\n75.92\nBrown\n\n\n537\n181.18\n82.27\nBlue\n\n\n538\n170.65\n75.14\nBlue\n\n\n539\n170.26\n97.25\nBrown\n\n\n540\n165.03\n93.62\nGreen\n\n\n541\n168.03\n87.08\nBlue\n\n\n542\n172.68\n74.72\nGrey\n\n\n543\n171.32\n88.08\nBrown\n\n\n544\n167.97\n76.15\nBrown\n\n\n545\n175.27\n76.56\nGreen\n\n\n546\n174.32\n87.44\nBrown\n\n\n547\n173.53\n90.18\nBlue\n\n\n548\n170.63\n80.79\nGrey\n\n\n549\n170.35\n83.07\nGrey\n\n\n550\n179.55\n81.76\nGrey\n\n\n551\n170.29\n84.38\nGrey\n\n\n552\n173.69\n81.86\nBrown\n\n\n553\n165.75\n83.78\nGrey\n\n\n554\n174.85\n92.61\nBlue\n\n\n555\n165.42\n58.67\nBrown\n\n\n556\n168.46\n77.87\nGrey\n\n\n557\n180.39\n87.33\nGrey\n\n\n558\n173.58\n72.97\nGrey\n\n\n559\n175.95\n86.27\nBrown\n\n\n560\n165.24\n94.69\nBlue\n\n\n561\n162.92\n80.14\nBlue\n\n\n562\n168.76\n78.44\nGrey\n\n\n563\n154.97\n69.71\nGrey\n\n\n564\n166.60\n73.44\nBlue\n\n\n565\n178.39\n82.86\nBlue\n\n\n566\n170.43\n67.07\nBrown\n\n\n567\n181.70\n80.12\nGreen\n\n\n568\n167.61\n60.33\nBlue\n\n\n569\n172.11\n75.91\nGreen\n\n\n570\n164.07\n84.60\nBlue\n\n\n571\n171.30\n94.47\nGrey\n\n\n572\n165.82\n86.66\nGreen\n\n\n573\n177.87\n103.07\nGrey\n\n\n574\n167.39\n93.45\nGrey\n\n\n575\n170.40\n65.86\nGrey\n\n\n576\n172.24\n71.97\nGrey\n\n\n577\n177.66\n66.35\nBlue\n\n\n578\n158.44\n71.75\nBrown\n\n\n579\n167.43\n69.87\nGreen\n\n\n580\n177.33\n83.16\nBrown\n\n\n581\n167.91\n89.62\nBrown\n\n\n582\n168.51\n84.52\nGrey\n\n\n583\n166.01\n81.04\nBlue\n\n\n584\n167.88\n84.78\nBrown\n\n\n585\n169.27\n94.25\nBlue\n\n\n586\n162.54\n76.27\nGreen\n\n\n587\n163.42\n81.45\nGreen\n\n\n588\n169.89\n92.00\nBlue\n\n\n589\n182.90\n79.78\nGrey\n\n\n590\n170.13\n77.98\nBlue\n\n\n591\n160.36\n46.75\nGreen\n\n\n592\n163.62\n84.18\nGrey\n\n\n593\n176.19\n74.64\nBrown\n\n\n594\n182.83\n98.21\nBrown\n\n\n595\n155.29\n78.63\nGreen\n\n\n596\n181.12\n75.33\nBlue\n\n\n597\n170.06\n94.14\nGreen\n\n\n598\n160.65\n93.49\nBlue\n\n\n599\n173.07\n71.78\nBlue\n\n\n600\n182.79\n94.18\nGreen\n\n\n601\n177.19\n79.02\nGreen\n\n\n602\n166.34\n81.89\nBlue\n\n\n603\n160.35\n85.94\nGreen\n\n\n604\n169.03\n76.71\nBrown\n\n\n605\n175.38\n61.18\nBrown\n\n\n606\n175.66\n67.94\nGrey\n\n\n607\n167.22\n70.38\nGreen\n\n\n608\n172.13\n74.97\nGreen\n\n\n609\n168.19\n64.19\nGrey\n\n\n610\n175.92\n58.16\nGrey\n\n\n611\n156.76\n76.10\nBrown\n\n\n612\n171.81\n80.61\nGreen\n\n\n613\n176.90\n89.31\nBrown\n\n\n614\n180.21\n91.56\nBlue\n\n\n615\n166.66\n75.97\nGrey\n\n\n616\n169.62\n55.54\nGrey\n\n\n617\n169.89\n68.11\nBrown\n\n\n618\n177.50\n71.99\nBlue\n\n\n619\n177.44\n90.41\nBrown\n\n\n620\n157.05\n67.50\nGrey\n\n\n621\n161.00\n77.55\nBrown\n\n\n622\n171.80\n79.21\nGreen\n\n\n623\n170.97\n84.51\nGrey\n\n\n624\n178.70\n77.99\nGreen\n\n\n625\n178.94\n95.44\nBrown\n\n\n626\n167.65\n72.41\nBrown\n\n\n627\n169.16\n76.84\nBrown\n\n\n628\n163.32\n72.46\nBlue\n\n\n629\n167.14\n77.53\nBrown\n\n\n630\n164.91\n79.02\nBrown\n\n\n631\n167.98\n82.24\nGreen\n\n\n632\n181.76\n95.77\nGreen\n\n\n633\n164.63\n85.10\nBrown\n\n\n634\n172.93\n77.99\nGreen\n\n\n635\n165.38\n94.88\nBrown\n\n\n636\n170.85\n82.02\nBrown\n\n\n637\n172.30\n91.33\nGrey\n\n\n638\n178.09\n75.24\nGreen\n\n\n639\n168.69\n67.43\nGrey\n\n\n640\n176.22\n79.82\nGrey\n\n\n641\n181.02\n79.45\nGrey\n\n\n642\n167.53\n56.23\nGreen\n\n\n643\n175.85\n83.48\nGreen\n\n\n644\n182.90\n73.67\nGreen\n\n\n645\n173.30\n86.46\nGrey\n\n\n646\n174.78\n84.77\nBlue\n\n\n647\n174.23\n82.70\nBrown\n\n\n648\n175.45\n89.24\nBrown\n\n\n649\n169.58\n77.48\nGrey\n\n\n650\n171.74\n94.38\nGreen\n\n\n651\n173.35\n74.55\nBrown\n\n\n652\n165.43\n74.13\nBlue\n\n\n653\n170.78\n85.91\nBlue\n\n\n654\n175.53\n96.12\nGreen\n\n\n655\n181.40\n85.99\nBlue\n\n\n656\n169.12\n76.91\nGrey\n\n\n657\n174.09\n78.85\nBrown\n\n\n658\n169.49\n85.84\nGrey\n\n\n659\n176.24\n76.18\nGreen\n\n\n660\n172.30\n82.70\nBrown\n\n\n661\n167.43\n83.55\nGreen\n\n\n662\n171.17\n91.64\nGreen\n\n\n663\n168.20\n77.96\nBlue\n\n\n664\n166.97\n94.01\nGreen\n\n\n665\n167.31\n60.13\nGrey\n\n\n666\n167.29\n75.21\nBrown\n\n\n667\n178.23\n81.45\nBlue\n\n\n668\n166.20\n67.92\nGrey\n\n\n669\n169.02\n87.46\nBlue\n\n\n670\n167.85\n72.56\nBlue\n\n\n671\n158.30\n78.00\nGreen\n\n\n672\n180.64\n73.73\nBlue\n\n\n673\n174.76\n78.45\nBlue\n\n\n674\n160.15\n67.49\nBlue\n\n\n675\n173.60\n88.69\nBrown\n\n\n676\n183.11\n87.03\nGrey\n\n\n677\n162.92\n87.31\nBlue\n\n\n678\n167.58\n83.90\nGrey\n\n\n679\n170.65\n99.31\nGrey\n\n\n680\n170.93\n87.30\nBrown\n\n\n681\n174.07\n85.59\nBrown\n\n\n682\n158.26\n62.89\nBlue\n\n\n683\n171.36\n92.67\nBrown\n\n\n684\n167.58\n90.09\nBlue\n\n\n685\n160.64\n74.61\nBlue\n\n\n686\n167.97\n86.69\nGrey\n\n\n687\n171.31\n60.75\nBrown\n\n\n688\n170.41\n85.50\nGreen\n\n\n689\n178.91\n82.58\nGrey\n\n\n690\n170.73\n79.52\nBlue\n\n\n691\n182.26\n70.28\nGreen\n\n\n692\n169.20\n83.84\nGrey\n\n\n693\n175.44\n84.43\nBlue\n\n\n694\n171.66\n81.24\nBlue\n\n\n695\n168.24\n81.01\nGreen\n\n\n696\n170.88\n81.51\nBlue\n\n\n697\n179.31\n68.41\nBrown\n\n\n698\n173.38\n58.64\nBlue\n\n\n699\n171.78\n81.16\nGreen\n\n\n700\n176.31\n66.76\nBlue\n\n\n701\n173.27\n81.98\nBlue\n\n\n702\n180.20\n80.06\nBrown\n\n\n703\n163.07\n76.68\nBlue\n\n\n704\n178.05\n85.70\nBlue\n\n\n705\n169.52\n97.65\nGrey\n\n\n706\n174.09\n80.44\nBlue\n\n\n707\n162.67\n83.88\nGreen\n\n\n708\n162.50\n97.09\nBrown\n\n\n709\n174.33\n70.61\nBrown\n\n\n710\n164.46\n75.45\nBrown\n\n\n711\n160.98\n86.42\nGrey\n\n\n712\n164.09\n75.25\nBrown\n\n\n713\n165.32\n99.93\nBlue\n\n\n714\n165.29\n92.95\nBrown\n\n\n715\n171.09\n70.43\nGrey\n\n\n716\n176.25\n73.65\nBlue\n\n\n717\n170.16\n88.82\nBrown\n\n\n718\n167.17\n94.42\nGreen\n\n\n719\n176.76\n87.31\nBlue\n\n\n720\n167.00\n86.57\nBlue\n\n\n721\n163.93\n94.25\nGreen\n\n\n722\n177.56\n77.72\nGrey\n\n\n723\n157.02\n85.21\nGreen\n\n\n724\n171.27\n99.56\nBlue\n\n\n725\n170.19\n73.13\nBrown\n\n\n726\n168.61\n85.38\nBlue\n\n\n727\n174.91\n92.50\nBrown\n\n\n728\n174.96\n78.84\nBrown\n\n\n729\n167.10\n68.13\nGrey\n\n\n730\n159.56\n70.02\nBlue\n\n\n731\n171.81\n86.07\nGreen\n\n\n732\n172.12\n77.30\nGrey\n\n\n733\n169.40\n98.71\nGreen\n\n\n734\n172.63\n90.88\nBlue\n\n\n735\n172.14\n81.16\nBlue\n\n\n736\n171.49\n93.67\nGreen\n\n\n737\n164.27\n75.76\nGrey\n\n\n738\n174.75\n71.92\nBlue\n\n\n739\n178.49\n65.73\nBrown\n\n\n740\n162.42\n73.07\nBlue\n\n\n741\n173.25\n65.75\nGreen\n\n\n742\n161.81\n74.14\nGrey\n\n\n743\n167.29\n76.99\nGreen\n\n\n744\n167.55\n70.70\nBlue\n\n\n745\n166.57\n90.76\nBrown\n\n\n746\n165.45\n74.49\nGreen\n\n\n747\n166.44\n61.70\nBrown\n\n\n748\n171.62\n77.31\nBlue\n\n\n749\n182.53\n96.66\nBlue\n\n\n750\n175.17\n98.71\nGrey\n\n\n751\n170.34\n64.02\nGrey\n\n\n752\n182.65\n75.77\nBrown\n\n\n753\n164.66\n83.33\nBlue\n\n\n754\n173.94\n90.77\nGreen\n\n\n755\n174.00\n70.61\nGreen\n\n\n756\n167.35\n73.08\nGreen\n\n\n757\n164.04\n70.76\nGrey\n\n\n758\n167.68\n86.44\nBlue\n\n\n759\n172.91\n59.66\nBrown\n\n\n760\n172.79\n87.22\nGreen\n\n\n761\n165.05\n87.25\nGrey\n\n\n762\n170.16\n72.49\nBrown\n\n\n763\n174.74\n85.97\nBrown\n\n\n764\n166.85\n75.14\nBrown\n\n\n765\n171.46\n84.01\nGrey\n\n\n766\n171.50\n82.97\nGrey\n\n\n767\n166.24\n55.69\nGreen\n\n\n768\n174.29\n74.66\nBlue\n\n\n769\n168.51\n93.75\nBrown\n\n\n770\n176.74\n70.11\nGreen\n\n\n771\n169.57\n82.94\nGrey\n\n\n772\n176.23\n81.36\nBrown\n\n\n773\n172.81\n84.42\nGrey\n\n\n774\n171.68\n78.66\nGreen\n\n\n775\n164.13\n62.75\nGreen\n\n\n776\n170.14\n86.67\nBrown\n\n\n777\n160.52\n87.41\nBlue\n\n\n778\n164.57\n58.84\nBrown\n\n\n779\n150.87\n81.67\nBlue\n\n\n780\n159.05\n86.09\nBlue\n\n\n781\n167.98\n85.85\nGreen\n\n\n782\n164.63\n65.40\nBrown\n\n\n783\n158.62\n74.42\nBrown\n\n\n784\n172.93\n88.11\nBrown\n\n\n785\n174.53\n92.38\nGreen\n\n\n786\n172.78\n76.66\nBlue\n\n\n787\n169.17\n88.52\nBrown\n\n\n788\n171.81\n94.81\nGrey\n\n\n789\n167.42\n87.61\nGrey\n\n\n790\n175.75\n76.07\nGrey\n\n\n791\n162.83\n78.39\nBrown\n\n\n792\n170.01\n79.74\nBrown\n\n\n793\n175.42\n110.79\nBlue\n\n\n794\n174.97\n88.70\nGrey\n\n\n795\n169.20\n84.18\nGreen\n\n\n796\n175.68\n79.86\nGreen\n\n\n797\n172.42\n67.29\nBlue\n\n\n798\n170.98\n73.88\nBlue\n\n\n799\n165.07\n80.21\nBrown\n\n\n800\n177.50\n63.72\nGreen\n\n\n801\n166.25\n70.62\nBrown\n\n\n802\n166.30\n77.81\nGrey\n\n\n803\n181.52\n85.54\nBrown\n\n\n804\n175.26\n60.75\nGrey\n\n\n805\n173.38\n70.63\nBrown\n\n\n806\n164.51\n78.71\nBlue\n\n\n807\n171.78\n84.71\nBlue\n\n\n808\n165.66\n68.92\nGreen\n\n\n809\n167.41\n69.24\nBlue\n\n\n810\n168.40\n68.86\nGrey\n\n\n811\n172.10\n74.32\nGrey\n\n\n812\n174.84\n69.74\nGrey\n\n\n813\n168.22\n76.41\nGrey\n\n\n814\n175.35\n80.89\nGrey\n\n\n815\n168.27\n86.86\nGrey\n\n\n816\n161.11\n60.64\nGreen\n\n\n817\n171.52\n96.93\nGreen\n\n\n818\n168.70\n71.37\nBrown\n\n\n819\n172.92\n71.97\nGrey\n\n\n820\n157.79\n78.13\nBrown\n\n\n821\n160.46\n84.63\nGreen\n\n\n822\n167.48\n73.23\nBlue\n\n\n823\n164.42\n87.27\nBrown\n\n\n824\n167.16\n85.06\nGreen\n\n\n825\n164.22\n76.83\nBlue\n\n\n826\n167.38\n85.58\nGrey\n\n\n827\n167.92\n88.72\nBlue\n\n\n828\n169.62\n70.35\nBlue\n\n\n829\n169.46\n76.79\nGreen\n\n\n830\n169.55\n69.88\nGreen\n\n\n831\n180.36\n80.48\nBrown\n\n\n832\n169.79\n96.97\nGrey\n\n\n833\n168.62\n90.98\nGrey\n\n\n834\n165.75\n78.29\nBlue\n\n\n835\n178.58\n97.01\nBrown\n\n\n836\n170.68\n88.18\nBrown\n\n\n837\n170.55\n90.80\nGrey\n\n\n838\n174.44\n106.58\nBlue\n\n\n839\n159.85\n71.98\nGrey\n\n\n840\n164.98\n80.27\nGreen\n\n\n841\n168.35\n70.84\nGrey\n\n\n842\n175.64\n73.48\nBrown\n\n\n843\n164.38\n77.01\nBlue\n\n\n844\n171.51\n82.79\nBrown\n\n\n845\n164.49\n94.40\nGrey\n\n\n846\n165.46\n91.62\nBrown\n\n\n847\n167.85\n71.71\nGreen\n\n\n848\n158.98\n105.06\nGrey\n\n\n849\n165.87\n86.96\nGrey\n\n\n850\n176.04\n71.93\nGrey\n\n\n851\n177.04\n102.43\nGreen\n\n\n852\n177.15\n71.39\nBlue\n\n\n853\n168.60\n85.07\nBrown\n\n\n854\n169.89\n80.78\nGreen\n\n\n855\n177.50\n94.62\nGrey\n\n\n856\n162.37\n87.83\nBrown\n\n\n857\n174.27\n77.46\nBrown\n\n\n858\n169.69\n68.66\nBrown\n\n\n859\n168.12\n67.13\nGrey\n\n\n860\n176.64\n89.09\nGreen\n\n\n861\n170.16\n80.37\nBlue\n\n\n862\n176.18\n77.91\nBlue\n\n\n863\n181.13\n94.62\nGreen\n\n\n864\n169.48\n55.62\nBlue\n\n\n865\n160.23\n76.41\nGreen\n\n\n866\n173.59\n64.94\nGreen\n\n\n867\n177.58\n59.83\nGreen\n\n\n868\n158.73\n77.83\nBlue\n\n\n869\n166.51\n59.24\nGrey\n\n\n870\n171.16\n76.54\nGreen\n\n\n871\n166.75\n87.15\nBlue\n\n\n872\n167.07\n74.78\nBlue\n\n\n873\n170.56\n65.90\nBlue\n\n\n874\n170.69\n92.34\nBrown\n\n\n875\n178.77\n72.60\nGrey\n\n\n876\n184.79\n80.71\nBrown\n\n\n877\n169.02\n81.13\nBrown\n\n\n878\n177.64\n72.06\nBlue\n\n\n879\n178.17\n79.74\nBrown\n\n\n880\n168.18\n86.66\nGreen\n\n\n881\n173.20\n73.21\nGreen\n\n\n882\n160.10\n74.50\nGrey\n\n\n883\n166.98\n75.43\nBlue\n\n\n884\n162.33\n69.60\nBrown\n\n\n885\n155.45\n75.84\nBrown\n\n\n886\n165.47\n90.28\nBrown\n\n\n887\n156.23\n84.61\nBlue\n\n\n888\n167.94\n72.79\nGreen\n\n\n889\n169.33\n80.79\nBlue\n\n\n890\n170.70\n93.83\nBrown\n\n\n891\n179.78\n91.38\nGreen\n\n\n892\n174.66\n85.08\nGrey\n\n\n893\n166.21\n72.35\nGreen\n\n\n894\n173.43\n86.37\nBrown\n\n\n895\n176.87\n73.21\nBrown\n\n\n896\n171.07\n80.50\nGreen\n\n\n897\n171.75\n83.00\nBrown\n\n\n898\n167.14\n82.98\nBrown\n\n\n899\n164.90\n78.77\nGrey\n\n\n900\n173.61\n84.14\nGrey\n\n\n901\n169.68\n80.68\nBrown\n\n\n902\n160.71\n69.94\nGrey\n\n\n903\n168.64\n78.60\nBlue\n\n\n904\n167.91\n73.80\nGrey\n\n\n905\n174.40\n66.62\nBrown\n\n\n906\n163.77\n81.82\nBrown\n\n\n907\n175.53\n62.93\nBrown\n\n\n908\n171.69\n93.54\nGreen\n\n\n909\n164.33\n69.30\nBlue\n\n\n910\n164.88\n75.57\nBrown\n\n\n911\n175.25\n93.02\nGrey\n\n\n912\n174.47\n62.93\nBrown\n\n\n913\n176.47\n72.97\nGreen\n\n\n914\n173.87\n77.61\nBlue\n\n\n915\n175.21\n76.88\nBrown\n\n\n916\n162.28\n70.08\nBlue\n\n\n917\n169.21\n95.13\nGreen\n\n\n918\n166.93\n88.95\nBlue\n\n\n919\n171.42\n75.58\nGreen\n\n\n920\n168.69\n84.96\nGrey\n\n\n921\n179.67\n66.91\nGreen\n\n\n922\n173.80\n89.03\nGreen\n\n\n923\n177.50\n77.55\nGreen\n\n\n924\n163.92\n64.20\nBlue\n\n\n925\n164.48\n84.95\nGreen\n\n\n926\n167.46\n86.53\nGrey\n\n\n927\n168.09\n94.25\nBlue\n\n\n928\n179.06\n77.81\nBlue\n\n\n929\n173.58\n94.34\nBrown\n\n\n930\n162.63\n59.21\nGrey\n\n\n931\n168.75\n78.31\nGreen\n\n\n932\n165.95\n72.21\nGreen\n\n\n933\n166.54\n93.45\nGreen\n\n\n934\n172.17\n86.66\nBlue\n\n\n935\n161.55\n84.90\nBlue\n\n\n936\n175.02\n74.64\nBrown\n\n\n937\n173.98\n75.45\nGreen\n\n\n938\n170.29\n77.07\nGreen\n\n\n939\n172.37\n60.96\nGreen\n\n\n940\n174.49\n57.51\nBlue\n\n\n941\n178.04\n76.22\nBrown\n\n\n942\n178.74\n67.61\nBlue\n\n\n943\n166.51\n59.70\nGrey\n\n\n944\n174.46\n89.68\nBrown\n\n\n945\n174.72\n87.87\nBlue\n\n\n946\n169.27\n97.67\nGreen\n\n\n947\n174.95\n72.39\nGreen\n\n\n948\n174.66\n76.67\nGreen\n\n\n949\n172.80\n84.31\nGreen\n\n\n950\n170.07\n88.15\nGrey\n\n\n951\n165.03\n85.43\nBlue\n\n\n952\n167.46\n85.82\nGrey\n\n\n953\n168.82\n84.41\nBrown\n\n\n954\n173.16\n85.40\nGrey\n\n\n955\n170.28\n95.64\nBrown\n\n\n956\n180.22\n62.63\nBrown\n\n\n957\n179.28\n88.18\nGrey\n\n\n958\n174.96\n84.55\nGrey\n\n\n959\n170.05\n79.81\nBlue\n\n\n960\n171.82\n72.94\nGreen\n\n\n961\n174.35\n87.19\nGreen\n\n\n962\n162.12\n64.63\nGreen\n\n\n963\n172.52\n72.67\nGrey\n\n\n964\n169.63\n81.66\nGrey\n\n\n965\n180.01\n71.99\nGrey\n\n\n966\n171.54\n79.13\nBrown\n\n\n967\n164.75\n88.90\nGreen\n\n\n968\n174.17\n87.39\nGrey\n\n\n969\n180.01\n69.61\nGrey\n\n\n970\n178.25\n68.80\nBrown\n\n\n971\n170.21\n71.18\nGrey\n\n\n972\n177.13\n73.50\nBlue\n\n\n973\n176.37\n87.15\nGrey\n\n\n974\n172.05\n98.06\nBlue\n\n\n975\n165.59\n78.70\nBlue\n\n\n976\n172.64\n77.49\nGreen\n\n\n977\n175.23\n86.77\nGreen\n\n\n978\n167.40\n72.21\nGrey\n\n\n979\n183.37\n81.27\nBlue\n\n\n980\n162.39\n77.16\nBlue\n\n\n981\n169.06\n81.30\nGreen\n\n\n982\n175.67\n81.96\nBlue\n\n\n983\n163.60\n70.58\nGrey\n\n\n984\n171.01\n64.23\nBlue\n\n\n985\n177.98\n69.07\nGrey\n\n\n986\n172.03\n85.05\nGrey\n\n\n987\n166.31\n77.44\nGreen\n\n\n988\n182.59\n78.23\nGrey\n\n\n989\n160.62\n93.77\nBrown\n\n\n990\n174.78\n64.95\nBrown\n\n\n991\n173.50\n80.96\nGrey\n\n\n992\n176.38\n86.78\nBlue\n\n\n993\n167.54\n86.58\nBrown\n\n\n994\n159.14\n84.11\nBrown\n\n\n995\n168.76\n76.32\nGreen\n\n\n996\n165.34\n88.06\nBlue\n\n\n997\n172.12\n83.62\nBrown\n\n\n998\n166.88\n69.48\nGrey\n\n\n999\n173.81\n67.12\nGreen\n\n\n1000\n171.24\n78.69\nGrey"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#random-assignment",
    "href": "content/slides/01-07-experiments.html#random-assignment",
    "title": "Causal Effects and Experiments",
    "section": "Random assignment",
    "text": "Random assignment\nI’m now going to flip (an imaginary, computer-generated) coin for each of these 1,000 individuals to assigned them to group A or B:\n\n\n\n\n\nID\nHeight\nWeight\nEye colour\nGroup\n\n\n\n\n1\n176.70\n73.11\nGreen\nA\n\n\n2\n173.55\n68.74\nBlue\nB\n\n\n3\n171.55\n92.57\nBrown\nB\n\n\n4\n181.01\n74.95\nBlue\nA\n\n\n5\n183.80\n97.08\nBlue\nA\n\n\n6\n173.27\n74.24\nBlue\nA\n\n\n7\n174.98\n77.08\nBrown\nB\n\n\n8\n179.96\n78.62\nBlue\nB\n\n\n9\n172.05\n77.39\nBlue\nB\n\n\n10\n167.73\n78.41\nBrown\nA\n\n\n11\n160.50\n84.77\nBlue\nB\n\n\n12\n166.59\n75.98\nGrey\nA\n\n\n13\n166.95\n74.71\nBlue\nA\n\n\n14\n168.57\n64.72\nBlue\nA\n\n\n15\n169.37\n75.80\nBlue\nB\n\n\n16\n167.70\n84.09\nBrown\nA\n\n\n17\n171.61\n94.79\nGreen\nA\n\n\n18\n168.93\n62.46\nBrown\nB\n\n\n19\n170.26\n101.50\nBrown\nB\n\n\n20\n170.47\n85.83\nGrey\nB\n\n\n21\n171.18\n62.35\nBrown\nA\n\n\n22\n171.54\n70.39\nBlue\nA\n\n\n23\n172.77\n93.21\nBrown\nB\n\n\n24\n163.10\n79.40\nGrey\nA\n\n\n25\n180.80\n67.30\nBlue\nA\n\n\n26\n167.67\n88.59\nGrey\nB\n\n\n27\n158.11\n75.07\nGreen\nB\n\n\n28\n166.25\n83.27\nBlue\nB\n\n\n29\n173.64\n78.02\nBlue\nA\n\n\n30\n163.98\n84.68\nGrey\nA\n\n\n31\n171.67\n104.21\nBrown\nB\n\n\n32\n167.83\n78.15\nBrown\nB\n\n\n33\n171.97\n73.39\nGrey\nA\n\n\n34\n170.63\n87.91\nGreen\nA\n\n\n35\n170.29\n86.38\nBlue\nB\n\n\n36\n175.91\n78.67\nBrown\nB\n\n\n37\n155.02\n76.91\nBlue\nA\n\n\n38\n169.87\n77.31\nBlue\nB\n\n\n39\n160.97\n83.04\nGrey\nB\n\n\n40\n181.44\n77.12\nBrown\nB\n\n\n41\n171.61\n80.47\nBlue\nB\n\n\n42\n160.03\n79.89\nGrey\nA\n\n\n43\n180.68\n86.36\nGrey\nB\n\n\n44\n171.72\n58.37\nGrey\nA\n\n\n45\n173.06\n72.72\nBrown\nB\n\n\n46\n165.58\n72.96\nGreen\nA\n\n\n47\n164.36\n71.56\nBlue\nB\n\n\n48\n176.62\n96.55\nGreen\nA\n\n\n49\n166.93\n72.74\nBrown\nB\n\n\n50\n168.36\n61.42\nGrey\nA\n\n\n51\n164.11\n73.31\nGreen\nA\n\n\n52\n174.72\n69.63\nGreen\nA\n\n\n53\n179.73\n96.29\nGreen\nB\n\n\n54\n165.44\n89.68\nGreen\nA\n\n\n55\n166.47\n65.65\nBlue\nA\n\n\n56\n167.54\n79.92\nBrown\nB\n\n\n57\n162.22\n68.93\nGreen\nB\n\n\n58\n166.86\n67.93\nBrown\nB\n\n\n59\n172.37\n88.66\nBlue\nB\n\n\n60\n169.70\n79.99\nGrey\nB\n\n\n61\n181.24\n88.59\nGreen\nA\n\n\n62\n173.09\n75.79\nBrown\nA\n\n\n63\n180.57\n88.09\nBlue\nB\n\n\n64\n168.34\n65.53\nBlue\nA\n\n\n65\n164.15\n77.92\nBlue\nB\n\n\n66\n173.63\n76.08\nGrey\nA\n\n\n67\n161.33\n50.96\nGrey\nA\n\n\n68\n171.08\n87.73\nBlue\nA\n\n\n69\n164.42\n89.42\nGreen\nA\n\n\n70\n158.65\n81.30\nGrey\nB\n\n\n71\n168.36\n77.29\nGrey\nA\n\n\n72\n174.80\n68.38\nGrey\nA\n\n\n73\n176.14\n98.85\nGreen\nA\n\n\n74\n166.37\n85.02\nGrey\nA\n\n\n75\n155.34\n96.23\nGreen\nA\n\n\n76\n167.44\n91.24\nGreen\nA\n\n\n77\n166.41\n82.22\nGreen\nB\n\n\n78\n166.18\n86.13\nGreen\nA\n\n\n79\n163.02\n75.37\nBrown\nB\n\n\n80\n170.98\n86.68\nBlue\nB\n\n\n81\n164.37\n73.72\nBrown\nB\n\n\n82\n160.15\n80.81\nGrey\nB\n\n\n83\n173.82\n89.12\nBlue\nA\n\n\n84\n168.66\n86.82\nBrown\nA\n\n\n85\n176.17\n73.80\nGreen\nA\n\n\n86\n165.34\n98.34\nBrown\nB\n\n\n87\n171.37\n59.93\nGreen\nB\n\n\n88\n164.64\n84.63\nBrown\nA\n\n\n89\n173.24\n77.30\nGrey\nB\n\n\n90\n174.41\n91.50\nBlue\nA\n\n\n91\n167.97\n81.73\nGrey\nA\n\n\n92\n158.54\n86.14\nBlue\nA\n\n\n93\n168.61\n75.99\nGrey\nB\n\n\n94\n162.33\n65.08\nGreen\nB\n\n\n95\n160.27\n89.90\nGreen\nB\n\n\n96\n171.01\n98.48\nGreen\nB\n\n\n97\n167.17\n83.86\nBlue\nB\n\n\n98\n173.52\n79.00\nBlue\nA\n\n\n99\n165.74\n88.70\nGreen\nA\n\n\n100\n163.21\n88.79\nGreen\nA\n\n\n101\n177.41\n83.51\nBrown\nB\n\n\n102\n170.89\n78.99\nGrey\nA\n\n\n103\n157.66\n85.13\nBrown\nA\n\n\n104\n160.34\n63.07\nGreen\nB\n\n\n105\n163.16\n74.19\nBlue\nA\n\n\n106\n158.75\n70.31\nBrown\nA\n\n\n107\n171.91\n89.09\nBlue\nA\n\n\n108\n162.10\n80.92\nGreen\nA\n\n\n109\n171.13\n85.51\nBlue\nB\n\n\n110\n167.04\n96.91\nBlue\nB\n\n\n111\n171.06\n67.56\nGrey\nA\n\n\n112\n172.15\n88.39\nGreen\nB\n\n\n113\n176.48\n84.85\nBrown\nB\n\n\n114\n166.62\n87.99\nBlue\nA\n\n\n115\n174.70\n66.92\nBlue\nA\n\n\n116\n163.54\n102.28\nGrey\nA\n\n\n117\n165.48\n89.71\nGreen\nB\n\n\n118\n155.34\n79.88\nGreen\nB\n\n\n119\n181.99\n87.62\nGrey\nB\n\n\n120\n168.61\n76.69\nBrown\nA\n\n\n121\n163.71\n70.37\nGreen\nB\n\n\n122\n176.03\n90.43\nBrown\nB\n\n\n123\n170.70\n68.23\nBrown\nB\n\n\n124\n171.70\n80.07\nBlue\nB\n\n\n125\n160.91\n57.58\nBlue\nB\n\n\n126\n168.49\n85.05\nBlue\nA\n\n\n127\n165.55\n68.48\nBrown\nA\n\n\n128\n170.33\n85.95\nGreen\nA\n\n\n129\n168.21\n79.27\nGrey\nB\n\n\n130\n166.61\n74.71\nBrown\nA\n\n\n131\n168.09\n79.46\nGreen\nB\n\n\n132\n170.66\n96.20\nGreen\nA\n\n\n133\n162.13\n68.03\nBlue\nB\n\n\n134\n177.47\n71.34\nBrown\nA\n\n\n135\n165.83\n80.94\nBlue\nA\n\n\n136\n177.58\n82.21\nGreen\nB\n\n\n137\n172.26\n78.10\nGrey\nB\n\n\n138\n176.43\n62.73\nBrown\nA\n\n\n139\n171.51\n102.06\nGreen\nB\n\n\n140\n178.49\n82.57\nBlue\nB\n\n\n141\n167.92\n67.29\nGreen\nA\n\n\n142\n167.98\n86.95\nBlue\nA\n\n\n143\n179.43\n84.06\nGrey\nA\n\n\n144\n161.69\n68.49\nGreen\nB\n\n\n145\n164.02\n87.07\nBlue\nB\n\n\n146\n171.64\n60.86\nBlue\nA\n\n\n147\n171.23\n91.08\nGrey\nB\n\n\n148\n175.62\n78.79\nBrown\nB\n\n\n149\n169.01\n97.64\nGreen\nB\n\n\n150\n179.82\n83.21\nBrown\nB\n\n\n151\n163.30\n68.93\nBrown\nB\n\n\n152\n169.15\n76.73\nBlue\nB\n\n\n153\n165.59\n77.66\nGreen\nB\n\n\n154\n168.86\n78.91\nGrey\nB\n\n\n155\n175.22\n91.87\nBrown\nB\n\n\n156\n166.88\n66.71\nBlue\nB\n\n\n157\n169.74\n101.18\nGreen\nB\n\n\n158\n169.29\n81.76\nGreen\nB\n\n\n159\n177.13\n61.47\nGreen\nA\n\n\n160\n174.49\n99.84\nGreen\nA\n\n\n161\n173.27\n87.31\nGrey\nB\n\n\n162\n160.65\n86.83\nBrown\nA\n\n\n163\n175.89\n73.66\nBlue\nA\n\n\n164\n173.27\n65.95\nBrown\nA\n\n\n165\n180.82\n76.49\nGreen\nB\n\n\n166\n168.35\n80.63\nGrey\nB\n\n\n167\n161.14\n70.26\nBrown\nB\n\n\n168\n166.12\n80.28\nGrey\nB\n\n\n169\n174.57\n71.21\nBrown\nA\n\n\n170\n164.05\n76.54\nGreen\nB\n\n\n171\n170.84\n60.52\nGreen\nA\n\n\n172\n169.38\n90.74\nGrey\nB\n\n\n173\n158.51\n85.49\nGreen\nA\n\n\n174\n168.75\n79.91\nBrown\nA\n\n\n175\n168.53\n76.29\nBrown\nB\n\n\n176\n169.74\n73.68\nGrey\nA\n\n\n177\n170.12\n82.79\nBlue\nB\n\n\n178\n172.36\n94.37\nGreen\nA\n\n\n179\n173.28\n65.94\nBrown\nB\n\n\n180\n158.36\n69.75\nGrey\nA\n\n\n181\n175.32\n74.02\nBlue\nB\n\n\n182\n170.36\n81.16\nGreen\nA\n\n\n183\n178.17\n83.29\nBrown\nA\n\n\n184\n168.66\n75.14\nGreen\nB\n\n\n185\n173.14\n77.00\nGrey\nB\n\n\n186\n165.64\n91.08\nGreen\nA\n\n\n187\n175.92\n78.84\nBrown\nB\n\n\n188\n169.93\n66.59\nGrey\nB\n\n\n189\n168.88\n86.20\nBlue\nB\n\n\n190\n172.88\n78.38\nGreen\nA\n\n\n191\n167.17\n82.70\nBlue\nA\n\n\n192\n169.07\n88.74\nGreen\nA\n\n\n193\n169.77\n79.44\nBrown\nB\n\n\n194\n159.38\n86.55\nGreen\nB\n\n\n195\n170.02\n84.49\nGrey\nA\n\n\n196\n160.70\n76.15\nGreen\nA\n\n\n197\n162.99\n89.73\nGreen\nA\n\n\n198\n165.41\n56.54\nGreen\nB\n\n\n199\n161.43\n71.48\nBlue\nB\n\n\n200\n166.06\n90.44\nGrey\nA\n\n\n201\n166.83\n70.75\nBlue\nB\n\n\n202\n176.01\n85.24\nGrey\nB\n\n\n203\n171.49\n65.49\nBlue\nA\n\n\n204\n172.80\n69.30\nBrown\nB\n\n\n205\n168.72\n74.75\nBlue\nB\n\n\n206\n168.54\n75.65\nBrown\nB\n\n\n207\n171.42\n48.03\nBrown\nA\n\n\n208\n175.27\n87.44\nBlue\nB\n\n\n209\n168.28\n90.87\nGreen\nA\n\n\n210\n181.92\n68.40\nBlue\nA\n\n\n211\n165.43\n87.81\nBrown\nA\n\n\n212\n179.36\n83.75\nBrown\nB\n\n\n213\n173.77\n72.80\nGrey\nB\n\n\n214\n167.28\n80.26\nGrey\nA\n\n\n215\n168.76\n93.66\nBlue\nA\n\n\n216\n172.25\n78.19\nBlue\nA\n\n\n217\n166.25\n74.57\nGrey\nB\n\n\n218\n171.80\n76.14\nGreen\nA\n\n\n219\n168.50\n73.20\nBrown\nA\n\n\n220\n167.45\n69.53\nGreen\nA\n\n\n221\n177.77\n91.10\nGrey\nB\n\n\n222\n175.90\n76.32\nGreen\nB\n\n\n223\n177.95\n83.79\nBrown\nA\n\n\n224\n176.93\n87.98\nBrown\nA\n\n\n225\n173.42\n83.36\nGrey\nB\n\n\n226\n184.90\n84.10\nBrown\nA\n\n\n227\n170.05\n55.27\nGrey\nB\n\n\n228\n159.57\n86.02\nGreen\nA\n\n\n229\n175.18\n88.22\nBlue\nB\n\n\n230\n168.10\n91.42\nBrown\nA\n\n\n231\n164.20\n66.63\nBrown\nB\n\n\n232\n165.30\n89.41\nBlue\nA\n\n\n233\n169.29\n70.78\nBlue\nB\n\n\n234\n167.40\n67.80\nBlue\nB\n\n\n235\n175.08\n90.91\nBrown\nB\n\n\n236\n167.12\n65.75\nGreen\nB\n\n\n237\n169.35\n68.78\nGrey\nA\n\n\n238\n173.14\n81.73\nGreen\nB\n\n\n239\n170.92\n82.65\nGrey\nB\n\n\n240\n166.66\n95.30\nGreen\nA\n\n\n241\n161.96\n57.98\nBrown\nB\n\n\n242\n173.02\n57.41\nBlue\nA\n\n\n243\n174.14\n57.52\nGreen\nB\n\n\n244\n159.54\n74.67\nGreen\nB\n\n\n245\n177.56\n66.55\nBrown\nA\n\n\n246\n172.33\n68.01\nBlue\nA\n\n\n247\n170.31\n77.96\nGreen\nA\n\n\n248\n183.41\n65.97\nGreen\nB\n\n\n249\n168.93\n75.41\nBlue\nA\n\n\n250\n157.23\n70.82\nBlue\nA\n\n\n251\n170.43\n90.30\nBlue\nB\n\n\n252\n165.91\n80.87\nBrown\nA\n\n\n253\n173.99\n71.48\nGrey\nA\n\n\n254\n171.86\n73.99\nBlue\nB\n\n\n255\n173.00\n71.55\nBlue\nB\n\n\n256\n166.81\n66.82\nGreen\nB\n\n\n257\n169.47\n83.92\nGreen\nB\n\n\n258\n163.14\n68.18\nGreen\nB\n\n\n259\n170.96\n73.63\nBlue\nA\n\n\n260\n169.76\n90.32\nBlue\nB\n\n\n261\n164.57\n79.52\nGrey\nA\n\n\n262\n171.09\n74.96\nBrown\nB\n\n\n263\n182.81\n84.71\nGrey\nB\n\n\n264\n157.26\n82.23\nBlue\nA\n\n\n265\n178.62\n77.57\nBrown\nB\n\n\n266\n167.95\n71.93\nBlue\nA\n\n\n267\n165.84\n75.13\nBrown\nB\n\n\n268\n157.22\n80.20\nBrown\nB\n\n\n269\n169.55\n84.40\nBrown\nB\n\n\n270\n166.75\n80.27\nGrey\nB\n\n\n271\n160.29\n94.94\nBlue\nB\n\n\n272\n174.13\n87.81\nGreen\nB\n\n\n273\n170.48\n91.79\nBrown\nB\n\n\n274\n158.52\n90.16\nBlue\nB\n\n\n275\n175.57\n70.02\nBlue\nA\n\n\n276\n165.78\n72.29\nGrey\nA\n\n\n277\n162.34\n62.37\nBrown\nB\n\n\n278\n186.29\n78.78\nBrown\nB\n\n\n279\n164.20\n81.83\nGrey\nB\n\n\n280\n165.60\n73.44\nBlue\nB\n\n\n281\n167.50\n61.65\nBlue\nB\n\n\n282\n173.10\n72.15\nBrown\nB\n\n\n283\n172.03\n82.45\nGreen\nA\n\n\n284\n179.34\n82.94\nGrey\nA\n\n\n285\n180.37\n64.45\nGreen\nB\n\n\n286\n180.26\n92.08\nBlue\nB\n\n\n287\n162.82\n80.10\nGreen\nA\n\n\n288\n163.48\n77.86\nGrey\nB\n\n\n289\n177.84\n90.93\nGrey\nA\n\n\n290\n167.19\n73.73\nBrown\nB\n\n\n291\n161.33\n66.57\nBrown\nB\n\n\n292\n163.76\n70.94\nGreen\nA\n\n\n293\n175.46\n82.02\nGreen\nA\n\n\n294\n160.63\n70.61\nGreen\nB\n\n\n295\n169.44\n62.60\nBrown\nA\n\n\n296\n170.23\n80.45\nBlue\nA\n\n\n297\n156.04\n84.73\nGrey\nA\n\n\n298\n177.95\n81.07\nBlue\nA\n\n\n299\n160.54\n75.62\nGrey\nA\n\n\n300\n170.91\n70.18\nBrown\nA\n\n\n301\n176.86\n94.99\nBrown\nA\n\n\n302\n171.33\n67.79\nBlue\nB\n\n\n303\n175.45\n75.93\nBlue\nB\n\n\n304\n170.62\n68.55\nGreen\nA\n\n\n305\n168.17\n83.77\nBrown\nA\n\n\n306\n173.84\n71.72\nBrown\nA\n\n\n307\n159.01\n73.11\nGreen\nB\n\n\n308\n166.08\n89.02\nGreen\nA\n\n\n309\n176.74\n64.43\nGrey\nB\n\n\n310\n172.34\n89.83\nBrown\nB\n\n\n311\n163.58\n87.20\nBrown\nB\n\n\n312\n174.14\n77.71\nGreen\nA\n\n\n313\n155.61\n89.70\nBlue\nA\n\n\n314\n162.92\n83.94\nGrey\nA\n\n\n315\n178.39\n71.42\nGreen\nA\n\n\n316\n175.89\n77.52\nGrey\nB\n\n\n317\n171.22\n89.30\nGrey\nA\n\n\n318\n164.02\n77.52\nGreen\nB\n\n\n319\n161.22\n90.84\nGreen\nB\n\n\n320\n176.77\n88.15\nBrown\nA\n\n\n321\n178.65\n72.52\nBlue\nA\n\n\n322\n166.52\n90.45\nBlue\nB\n\n\n323\n179.48\n91.96\nBlue\nA\n\n\n324\n176.15\n74.59\nGrey\nB\n\n\n325\n171.16\n83.82\nBrown\nA\n\n\n326\n177.57\n70.86\nBlue\nB\n\n\n327\n175.48\n77.32\nGreen\nA\n\n\n328\n178.58\n80.32\nGreen\nB\n\n\n329\n164.07\n67.66\nBlue\nA\n\n\n330\n175.18\n74.10\nBrown\nB\n\n\n331\n161.35\n55.65\nGrey\nB\n\n\n332\n169.80\n80.75\nBlue\nA\n\n\n333\n169.63\n93.98\nBrown\nB\n\n\n334\n170.97\n75.78\nGrey\nA\n\n\n335\n172.17\n86.89\nGreen\nA\n\n\n336\n175.11\n86.04\nBlue\nA\n\n\n337\n178.89\n76.03\nGrey\nB\n\n\n338\n165.26\n85.85\nGrey\nB\n\n\n339\n165.95\n82.71\nGrey\nB\n\n\n340\n166.84\n87.37\nGrey\nB\n\n\n341\n171.02\n72.19\nGreen\nA\n\n\n342\n173.37\n95.15\nGreen\nA\n\n\n343\n165.46\n83.44\nGrey\nA\n\n\n344\n172.23\n77.78\nGreen\nB\n\n\n345\n173.60\n77.39\nBlue\nB\n\n\n346\n163.58\n77.77\nBrown\nB\n\n\n347\n171.49\n82.36\nBrown\nB\n\n\n348\n165.26\n78.44\nGrey\nB\n\n\n349\n162.94\n91.14\nGreen\nA\n\n\n350\n175.12\n97.98\nGreen\nB\n\n\n351\n174.74\n74.10\nBlue\nA\n\n\n352\n176.31\n72.38\nBrown\nB\n\n\n353\n176.32\n71.56\nGreen\nB\n\n\n354\n166.29\n62.28\nBlue\nB\n\n\n355\n171.00\n85.88\nGreen\nB\n\n\n356\n172.94\n85.40\nBrown\nA\n\n\n357\n177.44\n68.67\nBlue\nB\n\n\n358\n160.64\n83.10\nBrown\nA\n\n\n359\n176.62\n73.81\nBlue\nA\n\n\n360\n173.02\n87.66\nBlue\nA\n\n\n361\n163.30\n86.61\nBlue\nB\n\n\n362\n174.35\n73.65\nBrown\nA\n\n\n363\n179.27\n66.98\nBrown\nB\n\n\n364\n169.05\n73.35\nBrown\nA\n\n\n365\n171.59\n98.92\nBlue\nA\n\n\n366\n166.86\n81.15\nGreen\nA\n\n\n367\n164.47\n80.63\nBlue\nB\n\n\n368\n174.85\n88.93\nGrey\nB\n\n\n369\n169.17\n68.39\nGrey\nB\n\n\n370\n174.99\n71.34\nBrown\nA\n\n\n371\n174.55\n82.42\nGreen\nA\n\n\n372\n169.69\n84.97\nBrown\nA\n\n\n373\n164.98\n98.79\nBrown\nB\n\n\n374\n171.01\n75.99\nBrown\nB\n\n\n375\n168.87\n83.22\nBlue\nA\n\n\n376\n168.98\n80.16\nGreen\nB\n\n\n377\n169.61\n75.51\nGrey\nA\n\n\n378\n168.95\n98.18\nBlue\nA\n\n\n379\n170.04\n75.47\nGreen\nB\n\n\n380\n178.15\n80.74\nBrown\nB\n\n\n381\n170.86\n76.62\nGrey\nB\n\n\n382\n168.63\n69.44\nGrey\nA\n\n\n383\n169.77\n93.70\nBrown\nB\n\n\n384\n155.82\n85.51\nGrey\nB\n\n\n385\n158.99\n86.62\nGreen\nB\n\n\n386\n163.80\n86.46\nGrey\nA\n\n\n387\n164.59\n81.46\nBrown\nB\n\n\n388\n165.72\n92.99\nGrey\nB\n\n\n389\n161.43\n88.09\nGrey\nA\n\n\n390\n170.34\n86.97\nBrown\nA\n\n\n391\n163.21\n72.31\nBlue\nA\n\n\n392\n162.23\n88.33\nGreen\nB\n\n\n393\n180.83\n84.00\nBrown\nB\n\n\n394\n171.08\n80.97\nGreen\nB\n\n\n395\n181.84\n96.79\nGrey\nA\n\n\n396\n179.67\n95.43\nGreen\nB\n\n\n397\n169.64\n80.95\nBrown\nA\n\n\n398\n163.42\n91.17\nGreen\nA\n\n\n399\n164.58\n81.44\nBlue\nB\n\n\n400\n162.01\n94.56\nGreen\nA\n\n\n401\n166.94\n105.58\nGreen\nB\n\n\n402\n170.64\n79.27\nBrown\nB\n\n\n403\n181.23\n79.44\nGrey\nB\n\n\n404\n162.34\n75.26\nGrey\nA\n\n\n405\n167.55\n66.60\nBrown\nB\n\n\n406\n176.62\n76.30\nBlue\nA\n\n\n407\n159.02\n66.77\nBlue\nA\n\n\n408\n169.57\n77.50\nGrey\nA\n\n\n409\n173.24\n63.78\nBrown\nA\n\n\n410\n169.17\n82.77\nGreen\nB\n\n\n411\n171.18\n69.27\nBrown\nA\n\n\n412\n174.43\n58.19\nGreen\nB\n\n\n413\n166.06\n76.84\nBlue\nA\n\n\n414\n165.87\n74.95\nBrown\nB\n\n\n415\n170.64\n84.10\nGrey\nA\n\n\n416\n182.75\n80.60\nBrown\nA\n\n\n417\n168.04\n76.03\nGrey\nA\n\n\n418\n164.45\n100.22\nBrown\nA\n\n\n419\n162.04\n67.72\nBrown\nA\n\n\n420\n169.99\n84.94\nGreen\nA\n\n\n421\n176.92\n87.04\nGrey\nB\n\n\n422\n170.59\n79.15\nGreen\nB\n\n\n423\n160.99\n71.60\nBrown\nB\n\n\n424\n163.96\n86.42\nBrown\nB\n\n\n425\n162.62\n85.93\nGrey\nB\n\n\n426\n176.67\n58.55\nBrown\nA\n\n\n427\n163.91\n83.32\nGreen\nA\n\n\n428\n171.31\n73.43\nGreen\nB\n\n\n429\n166.62\n99.34\nBlue\nB\n\n\n430\n178.65\n62.36\nBlue\nB\n\n\n431\n171.67\n79.68\nBrown\nB\n\n\n432\n169.91\n84.26\nGrey\nA\n\n\n433\n166.36\n66.86\nGrey\nA\n\n\n434\n171.39\n88.02\nGrey\nA\n\n\n435\n169.66\n71.90\nGreen\nA\n\n\n436\n180.01\n60.59\nGrey\nA\n\n\n437\n171.27\n69.92\nGreen\nA\n\n\n438\n166.37\n75.06\nBrown\nB\n\n\n439\n172.52\n88.86\nBlue\nB\n\n\n440\n169.89\n86.78\nGreen\nB\n\n\n441\n171.11\n81.62\nGreen\nB\n\n\n442\n157.74\n84.24\nGrey\nA\n\n\n443\n158.42\n86.95\nBrown\nB\n\n\n444\n169.86\n82.49\nGreen\nA\n\n\n445\n180.74\n66.95\nGrey\nA\n\n\n446\n178.86\n86.22\nGrey\nA\n\n\n447\n181.83\n80.26\nGreen\nA\n\n\n448\n167.92\n86.38\nGrey\nA\n\n\n449\n167.20\n60.45\nBrown\nB\n\n\n450\n157.00\n109.69\nGreen\nB\n\n\n451\n175.13\n85.79\nGrey\nB\n\n\n452\n172.68\n97.45\nBrown\nA\n\n\n453\n169.39\n70.13\nGreen\nA\n\n\n454\n164.87\n85.52\nBlue\nB\n\n\n455\n174.14\n69.48\nGrey\nA\n\n\n456\n178.85\n62.86\nBrown\nB\n\n\n457\n161.78\n85.78\nBlue\nA\n\n\n458\n170.88\n78.04\nBrown\nB\n\n\n459\n169.38\n69.96\nBrown\nA\n\n\n460\n162.51\n83.56\nBlue\nB\n\n\n461\n164.87\n67.12\nGreen\nB\n\n\n462\n174.35\n69.83\nBrown\nA\n\n\n463\n172.82\n81.86\nBlue\nB\n\n\n464\n167.08\n78.35\nBrown\nA\n\n\n465\n174.23\n95.48\nGrey\nA\n\n\n466\n171.22\n90.52\nGreen\nB\n\n\n467\n168.46\n84.97\nBrown\nA\n\n\n468\n189.57\n77.43\nGreen\nA\n\n\n469\n174.85\n88.70\nBrown\nB\n\n\n470\n165.29\n80.47\nBlue\nA\n\n\n471\n171.87\n89.64\nGrey\nA\n\n\n472\n169.52\n79.42\nGreen\nB\n\n\n473\n166.05\n90.97\nGreen\nB\n\n\n474\n168.86\n66.72\nGrey\nB\n\n\n475\n185.59\n75.30\nBrown\nB\n\n\n476\n174.81\n72.62\nBlue\nB\n\n\n477\n171.50\n91.48\nGreen\nA\n\n\n478\n181.42\n70.84\nGreen\nA\n\n\n479\n172.63\n61.30\nBrown\nA\n\n\n480\n164.68\n86.60\nBlue\nB\n\n\n481\n170.60\n70.49\nBrown\nA\n\n\n482\n161.53\n90.85\nBlue\nA\n\n\n483\n165.57\n96.40\nBlue\nB\n\n\n484\n172.50\n69.94\nBrown\nA\n\n\n485\n161.80\n87.26\nBrown\nB\n\n\n486\n168.51\n89.39\nBrown\nB\n\n\n487\n166.50\n90.21\nBlue\nB\n\n\n488\n165.63\n77.65\nBrown\nA\n\n\n489\n165.10\n74.46\nBlue\nB\n\n\n490\n180.50\n76.57\nGrey\nB\n\n\n491\n174.84\n66.39\nBrown\nA\n\n\n492\n174.10\n85.40\nBlue\nA\n\n\n493\n169.37\n65.70\nBrown\nA\n\n\n494\n165.84\n75.62\nGreen\nB\n\n\n495\n174.03\n91.78\nBrown\nA\n\n\n496\n161.91\n76.69\nGreen\nB\n\n\n497\n165.82\n84.49\nBrown\nB\n\n\n498\n176.45\n77.73\nGreen\nB\n\n\n499\n163.76\n84.89\nGreen\nA\n\n\n500\n175.34\n74.79\nGrey\nA\n\n\n501\n176.19\n79.02\nGreen\nB\n\n\n502\n177.80\n68.79\nBrown\nA\n\n\n503\n166.40\n74.26\nBrown\nB\n\n\n504\n171.40\n64.85\nBrown\nB\n\n\n505\n172.94\n89.82\nBrown\nA\n\n\n506\n158.85\n95.25\nBlue\nA\n\n\n507\n178.19\n95.69\nBlue\nA\n\n\n508\n174.64\n71.65\nBrown\nA\n\n\n509\n173.31\n61.63\nGreen\nA\n\n\n510\n168.41\n89.21\nGrey\nA\n\n\n511\n167.37\n83.13\nBrown\nB\n\n\n512\n163.14\n77.23\nBrown\nB\n\n\n513\n154.94\n78.91\nBrown\nB\n\n\n514\n177.54\n84.86\nGrey\nB\n\n\n515\n168.35\n80.72\nGrey\nB\n\n\n516\n165.30\n71.59\nGreen\nA\n\n\n517\n171.38\n90.15\nGrey\nB\n\n\n518\n178.61\n78.10\nBrown\nA\n\n\n519\n169.61\n86.23\nGreen\nB\n\n\n520\n173.33\n71.01\nGrey\nA\n\n\n521\n167.47\n72.63\nGrey\nB\n\n\n522\n163.41\n98.54\nBlue\nB\n\n\n523\n161.36\n86.59\nGreen\nB\n\n\n524\n168.90\n88.02\nBrown\nA\n\n\n525\n170.51\n79.95\nGrey\nB\n\n\n526\n170.22\n80.82\nBrown\nA\n\n\n527\n176.16\n76.19\nBlue\nB\n\n\n528\n185.36\n82.77\nGrey\nA\n\n\n529\n180.28\n92.57\nGrey\nB\n\n\n530\n166.17\n91.87\nBrown\nA\n\n\n531\n170.42\n74.92\nBlue\nB\n\n\n532\n166.80\n99.50\nGreen\nB\n\n\n533\n162.70\n91.72\nBrown\nB\n\n\n534\n179.32\n87.12\nGreen\nB\n\n\n535\n171.36\n74.33\nGreen\nA\n\n\n536\n164.12\n75.92\nBrown\nB\n\n\n537\n181.18\n82.27\nBlue\nB\n\n\n538\n170.65\n75.14\nBlue\nA\n\n\n539\n170.26\n97.25\nBrown\nA\n\n\n540\n165.03\n93.62\nGreen\nA\n\n\n541\n168.03\n87.08\nBlue\nB\n\n\n542\n172.68\n74.72\nGrey\nA\n\n\n543\n171.32\n88.08\nBrown\nB\n\n\n544\n167.97\n76.15\nBrown\nA\n\n\n545\n175.27\n76.56\nGreen\nA\n\n\n546\n174.32\n87.44\nBrown\nA\n\n\n547\n173.53\n90.18\nBlue\nB\n\n\n548\n170.63\n80.79\nGrey\nB\n\n\n549\n170.35\n83.07\nGrey\nA\n\n\n550\n179.55\n81.76\nGrey\nA\n\n\n551\n170.29\n84.38\nGrey\nB\n\n\n552\n173.69\n81.86\nBrown\nA\n\n\n553\n165.75\n83.78\nGrey\nB\n\n\n554\n174.85\n92.61\nBlue\nA\n\n\n555\n165.42\n58.67\nBrown\nB\n\n\n556\n168.46\n77.87\nGrey\nB\n\n\n557\n180.39\n87.33\nGrey\nA\n\n\n558\n173.58\n72.97\nGrey\nA\n\n\n559\n175.95\n86.27\nBrown\nB\n\n\n560\n165.24\n94.69\nBlue\nB\n\n\n561\n162.92\n80.14\nBlue\nB\n\n\n562\n168.76\n78.44\nGrey\nB\n\n\n563\n154.97\n69.71\nGrey\nA\n\n\n564\n166.60\n73.44\nBlue\nB\n\n\n565\n178.39\n82.86\nBlue\nA\n\n\n566\n170.43\n67.07\nBrown\nB\n\n\n567\n181.70\n80.12\nGreen\nB\n\n\n568\n167.61\n60.33\nBlue\nA\n\n\n569\n172.11\n75.91\nGreen\nA\n\n\n570\n164.07\n84.60\nBlue\nA\n\n\n571\n171.30\n94.47\nGrey\nA\n\n\n572\n165.82\n86.66\nGreen\nB\n\n\n573\n177.87\n103.07\nGrey\nB\n\n\n574\n167.39\n93.45\nGrey\nA\n\n\n575\n170.40\n65.86\nGrey\nB\n\n\n576\n172.24\n71.97\nGrey\nA\n\n\n577\n177.66\n66.35\nBlue\nB\n\n\n578\n158.44\n71.75\nBrown\nB\n\n\n579\n167.43\n69.87\nGreen\nB\n\n\n580\n177.33\n83.16\nBrown\nA\n\n\n581\n167.91\n89.62\nBrown\nA\n\n\n582\n168.51\n84.52\nGrey\nA\n\n\n583\n166.01\n81.04\nBlue\nB\n\n\n584\n167.88\n84.78\nBrown\nB\n\n\n585\n169.27\n94.25\nBlue\nA\n\n\n586\n162.54\n76.27\nGreen\nA\n\n\n587\n163.42\n81.45\nGreen\nA\n\n\n588\n169.89\n92.00\nBlue\nA\n\n\n589\n182.90\n79.78\nGrey\nA\n\n\n590\n170.13\n77.98\nBlue\nA\n\n\n591\n160.36\n46.75\nGreen\nA\n\n\n592\n163.62\n84.18\nGrey\nA\n\n\n593\n176.19\n74.64\nBrown\nA\n\n\n594\n182.83\n98.21\nBrown\nA\n\n\n595\n155.29\n78.63\nGreen\nA\n\n\n596\n181.12\n75.33\nBlue\nB\n\n\n597\n170.06\n94.14\nGreen\nB\n\n\n598\n160.65\n93.49\nBlue\nA\n\n\n599\n173.07\n71.78\nBlue\nB\n\n\n600\n182.79\n94.18\nGreen\nB\n\n\n601\n177.19\n79.02\nGreen\nA\n\n\n602\n166.34\n81.89\nBlue\nB\n\n\n603\n160.35\n85.94\nGreen\nB\n\n\n604\n169.03\n76.71\nBrown\nB\n\n\n605\n175.38\n61.18\nBrown\nB\n\n\n606\n175.66\n67.94\nGrey\nA\n\n\n607\n167.22\n70.38\nGreen\nB\n\n\n608\n172.13\n74.97\nGreen\nA\n\n\n609\n168.19\n64.19\nGrey\nA\n\n\n610\n175.92\n58.16\nGrey\nA\n\n\n611\n156.76\n76.10\nBrown\nB\n\n\n612\n171.81\n80.61\nGreen\nB\n\n\n613\n176.90\n89.31\nBrown\nB\n\n\n614\n180.21\n91.56\nBlue\nA\n\n\n615\n166.66\n75.97\nGrey\nA\n\n\n616\n169.62\n55.54\nGrey\nA\n\n\n617\n169.89\n68.11\nBrown\nB\n\n\n618\n177.50\n71.99\nBlue\nB\n\n\n619\n177.44\n90.41\nBrown\nB\n\n\n620\n157.05\n67.50\nGrey\nA\n\n\n621\n161.00\n77.55\nBrown\nB\n\n\n622\n171.80\n79.21\nGreen\nB\n\n\n623\n170.97\n84.51\nGrey\nB\n\n\n624\n178.70\n77.99\nGreen\nB\n\n\n625\n178.94\n95.44\nBrown\nB\n\n\n626\n167.65\n72.41\nBrown\nB\n\n\n627\n169.16\n76.84\nBrown\nA\n\n\n628\n163.32\n72.46\nBlue\nA\n\n\n629\n167.14\n77.53\nBrown\nB\n\n\n630\n164.91\n79.02\nBrown\nB\n\n\n631\n167.98\n82.24\nGreen\nB\n\n\n632\n181.76\n95.77\nGreen\nA\n\n\n633\n164.63\n85.10\nBrown\nB\n\n\n634\n172.93\n77.99\nGreen\nA\n\n\n635\n165.38\n94.88\nBrown\nA\n\n\n636\n170.85\n82.02\nBrown\nB\n\n\n637\n172.30\n91.33\nGrey\nA\n\n\n638\n178.09\n75.24\nGreen\nA\n\n\n639\n168.69\n67.43\nGrey\nA\n\n\n640\n176.22\n79.82\nGrey\nA\n\n\n641\n181.02\n79.45\nGrey\nA\n\n\n642\n167.53\n56.23\nGreen\nB\n\n\n643\n175.85\n83.48\nGreen\nB\n\n\n644\n182.90\n73.67\nGreen\nA\n\n\n645\n173.30\n86.46\nGrey\nB\n\n\n646\n174.78\n84.77\nBlue\nA\n\n\n647\n174.23\n82.70\nBrown\nB\n\n\n648\n175.45\n89.24\nBrown\nB\n\n\n649\n169.58\n77.48\nGrey\nB\n\n\n650\n171.74\n94.38\nGreen\nA\n\n\n651\n173.35\n74.55\nBrown\nB\n\n\n652\n165.43\n74.13\nBlue\nB\n\n\n653\n170.78\n85.91\nBlue\nA\n\n\n654\n175.53\n96.12\nGreen\nB\n\n\n655\n181.40\n85.99\nBlue\nA\n\n\n656\n169.12\n76.91\nGrey\nA\n\n\n657\n174.09\n78.85\nBrown\nA\n\n\n658\n169.49\n85.84\nGrey\nB\n\n\n659\n176.24\n76.18\nGreen\nA\n\n\n660\n172.30\n82.70\nBrown\nB\n\n\n661\n167.43\n83.55\nGreen\nA\n\n\n662\n171.17\n91.64\nGreen\nA\n\n\n663\n168.20\n77.96\nBlue\nB\n\n\n664\n166.97\n94.01\nGreen\nB\n\n\n665\n167.31\n60.13\nGrey\nA\n\n\n666\n167.29\n75.21\nBrown\nB\n\n\n667\n178.23\n81.45\nBlue\nB\n\n\n668\n166.20\n67.92\nGrey\nB\n\n\n669\n169.02\n87.46\nBlue\nA\n\n\n670\n167.85\n72.56\nBlue\nA\n\n\n671\n158.30\n78.00\nGreen\nA\n\n\n672\n180.64\n73.73\nBlue\nB\n\n\n673\n174.76\n78.45\nBlue\nA\n\n\n674\n160.15\n67.49\nBlue\nA\n\n\n675\n173.60\n88.69\nBrown\nA\n\n\n676\n183.11\n87.03\nGrey\nB\n\n\n677\n162.92\n87.31\nBlue\nA\n\n\n678\n167.58\n83.90\nGrey\nA\n\n\n679\n170.65\n99.31\nGrey\nB\n\n\n680\n170.93\n87.30\nBrown\nA\n\n\n681\n174.07\n85.59\nBrown\nB\n\n\n682\n158.26\n62.89\nBlue\nA\n\n\n683\n171.36\n92.67\nBrown\nB\n\n\n684\n167.58\n90.09\nBlue\nA\n\n\n685\n160.64\n74.61\nBlue\nB\n\n\n686\n167.97\n86.69\nGrey\nA\n\n\n687\n171.31\n60.75\nBrown\nB\n\n\n688\n170.41\n85.50\nGreen\nB\n\n\n689\n178.91\n82.58\nGrey\nB\n\n\n690\n170.73\n79.52\nBlue\nB\n\n\n691\n182.26\n70.28\nGreen\nB\n\n\n692\n169.20\n83.84\nGrey\nA\n\n\n693\n175.44\n84.43\nBlue\nA\n\n\n694\n171.66\n81.24\nBlue\nB\n\n\n695\n168.24\n81.01\nGreen\nB\n\n\n696\n170.88\n81.51\nBlue\nA\n\n\n697\n179.31\n68.41\nBrown\nA\n\n\n698\n173.38\n58.64\nBlue\nA\n\n\n699\n171.78\n81.16\nGreen\nB\n\n\n700\n176.31\n66.76\nBlue\nA\n\n\n701\n173.27\n81.98\nBlue\nA\n\n\n702\n180.20\n80.06\nBrown\nB\n\n\n703\n163.07\n76.68\nBlue\nB\n\n\n704\n178.05\n85.70\nBlue\nA\n\n\n705\n169.52\n97.65\nGrey\nA\n\n\n706\n174.09\n80.44\nBlue\nA\n\n\n707\n162.67\n83.88\nGreen\nA\n\n\n708\n162.50\n97.09\nBrown\nA\n\n\n709\n174.33\n70.61\nBrown\nB\n\n\n710\n164.46\n75.45\nBrown\nA\n\n\n711\n160.98\n86.42\nGrey\nB\n\n\n712\n164.09\n75.25\nBrown\nA\n\n\n713\n165.32\n99.93\nBlue\nB\n\n\n714\n165.29\n92.95\nBrown\nB\n\n\n715\n171.09\n70.43\nGrey\nB\n\n\n716\n176.25\n73.65\nBlue\nB\n\n\n717\n170.16\n88.82\nBrown\nB\n\n\n718\n167.17\n94.42\nGreen\nA\n\n\n719\n176.76\n87.31\nBlue\nB\n\n\n720\n167.00\n86.57\nBlue\nA\n\n\n721\n163.93\n94.25\nGreen\nB\n\n\n722\n177.56\n77.72\nGrey\nB\n\n\n723\n157.02\n85.21\nGreen\nB\n\n\n724\n171.27\n99.56\nBlue\nA\n\n\n725\n170.19\n73.13\nBrown\nB\n\n\n726\n168.61\n85.38\nBlue\nA\n\n\n727\n174.91\n92.50\nBrown\nB\n\n\n728\n174.96\n78.84\nBrown\nA\n\n\n729\n167.10\n68.13\nGrey\nB\n\n\n730\n159.56\n70.02\nBlue\nA\n\n\n731\n171.81\n86.07\nGreen\nA\n\n\n732\n172.12\n77.30\nGrey\nB\n\n\n733\n169.40\n98.71\nGreen\nA\n\n\n734\n172.63\n90.88\nBlue\nB\n\n\n735\n172.14\n81.16\nBlue\nA\n\n\n736\n171.49\n93.67\nGreen\nB\n\n\n737\n164.27\n75.76\nGrey\nA\n\n\n738\n174.75\n71.92\nBlue\nB\n\n\n739\n178.49\n65.73\nBrown\nB\n\n\n740\n162.42\n73.07\nBlue\nA\n\n\n741\n173.25\n65.75\nGreen\nB\n\n\n742\n161.81\n74.14\nGrey\nA\n\n\n743\n167.29\n76.99\nGreen\nB\n\n\n744\n167.55\n70.70\nBlue\nB\n\n\n745\n166.57\n90.76\nBrown\nB\n\n\n746\n165.45\n74.49\nGreen\nA\n\n\n747\n166.44\n61.70\nBrown\nA\n\n\n748\n171.62\n77.31\nBlue\nB\n\n\n749\n182.53\n96.66\nBlue\nB\n\n\n750\n175.17\n98.71\nGrey\nB\n\n\n751\n170.34\n64.02\nGrey\nA\n\n\n752\n182.65\n75.77\nBrown\nB\n\n\n753\n164.66\n83.33\nBlue\nA\n\n\n754\n173.94\n90.77\nGreen\nA\n\n\n755\n174.00\n70.61\nGreen\nA\n\n\n756\n167.35\n73.08\nGreen\nB\n\n\n757\n164.04\n70.76\nGrey\nA\n\n\n758\n167.68\n86.44\nBlue\nB\n\n\n759\n172.91\n59.66\nBrown\nA\n\n\n760\n172.79\n87.22\nGreen\nA\n\n\n761\n165.05\n87.25\nGrey\nB\n\n\n762\n170.16\n72.49\nBrown\nB\n\n\n763\n174.74\n85.97\nBrown\nA\n\n\n764\n166.85\n75.14\nBrown\nA\n\n\n765\n171.46\n84.01\nGrey\nB\n\n\n766\n171.50\n82.97\nGrey\nA\n\n\n767\n166.24\n55.69\nGreen\nA\n\n\n768\n174.29\n74.66\nBlue\nB\n\n\n769\n168.51\n93.75\nBrown\nA\n\n\n770\n176.74\n70.11\nGreen\nA\n\n\n771\n169.57\n82.94\nGrey\nB\n\n\n772\n176.23\n81.36\nBrown\nA\n\n\n773\n172.81\n84.42\nGrey\nB\n\n\n774\n171.68\n78.66\nGreen\nB\n\n\n775\n164.13\n62.75\nGreen\nB\n\n\n776\n170.14\n86.67\nBrown\nB\n\n\n777\n160.52\n87.41\nBlue\nB\n\n\n778\n164.57\n58.84\nBrown\nB\n\n\n779\n150.87\n81.67\nBlue\nA\n\n\n780\n159.05\n86.09\nBlue\nB\n\n\n781\n167.98\n85.85\nGreen\nB\n\n\n782\n164.63\n65.40\nBrown\nB\n\n\n783\n158.62\n74.42\nBrown\nA\n\n\n784\n172.93\n88.11\nBrown\nA\n\n\n785\n174.53\n92.38\nGreen\nA\n\n\n786\n172.78\n76.66\nBlue\nA\n\n\n787\n169.17\n88.52\nBrown\nB\n\n\n788\n171.81\n94.81\nGrey\nA\n\n\n789\n167.42\n87.61\nGrey\nB\n\n\n790\n175.75\n76.07\nGrey\nA\n\n\n791\n162.83\n78.39\nBrown\nA\n\n\n792\n170.01\n79.74\nBrown\nA\n\n\n793\n175.42\n110.79\nBlue\nA\n\n\n794\n174.97\n88.70\nGrey\nB\n\n\n795\n169.20\n84.18\nGreen\nA\n\n\n796\n175.68\n79.86\nGreen\nA\n\n\n797\n172.42\n67.29\nBlue\nB\n\n\n798\n170.98\n73.88\nBlue\nA\n\n\n799\n165.07\n80.21\nBrown\nA\n\n\n800\n177.50\n63.72\nGreen\nB\n\n\n801\n166.25\n70.62\nBrown\nB\n\n\n802\n166.30\n77.81\nGrey\nB\n\n\n803\n181.52\n85.54\nBrown\nA\n\n\n804\n175.26\n60.75\nGrey\nA\n\n\n805\n173.38\n70.63\nBrown\nB\n\n\n806\n164.51\n78.71\nBlue\nA\n\n\n807\n171.78\n84.71\nBlue\nB\n\n\n808\n165.66\n68.92\nGreen\nB\n\n\n809\n167.41\n69.24\nBlue\nA\n\n\n810\n168.40\n68.86\nGrey\nA\n\n\n811\n172.10\n74.32\nGrey\nA\n\n\n812\n174.84\n69.74\nGrey\nB\n\n\n813\n168.22\n76.41\nGrey\nA\n\n\n814\n175.35\n80.89\nGrey\nA\n\n\n815\n168.27\n86.86\nGrey\nA\n\n\n816\n161.11\n60.64\nGreen\nB\n\n\n817\n171.52\n96.93\nGreen\nA\n\n\n818\n168.70\n71.37\nBrown\nB\n\n\n819\n172.92\n71.97\nGrey\nB\n\n\n820\n157.79\n78.13\nBrown\nB\n\n\n821\n160.46\n84.63\nGreen\nA\n\n\n822\n167.48\n73.23\nBlue\nA\n\n\n823\n164.42\n87.27\nBrown\nB\n\n\n824\n167.16\n85.06\nGreen\nB\n\n\n825\n164.22\n76.83\nBlue\nB\n\n\n826\n167.38\n85.58\nGrey\nA\n\n\n827\n167.92\n88.72\nBlue\nA\n\n\n828\n169.62\n70.35\nBlue\nA\n\n\n829\n169.46\n76.79\nGreen\nB\n\n\n830\n169.55\n69.88\nGreen\nB\n\n\n831\n180.36\n80.48\nBrown\nB\n\n\n832\n169.79\n96.97\nGrey\nA\n\n\n833\n168.62\n90.98\nGrey\nB\n\n\n834\n165.75\n78.29\nBlue\nA\n\n\n835\n178.58\n97.01\nBrown\nB\n\n\n836\n170.68\n88.18\nBrown\nB\n\n\n837\n170.55\n90.80\nGrey\nB\n\n\n838\n174.44\n106.58\nBlue\nA\n\n\n839\n159.85\n71.98\nGrey\nB\n\n\n840\n164.98\n80.27\nGreen\nB\n\n\n841\n168.35\n70.84\nGrey\nB\n\n\n842\n175.64\n73.48\nBrown\nA\n\n\n843\n164.38\n77.01\nBlue\nB\n\n\n844\n171.51\n82.79\nBrown\nA\n\n\n845\n164.49\n94.40\nGrey\nB\n\n\n846\n165.46\n91.62\nBrown\nA\n\n\n847\n167.85\n71.71\nGreen\nA\n\n\n848\n158.98\n105.06\nGrey\nA\n\n\n849\n165.87\n86.96\nGrey\nB\n\n\n850\n176.04\n71.93\nGrey\nB\n\n\n851\n177.04\n102.43\nGreen\nB\n\n\n852\n177.15\n71.39\nBlue\nA\n\n\n853\n168.60\n85.07\nBrown\nA\n\n\n854\n169.89\n80.78\nGreen\nA\n\n\n855\n177.50\n94.62\nGrey\nB\n\n\n856\n162.37\n87.83\nBrown\nB\n\n\n857\n174.27\n77.46\nBrown\nB\n\n\n858\n169.69\n68.66\nBrown\nB\n\n\n859\n168.12\n67.13\nGrey\nA\n\n\n860\n176.64\n89.09\nGreen\nA\n\n\n861\n170.16\n80.37\nBlue\nB\n\n\n862\n176.18\n77.91\nBlue\nB\n\n\n863\n181.13\n94.62\nGreen\nA\n\n\n864\n169.48\n55.62\nBlue\nA\n\n\n865\n160.23\n76.41\nGreen\nB\n\n\n866\n173.59\n64.94\nGreen\nA\n\n\n867\n177.58\n59.83\nGreen\nA\n\n\n868\n158.73\n77.83\nBlue\nB\n\n\n869\n166.51\n59.24\nGrey\nA\n\n\n870\n171.16\n76.54\nGreen\nA\n\n\n871\n166.75\n87.15\nBlue\nA\n\n\n872\n167.07\n74.78\nBlue\nB\n\n\n873\n170.56\n65.90\nBlue\nA\n\n\n874\n170.69\n92.34\nBrown\nB\n\n\n875\n178.77\n72.60\nGrey\nA\n\n\n876\n184.79\n80.71\nBrown\nB\n\n\n877\n169.02\n81.13\nBrown\nA\n\n\n878\n177.64\n72.06\nBlue\nA\n\n\n879\n178.17\n79.74\nBrown\nB\n\n\n880\n168.18\n86.66\nGreen\nB\n\n\n881\n173.20\n73.21\nGreen\nB\n\n\n882\n160.10\n74.50\nGrey\nB\n\n\n883\n166.98\n75.43\nBlue\nB\n\n\n884\n162.33\n69.60\nBrown\nB\n\n\n885\n155.45\n75.84\nBrown\nA\n\n\n886\n165.47\n90.28\nBrown\nA\n\n\n887\n156.23\n84.61\nBlue\nA\n\n\n888\n167.94\n72.79\nGreen\nB\n\n\n889\n169.33\n80.79\nBlue\nB\n\n\n890\n170.70\n93.83\nBrown\nB\n\n\n891\n179.78\n91.38\nGreen\nA\n\n\n892\n174.66\n85.08\nGrey\nB\n\n\n893\n166.21\n72.35\nGreen\nB\n\n\n894\n173.43\n86.37\nBrown\nA\n\n\n895\n176.87\n73.21\nBrown\nB\n\n\n896\n171.07\n80.50\nGreen\nB\n\n\n897\n171.75\n83.00\nBrown\nA\n\n\n898\n167.14\n82.98\nBrown\nA\n\n\n899\n164.90\n78.77\nGrey\nA\n\n\n900\n173.61\n84.14\nGrey\nA\n\n\n901\n169.68\n80.68\nBrown\nA\n\n\n902\n160.71\n69.94\nGrey\nB\n\n\n903\n168.64\n78.60\nBlue\nA\n\n\n904\n167.91\n73.80\nGrey\nA\n\n\n905\n174.40\n66.62\nBrown\nB\n\n\n906\n163.77\n81.82\nBrown\nA\n\n\n907\n175.53\n62.93\nBrown\nB\n\n\n908\n171.69\n93.54\nGreen\nB\n\n\n909\n164.33\n69.30\nBlue\nA\n\n\n910\n164.88\n75.57\nBrown\nA\n\n\n911\n175.25\n93.02\nGrey\nB\n\n\n912\n174.47\n62.93\nBrown\nB\n\n\n913\n176.47\n72.97\nGreen\nB\n\n\n914\n173.87\n77.61\nBlue\nB\n\n\n915\n175.21\n76.88\nBrown\nB\n\n\n916\n162.28\n70.08\nBlue\nA\n\n\n917\n169.21\n95.13\nGreen\nB\n\n\n918\n166.93\n88.95\nBlue\nA\n\n\n919\n171.42\n75.58\nGreen\nB\n\n\n920\n168.69\n84.96\nGrey\nA\n\n\n921\n179.67\n66.91\nGreen\nA\n\n\n922\n173.80\n89.03\nGreen\nB\n\n\n923\n177.50\n77.55\nGreen\nA\n\n\n924\n163.92\n64.20\nBlue\nA\n\n\n925\n164.48\n84.95\nGreen\nB\n\n\n926\n167.46\n86.53\nGrey\nB\n\n\n927\n168.09\n94.25\nBlue\nA\n\n\n928\n179.06\n77.81\nBlue\nB\n\n\n929\n173.58\n94.34\nBrown\nA\n\n\n930\n162.63\n59.21\nGrey\nA\n\n\n931\n168.75\n78.31\nGreen\nA\n\n\n932\n165.95\n72.21\nGreen\nB\n\n\n933\n166.54\n93.45\nGreen\nA\n\n\n934\n172.17\n86.66\nBlue\nA\n\n\n935\n161.55\n84.90\nBlue\nA\n\n\n936\n175.02\n74.64\nBrown\nB\n\n\n937\n173.98\n75.45\nGreen\nA\n\n\n938\n170.29\n77.07\nGreen\nB\n\n\n939\n172.37\n60.96\nGreen\nA\n\n\n940\n174.49\n57.51\nBlue\nA\n\n\n941\n178.04\n76.22\nBrown\nA\n\n\n942\n178.74\n67.61\nBlue\nB\n\n\n943\n166.51\n59.70\nGrey\nA\n\n\n944\n174.46\n89.68\nBrown\nB\n\n\n945\n174.72\n87.87\nBlue\nB\n\n\n946\n169.27\n97.67\nGreen\nB\n\n\n947\n174.95\n72.39\nGreen\nB\n\n\n948\n174.66\n76.67\nGreen\nA\n\n\n949\n172.80\n84.31\nGreen\nB\n\n\n950\n170.07\n88.15\nGrey\nB\n\n\n951\n165.03\n85.43\nBlue\nB\n\n\n952\n167.46\n85.82\nGrey\nA\n\n\n953\n168.82\n84.41\nBrown\nB\n\n\n954\n173.16\n85.40\nGrey\nA\n\n\n955\n170.28\n95.64\nBrown\nB\n\n\n956\n180.22\n62.63\nBrown\nA\n\n\n957\n179.28\n88.18\nGrey\nB\n\n\n958\n174.96\n84.55\nGrey\nA\n\n\n959\n170.05\n79.81\nBlue\nB\n\n\n960\n171.82\n72.94\nGreen\nA\n\n\n961\n174.35\n87.19\nGreen\nB\n\n\n962\n162.12\n64.63\nGreen\nB\n\n\n963\n172.52\n72.67\nGrey\nB\n\n\n964\n169.63\n81.66\nGrey\nB\n\n\n965\n180.01\n71.99\nGrey\nA\n\n\n966\n171.54\n79.13\nBrown\nA\n\n\n967\n164.75\n88.90\nGreen\nA\n\n\n968\n174.17\n87.39\nGrey\nA\n\n\n969\n180.01\n69.61\nGrey\nA\n\n\n970\n178.25\n68.80\nBrown\nA\n\n\n971\n170.21\n71.18\nGrey\nA\n\n\n972\n177.13\n73.50\nBlue\nA\n\n\n973\n176.37\n87.15\nGrey\nA\n\n\n974\n172.05\n98.06\nBlue\nA\n\n\n975\n165.59\n78.70\nBlue\nA\n\n\n976\n172.64\n77.49\nGreen\nA\n\n\n977\n175.23\n86.77\nGreen\nA\n\n\n978\n167.40\n72.21\nGrey\nA\n\n\n979\n183.37\n81.27\nBlue\nB\n\n\n980\n162.39\n77.16\nBlue\nA\n\n\n981\n169.06\n81.30\nGreen\nA\n\n\n982\n175.67\n81.96\nBlue\nA\n\n\n983\n163.60\n70.58\nGrey\nB\n\n\n984\n171.01\n64.23\nBlue\nB\n\n\n985\n177.98\n69.07\nGrey\nB\n\n\n986\n172.03\n85.05\nGrey\nB\n\n\n987\n166.31\n77.44\nGreen\nA\n\n\n988\n182.59\n78.23\nGrey\nA\n\n\n989\n160.62\n93.77\nBrown\nA\n\n\n990\n174.78\n64.95\nBrown\nA\n\n\n991\n173.50\n80.96\nGrey\nA\n\n\n992\n176.38\n86.78\nBlue\nB\n\n\n993\n167.54\n86.58\nBrown\nA\n\n\n994\n159.14\n84.11\nBrown\nB\n\n\n995\n168.76\n76.32\nGreen\nA\n\n\n996\n165.34\n88.06\nBlue\nA\n\n\n997\n172.12\n83.62\nBrown\nB\n\n\n998\n166.88\n69.48\nGrey\nA\n\n\n999\n173.81\n67.12\nGreen\nB\n\n\n1000\n171.24\n78.69\nGrey\nA"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#how-similar-are-these-groups",
    "href": "content/slides/01-07-experiments.html#how-similar-are-these-groups",
    "title": "Causal Effects and Experiments",
    "section": "How similar are these groups?",
    "text": "How similar are these groups?\nLet’s first check their heights:"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#how-similar-are-these-groups-1",
    "href": "content/slides/01-07-experiments.html#how-similar-are-these-groups-1",
    "title": "Causal Effects and Experiments",
    "section": "How similar are these groups?",
    "text": "How similar are these groups?\nAnd their weights:"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#how-similar-are-these-groups-2",
    "href": "content/slides/01-07-experiments.html#how-similar-are-these-groups-2",
    "title": "Causal Effects and Experiments",
    "section": "How similar are these groups?",
    "text": "How similar are these groups?\nAnd their eye colors:"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#making-sure-this-wasnt-a-fluke",
    "href": "content/slides/01-07-experiments.html#making-sure-this-wasnt-a-fluke",
    "title": "Causal Effects and Experiments",
    "section": "Making sure this wasn’t a fluke",
    "text": "Making sure this wasn’t a fluke\nLet’s re-run this:\n\n\n\n\n\nID\nHeight\nWeight\nEye colour\nGroup\n\n\n\n\n1\n176.70\n73.11\nGreen\nB\n\n\n2\n173.55\n68.74\nBlue\nB\n\n\n3\n171.55\n92.57\nBrown\nB\n\n\n4\n181.01\n74.95\nBlue\nB\n\n\n5\n183.80\n97.08\nBlue\nA\n\n\n6\n173.27\n74.24\nBlue\nA\n\n\n7\n174.98\n77.08\nBrown\nA\n\n\n8\n179.96\n78.62\nBlue\nA\n\n\n9\n172.05\n77.39\nBlue\nB\n\n\n10\n167.73\n78.41\nBrown\nB\n\n\n11\n160.50\n84.77\nBlue\nA\n\n\n12\n166.59\n75.98\nGrey\nA\n\n\n13\n166.95\n74.71\nBlue\nB\n\n\n14\n168.57\n64.72\nBlue\nB\n\n\n15\n169.37\n75.80\nBlue\nA\n\n\n16\n167.70\n84.09\nBrown\nA\n\n\n17\n171.61\n94.79\nGreen\nA\n\n\n18\n168.93\n62.46\nBrown\nB\n\n\n19\n170.26\n101.50\nBrown\nA\n\n\n20\n170.47\n85.83\nGrey\nA\n\n\n21\n171.18\n62.35\nBrown\nA\n\n\n22\n171.54\n70.39\nBlue\nA\n\n\n23\n172.77\n93.21\nBrown\nA\n\n\n24\n163.10\n79.40\nGrey\nA\n\n\n25\n180.80\n67.30\nBlue\nB\n\n\n26\n167.67\n88.59\nGrey\nB\n\n\n27\n158.11\n75.07\nGreen\nB\n\n\n28\n166.25\n83.27\nBlue\nA\n\n\n29\n173.64\n78.02\nBlue\nA\n\n\n30\n163.98\n84.68\nGrey\nB\n\n\n31\n171.67\n104.21\nBrown\nA\n\n\n32\n167.83\n78.15\nBrown\nA\n\n\n33\n171.97\n73.39\nGrey\nA\n\n\n34\n170.63\n87.91\nGreen\nB\n\n\n35\n170.29\n86.38\nBlue\nA\n\n\n36\n175.91\n78.67\nBrown\nB\n\n\n37\n155.02\n76.91\nBlue\nA\n\n\n38\n169.87\n77.31\nBlue\nA\n\n\n39\n160.97\n83.04\nGrey\nB\n\n\n40\n181.44\n77.12\nBrown\nA\n\n\n41\n171.61\n80.47\nBlue\nB\n\n\n42\n160.03\n79.89\nGrey\nA\n\n\n43\n180.68\n86.36\nGrey\nB\n\n\n44\n171.72\n58.37\nGrey\nA\n\n\n45\n173.06\n72.72\nBrown\nB\n\n\n46\n165.58\n72.96\nGreen\nA\n\n\n47\n164.36\n71.56\nBlue\nB\n\n\n48\n176.62\n96.55\nGreen\nA\n\n\n49\n166.93\n72.74\nBrown\nA\n\n\n50\n168.36\n61.42\nGrey\nB\n\n\n51\n164.11\n73.31\nGreen\nA\n\n\n52\n174.72\n69.63\nGreen\nA\n\n\n53\n179.73\n96.29\nGreen\nA\n\n\n54\n165.44\n89.68\nGreen\nA\n\n\n55\n166.47\n65.65\nBlue\nB\n\n\n56\n167.54\n79.92\nBrown\nA\n\n\n57\n162.22\n68.93\nGreen\nA\n\n\n58\n166.86\n67.93\nBrown\nA\n\n\n59\n172.37\n88.66\nBlue\nB\n\n\n60\n169.70\n79.99\nGrey\nA\n\n\n61\n181.24\n88.59\nGreen\nB\n\n\n62\n173.09\n75.79\nBrown\nB\n\n\n63\n180.57\n88.09\nBlue\nA\n\n\n64\n168.34\n65.53\nBlue\nA\n\n\n65\n164.15\n77.92\nBlue\nA\n\n\n66\n173.63\n76.08\nGrey\nA\n\n\n67\n161.33\n50.96\nGrey\nB\n\n\n68\n171.08\n87.73\nBlue\nB\n\n\n69\n164.42\n89.42\nGreen\nB\n\n\n70\n158.65\n81.30\nGrey\nA\n\n\n71\n168.36\n77.29\nGrey\nB\n\n\n72\n174.80\n68.38\nGrey\nB\n\n\n73\n176.14\n98.85\nGreen\nB\n\n\n74\n166.37\n85.02\nGrey\nA\n\n\n75\n155.34\n96.23\nGreen\nA\n\n\n76\n167.44\n91.24\nGreen\nA\n\n\n77\n166.41\n82.22\nGreen\nB\n\n\n78\n166.18\n86.13\nGreen\nB\n\n\n79\n163.02\n75.37\nBrown\nB\n\n\n80\n170.98\n86.68\nBlue\nA\n\n\n81\n164.37\n73.72\nBrown\nB\n\n\n82\n160.15\n80.81\nGrey\nA\n\n\n83\n173.82\n89.12\nBlue\nA\n\n\n84\n168.66\n86.82\nBrown\nA\n\n\n85\n176.17\n73.80\nGreen\nA\n\n\n86\n165.34\n98.34\nBrown\nB\n\n\n87\n171.37\n59.93\nGreen\nA\n\n\n88\n164.64\n84.63\nBrown\nA\n\n\n89\n173.24\n77.30\nGrey\nA\n\n\n90\n174.41\n91.50\nBlue\nB\n\n\n91\n167.97\n81.73\nGrey\nB\n\n\n92\n158.54\n86.14\nBlue\nA\n\n\n93\n168.61\n75.99\nGrey\nB\n\n\n94\n162.33\n65.08\nGreen\nB\n\n\n95\n160.27\n89.90\nGreen\nA\n\n\n96\n171.01\n98.48\nGreen\nB\n\n\n97\n167.17\n83.86\nBlue\nB\n\n\n98\n173.52\n79.00\nBlue\nA\n\n\n99\n165.74\n88.70\nGreen\nB\n\n\n100\n163.21\n88.79\nGreen\nA\n\n\n101\n177.41\n83.51\nBrown\nA\n\n\n102\n170.89\n78.99\nGrey\nB\n\n\n103\n157.66\n85.13\nBrown\nB\n\n\n104\n160.34\n63.07\nGreen\nA\n\n\n105\n163.16\n74.19\nBlue\nA\n\n\n106\n158.75\n70.31\nBrown\nB\n\n\n107\n171.91\n89.09\nBlue\nA\n\n\n108\n162.10\n80.92\nGreen\nA\n\n\n109\n171.13\n85.51\nBlue\nA\n\n\n110\n167.04\n96.91\nBlue\nB\n\n\n111\n171.06\n67.56\nGrey\nB\n\n\n112\n172.15\n88.39\nGreen\nA\n\n\n113\n176.48\n84.85\nBrown\nA\n\n\n114\n166.62\n87.99\nBlue\nB\n\n\n115\n174.70\n66.92\nBlue\nB\n\n\n116\n163.54\n102.28\nGrey\nB\n\n\n117\n165.48\n89.71\nGreen\nB\n\n\n118\n155.34\n79.88\nGreen\nA\n\n\n119\n181.99\n87.62\nGrey\nB\n\n\n120\n168.61\n76.69\nBrown\nA\n\n\n121\n163.71\n70.37\nGreen\nB\n\n\n122\n176.03\n90.43\nBrown\nA\n\n\n123\n170.70\n68.23\nBrown\nB\n\n\n124\n171.70\n80.07\nBlue\nA\n\n\n125\n160.91\n57.58\nBlue\nA\n\n\n126\n168.49\n85.05\nBlue\nB\n\n\n127\n165.55\n68.48\nBrown\nA\n\n\n128\n170.33\n85.95\nGreen\nB\n\n\n129\n168.21\n79.27\nGrey\nB\n\n\n130\n166.61\n74.71\nBrown\nA\n\n\n131\n168.09\n79.46\nGreen\nA\n\n\n132\n170.66\n96.20\nGreen\nB\n\n\n133\n162.13\n68.03\nBlue\nB\n\n\n134\n177.47\n71.34\nBrown\nA\n\n\n135\n165.83\n80.94\nBlue\nA\n\n\n136\n177.58\n82.21\nGreen\nA\n\n\n137\n172.26\n78.10\nGrey\nA\n\n\n138\n176.43\n62.73\nBrown\nA\n\n\n139\n171.51\n102.06\nGreen\nB\n\n\n140\n178.49\n82.57\nBlue\nB\n\n\n141\n167.92\n67.29\nGreen\nB\n\n\n142\n167.98\n86.95\nBlue\nB\n\n\n143\n179.43\n84.06\nGrey\nA\n\n\n144\n161.69\n68.49\nGreen\nA\n\n\n145\n164.02\n87.07\nBlue\nA\n\n\n146\n171.64\n60.86\nBlue\nA\n\n\n147\n171.23\n91.08\nGrey\nA\n\n\n148\n175.62\n78.79\nBrown\nB\n\n\n149\n169.01\n97.64\nGreen\nB\n\n\n150\n179.82\n83.21\nBrown\nB\n\n\n151\n163.30\n68.93\nBrown\nB\n\n\n152\n169.15\n76.73\nBlue\nA\n\n\n153\n165.59\n77.66\nGreen\nA\n\n\n154\n168.86\n78.91\nGrey\nA\n\n\n155\n175.22\n91.87\nBrown\nB\n\n\n156\n166.88\n66.71\nBlue\nB\n\n\n157\n169.74\n101.18\nGreen\nB\n\n\n158\n169.29\n81.76\nGreen\nA\n\n\n159\n177.13\n61.47\nGreen\nB\n\n\n160\n174.49\n99.84\nGreen\nB\n\n\n161\n173.27\n87.31\nGrey\nA\n\n\n162\n160.65\n86.83\nBrown\nB\n\n\n163\n175.89\n73.66\nBlue\nA\n\n\n164\n173.27\n65.95\nBrown\nB\n\n\n165\n180.82\n76.49\nGreen\nA\n\n\n166\n168.35\n80.63\nGrey\nA\n\n\n167\n161.14\n70.26\nBrown\nA\n\n\n168\n166.12\n80.28\nGrey\nA\n\n\n169\n174.57\n71.21\nBrown\nA\n\n\n170\n164.05\n76.54\nGreen\nA\n\n\n171\n170.84\n60.52\nGreen\nA\n\n\n172\n169.38\n90.74\nGrey\nA\n\n\n173\n158.51\n85.49\nGreen\nB\n\n\n174\n168.75\n79.91\nBrown\nB\n\n\n175\n168.53\n76.29\nBrown\nB\n\n\n176\n169.74\n73.68\nGrey\nB\n\n\n177\n170.12\n82.79\nBlue\nB\n\n\n178\n172.36\n94.37\nGreen\nB\n\n\n179\n173.28\n65.94\nBrown\nB\n\n\n180\n158.36\n69.75\nGrey\nA\n\n\n181\n175.32\n74.02\nBlue\nA\n\n\n182\n170.36\n81.16\nGreen\nA\n\n\n183\n178.17\n83.29\nBrown\nA\n\n\n184\n168.66\n75.14\nGreen\nA\n\n\n185\n173.14\n77.00\nGrey\nA\n\n\n186\n165.64\n91.08\nGreen\nA\n\n\n187\n175.92\n78.84\nBrown\nA\n\n\n188\n169.93\n66.59\nGrey\nB\n\n\n189\n168.88\n86.20\nBlue\nB\n\n\n190\n172.88\n78.38\nGreen\nA\n\n\n191\n167.17\n82.70\nBlue\nA\n\n\n192\n169.07\n88.74\nGreen\nB\n\n\n193\n169.77\n79.44\nBrown\nB\n\n\n194\n159.38\n86.55\nGreen\nB\n\n\n195\n170.02\n84.49\nGrey\nB\n\n\n196\n160.70\n76.15\nGreen\nA\n\n\n197\n162.99\n89.73\nGreen\nA\n\n\n198\n165.41\n56.54\nGreen\nA\n\n\n199\n161.43\n71.48\nBlue\nB\n\n\n200\n166.06\n90.44\nGrey\nB\n\n\n201\n166.83\n70.75\nBlue\nB\n\n\n202\n176.01\n85.24\nGrey\nB\n\n\n203\n171.49\n65.49\nBlue\nA\n\n\n204\n172.80\n69.30\nBrown\nA\n\n\n205\n168.72\n74.75\nBlue\nA\n\n\n206\n168.54\n75.65\nBrown\nB\n\n\n207\n171.42\n48.03\nBrown\nA\n\n\n208\n175.27\n87.44\nBlue\nB\n\n\n209\n168.28\n90.87\nGreen\nA\n\n\n210\n181.92\n68.40\nBlue\nA\n\n\n211\n165.43\n87.81\nBrown\nA\n\n\n212\n179.36\n83.75\nBrown\nA\n\n\n213\n173.77\n72.80\nGrey\nA\n\n\n214\n167.28\n80.26\nGrey\nB\n\n\n215\n168.76\n93.66\nBlue\nA\n\n\n216\n172.25\n78.19\nBlue\nA\n\n\n217\n166.25\n74.57\nGrey\nB\n\n\n218\n171.80\n76.14\nGreen\nB\n\n\n219\n168.50\n73.20\nBrown\nB\n\n\n220\n167.45\n69.53\nGreen\nA\n\n\n221\n177.77\n91.10\nGrey\nB\n\n\n222\n175.90\n76.32\nGreen\nB\n\n\n223\n177.95\n83.79\nBrown\nB\n\n\n224\n176.93\n87.98\nBrown\nA\n\n\n225\n173.42\n83.36\nGrey\nA\n\n\n226\n184.90\n84.10\nBrown\nB\n\n\n227\n170.05\n55.27\nGrey\nB\n\n\n228\n159.57\n86.02\nGreen\nA\n\n\n229\n175.18\n88.22\nBlue\nA\n\n\n230\n168.10\n91.42\nBrown\nA\n\n\n231\n164.20\n66.63\nBrown\nB\n\n\n232\n165.30\n89.41\nBlue\nA\n\n\n233\n169.29\n70.78\nBlue\nA\n\n\n234\n167.40\n67.80\nBlue\nA\n\n\n235\n175.08\n90.91\nBrown\nA\n\n\n236\n167.12\n65.75\nGreen\nB\n\n\n237\n169.35\n68.78\nGrey\nA\n\n\n238\n173.14\n81.73\nGreen\nA\n\n\n239\n170.92\n82.65\nGrey\nB\n\n\n240\n166.66\n95.30\nGreen\nA\n\n\n241\n161.96\n57.98\nBrown\nB\n\n\n242\n173.02\n57.41\nBlue\nB\n\n\n243\n174.14\n57.52\nGreen\nB\n\n\n244\n159.54\n74.67\nGreen\nB\n\n\n245\n177.56\n66.55\nBrown\nA\n\n\n246\n172.33\n68.01\nBlue\nB\n\n\n247\n170.31\n77.96\nGreen\nB\n\n\n248\n183.41\n65.97\nGreen\nB\n\n\n249\n168.93\n75.41\nBlue\nA\n\n\n250\n157.23\n70.82\nBlue\nB\n\n\n251\n170.43\n90.30\nBlue\nA\n\n\n252\n165.91\n80.87\nBrown\nB\n\n\n253\n173.99\n71.48\nGrey\nB\n\n\n254\n171.86\n73.99\nBlue\nA\n\n\n255\n173.00\n71.55\nBlue\nB\n\n\n256\n166.81\n66.82\nGreen\nB\n\n\n257\n169.47\n83.92\nGreen\nB\n\n\n258\n163.14\n68.18\nGreen\nA\n\n\n259\n170.96\n73.63\nBlue\nB\n\n\n260\n169.76\n90.32\nBlue\nB\n\n\n261\n164.57\n79.52\nGrey\nB\n\n\n262\n171.09\n74.96\nBrown\nA\n\n\n263\n182.81\n84.71\nGrey\nA\n\n\n264\n157.26\n82.23\nBlue\nB\n\n\n265\n178.62\n77.57\nBrown\nA\n\n\n266\n167.95\n71.93\nBlue\nB\n\n\n267\n165.84\n75.13\nBrown\nA\n\n\n268\n157.22\n80.20\nBrown\nB\n\n\n269\n169.55\n84.40\nBrown\nB\n\n\n270\n166.75\n80.27\nGrey\nA\n\n\n271\n160.29\n94.94\nBlue\nA\n\n\n272\n174.13\n87.81\nGreen\nB\n\n\n273\n170.48\n91.79\nBrown\nA\n\n\n274\n158.52\n90.16\nBlue\nA\n\n\n275\n175.57\n70.02\nBlue\nA\n\n\n276\n165.78\n72.29\nGrey\nB\n\n\n277\n162.34\n62.37\nBrown\nB\n\n\n278\n186.29\n78.78\nBrown\nA\n\n\n279\n164.20\n81.83\nGrey\nA\n\n\n280\n165.60\n73.44\nBlue\nB\n\n\n281\n167.50\n61.65\nBlue\nA\n\n\n282\n173.10\n72.15\nBrown\nB\n\n\n283\n172.03\n82.45\nGreen\nB\n\n\n284\n179.34\n82.94\nGrey\nA\n\n\n285\n180.37\n64.45\nGreen\nB\n\n\n286\n180.26\n92.08\nBlue\nB\n\n\n287\n162.82\n80.10\nGreen\nB\n\n\n288\n163.48\n77.86\nGrey\nB\n\n\n289\n177.84\n90.93\nGrey\nA\n\n\n290\n167.19\n73.73\nBrown\nB\n\n\n291\n161.33\n66.57\nBrown\nB\n\n\n292\n163.76\n70.94\nGreen\nB\n\n\n293\n175.46\n82.02\nGreen\nA\n\n\n294\n160.63\n70.61\nGreen\nA\n\n\n295\n169.44\n62.60\nBrown\nB\n\n\n296\n170.23\n80.45\nBlue\nA\n\n\n297\n156.04\n84.73\nGrey\nA\n\n\n298\n177.95\n81.07\nBlue\nB\n\n\n299\n160.54\n75.62\nGrey\nB\n\n\n300\n170.91\n70.18\nBrown\nA\n\n\n301\n176.86\n94.99\nBrown\nA\n\n\n302\n171.33\n67.79\nBlue\nB\n\n\n303\n175.45\n75.93\nBlue\nB\n\n\n304\n170.62\n68.55\nGreen\nB\n\n\n305\n168.17\n83.77\nBrown\nB\n\n\n306\n173.84\n71.72\nBrown\nB\n\n\n307\n159.01\n73.11\nGreen\nB\n\n\n308\n166.08\n89.02\nGreen\nB\n\n\n309\n176.74\n64.43\nGrey\nA\n\n\n310\n172.34\n89.83\nBrown\nA\n\n\n311\n163.58\n87.20\nBrown\nA\n\n\n312\n174.14\n77.71\nGreen\nA\n\n\n313\n155.61\n89.70\nBlue\nB\n\n\n314\n162.92\n83.94\nGrey\nB\n\n\n315\n178.39\n71.42\nGreen\nB\n\n\n316\n175.89\n77.52\nGrey\nA\n\n\n317\n171.22\n89.30\nGrey\nB\n\n\n318\n164.02\n77.52\nGreen\nB\n\n\n319\n161.22\n90.84\nGreen\nB\n\n\n320\n176.77\n88.15\nBrown\nA\n\n\n321\n178.65\n72.52\nBlue\nB\n\n\n322\n166.52\n90.45\nBlue\nA\n\n\n323\n179.48\n91.96\nBlue\nA\n\n\n324\n176.15\n74.59\nGrey\nB\n\n\n325\n171.16\n83.82\nBrown\nB\n\n\n326\n177.57\n70.86\nBlue\nA\n\n\n327\n175.48\n77.32\nGreen\nB\n\n\n328\n178.58\n80.32\nGreen\nB\n\n\n329\n164.07\n67.66\nBlue\nB\n\n\n330\n175.18\n74.10\nBrown\nB\n\n\n331\n161.35\n55.65\nGrey\nB\n\n\n332\n169.80\n80.75\nBlue\nA\n\n\n333\n169.63\n93.98\nBrown\nA\n\n\n334\n170.97\n75.78\nGrey\nB\n\n\n335\n172.17\n86.89\nGreen\nA\n\n\n336\n175.11\n86.04\nBlue\nB\n\n\n337\n178.89\n76.03\nGrey\nA\n\n\n338\n165.26\n85.85\nGrey\nA\n\n\n339\n165.95\n82.71\nGrey\nB\n\n\n340\n166.84\n87.37\nGrey\nA\n\n\n341\n171.02\n72.19\nGreen\nA\n\n\n342\n173.37\n95.15\nGreen\nA\n\n\n343\n165.46\n83.44\nGrey\nA\n\n\n344\n172.23\n77.78\nGreen\nA\n\n\n345\n173.60\n77.39\nBlue\nA\n\n\n346\n163.58\n77.77\nBrown\nB\n\n\n347\n171.49\n82.36\nBrown\nB\n\n\n348\n165.26\n78.44\nGrey\nB\n\n\n349\n162.94\n91.14\nGreen\nB\n\n\n350\n175.12\n97.98\nGreen\nA\n\n\n351\n174.74\n74.10\nBlue\nB\n\n\n352\n176.31\n72.38\nBrown\nB\n\n\n353\n176.32\n71.56\nGreen\nA\n\n\n354\n166.29\n62.28\nBlue\nB\n\n\n355\n171.00\n85.88\nGreen\nA\n\n\n356\n172.94\n85.40\nBrown\nB\n\n\n357\n177.44\n68.67\nBlue\nB\n\n\n358\n160.64\n83.10\nBrown\nB\n\n\n359\n176.62\n73.81\nBlue\nA\n\n\n360\n173.02\n87.66\nBlue\nB\n\n\n361\n163.30\n86.61\nBlue\nA\n\n\n362\n174.35\n73.65\nBrown\nA\n\n\n363\n179.27\n66.98\nBrown\nA\n\n\n364\n169.05\n73.35\nBrown\nB\n\n\n365\n171.59\n98.92\nBlue\nA\n\n\n366\n166.86\n81.15\nGreen\nA\n\n\n367\n164.47\n80.63\nBlue\nB\n\n\n368\n174.85\n88.93\nGrey\nA\n\n\n369\n169.17\n68.39\nGrey\nA\n\n\n370\n174.99\n71.34\nBrown\nB\n\n\n371\n174.55\n82.42\nGreen\nA\n\n\n372\n169.69\n84.97\nBrown\nB\n\n\n373\n164.98\n98.79\nBrown\nA\n\n\n374\n171.01\n75.99\nBrown\nA\n\n\n375\n168.87\n83.22\nBlue\nB\n\n\n376\n168.98\n80.16\nGreen\nA\n\n\n377\n169.61\n75.51\nGrey\nA\n\n\n378\n168.95\n98.18\nBlue\nA\n\n\n379\n170.04\n75.47\nGreen\nA\n\n\n380\n178.15\n80.74\nBrown\nA\n\n\n381\n170.86\n76.62\nGrey\nA\n\n\n382\n168.63\n69.44\nGrey\nA\n\n\n383\n169.77\n93.70\nBrown\nA\n\n\n384\n155.82\n85.51\nGrey\nA\n\n\n385\n158.99\n86.62\nGreen\nB\n\n\n386\n163.80\n86.46\nGrey\nB\n\n\n387\n164.59\n81.46\nBrown\nB\n\n\n388\n165.72\n92.99\nGrey\nA\n\n\n389\n161.43\n88.09\nGrey\nA\n\n\n390\n170.34\n86.97\nBrown\nA\n\n\n391\n163.21\n72.31\nBlue\nA\n\n\n392\n162.23\n88.33\nGreen\nB\n\n\n393\n180.83\n84.00\nBrown\nB\n\n\n394\n171.08\n80.97\nGreen\nB\n\n\n395\n181.84\n96.79\nGrey\nB\n\n\n396\n179.67\n95.43\nGreen\nA\n\n\n397\n169.64\n80.95\nBrown\nA\n\n\n398\n163.42\n91.17\nGreen\nA\n\n\n399\n164.58\n81.44\nBlue\nB\n\n\n400\n162.01\n94.56\nGreen\nA\n\n\n401\n166.94\n105.58\nGreen\nA\n\n\n402\n170.64\n79.27\nBrown\nA\n\n\n403\n181.23\n79.44\nGrey\nA\n\n\n404\n162.34\n75.26\nGrey\nB\n\n\n405\n167.55\n66.60\nBrown\nA\n\n\n406\n176.62\n76.30\nBlue\nB\n\n\n407\n159.02\n66.77\nBlue\nB\n\n\n408\n169.57\n77.50\nGrey\nB\n\n\n409\n173.24\n63.78\nBrown\nB\n\n\n410\n169.17\n82.77\nGreen\nA\n\n\n411\n171.18\n69.27\nBrown\nB\n\n\n412\n174.43\n58.19\nGreen\nA\n\n\n413\n166.06\n76.84\nBlue\nA\n\n\n414\n165.87\n74.95\nBrown\nB\n\n\n415\n170.64\n84.10\nGrey\nA\n\n\n416\n182.75\n80.60\nBrown\nA\n\n\n417\n168.04\n76.03\nGrey\nB\n\n\n418\n164.45\n100.22\nBrown\nB\n\n\n419\n162.04\n67.72\nBrown\nB\n\n\n420\n169.99\n84.94\nGreen\nA\n\n\n421\n176.92\n87.04\nGrey\nB\n\n\n422\n170.59\n79.15\nGreen\nB\n\n\n423\n160.99\n71.60\nBrown\nB\n\n\n424\n163.96\n86.42\nBrown\nA\n\n\n425\n162.62\n85.93\nGrey\nB\n\n\n426\n176.67\n58.55\nBrown\nB\n\n\n427\n163.91\n83.32\nGreen\nB\n\n\n428\n171.31\n73.43\nGreen\nB\n\n\n429\n166.62\n99.34\nBlue\nB\n\n\n430\n178.65\n62.36\nBlue\nA\n\n\n431\n171.67\n79.68\nBrown\nA\n\n\n432\n169.91\n84.26\nGrey\nA\n\n\n433\n166.36\n66.86\nGrey\nA\n\n\n434\n171.39\n88.02\nGrey\nA\n\n\n435\n169.66\n71.90\nGreen\nB\n\n\n436\n180.01\n60.59\nGrey\nB\n\n\n437\n171.27\n69.92\nGreen\nB\n\n\n438\n166.37\n75.06\nBrown\nB\n\n\n439\n172.52\n88.86\nBlue\nB\n\n\n440\n169.89\n86.78\nGreen\nB\n\n\n441\n171.11\n81.62\nGreen\nB\n\n\n442\n157.74\n84.24\nGrey\nA\n\n\n443\n158.42\n86.95\nBrown\nB\n\n\n444\n169.86\n82.49\nGreen\nA\n\n\n445\n180.74\n66.95\nGrey\nB\n\n\n446\n178.86\n86.22\nGrey\nB\n\n\n447\n181.83\n80.26\nGreen\nB\n\n\n448\n167.92\n86.38\nGrey\nA\n\n\n449\n167.20\n60.45\nBrown\nA\n\n\n450\n157.00\n109.69\nGreen\nB\n\n\n451\n175.13\n85.79\nGrey\nB\n\n\n452\n172.68\n97.45\nBrown\nA\n\n\n453\n169.39\n70.13\nGreen\nB\n\n\n454\n164.87\n85.52\nBlue\nB\n\n\n455\n174.14\n69.48\nGrey\nA\n\n\n456\n178.85\n62.86\nBrown\nA\n\n\n457\n161.78\n85.78\nBlue\nB\n\n\n458\n170.88\n78.04\nBrown\nA\n\n\n459\n169.38\n69.96\nBrown\nB\n\n\n460\n162.51\n83.56\nBlue\nB\n\n\n461\n164.87\n67.12\nGreen\nB\n\n\n462\n174.35\n69.83\nBrown\nA\n\n\n463\n172.82\n81.86\nBlue\nA\n\n\n464\n167.08\n78.35\nBrown\nA\n\n\n465\n174.23\n95.48\nGrey\nA\n\n\n466\n171.22\n90.52\nGreen\nB\n\n\n467\n168.46\n84.97\nBrown\nA\n\n\n468\n189.57\n77.43\nGreen\nA\n\n\n469\n174.85\n88.70\nBrown\nB\n\n\n470\n165.29\n80.47\nBlue\nA\n\n\n471\n171.87\n89.64\nGrey\nB\n\n\n472\n169.52\n79.42\nGreen\nA\n\n\n473\n166.05\n90.97\nGreen\nB\n\n\n474\n168.86\n66.72\nGrey\nB\n\n\n475\n185.59\n75.30\nBrown\nA\n\n\n476\n174.81\n72.62\nBlue\nA\n\n\n477\n171.50\n91.48\nGreen\nB\n\n\n478\n181.42\n70.84\nGreen\nB\n\n\n479\n172.63\n61.30\nBrown\nA\n\n\n480\n164.68\n86.60\nBlue\nB\n\n\n481\n170.60\n70.49\nBrown\nA\n\n\n482\n161.53\n90.85\nBlue\nA\n\n\n483\n165.57\n96.40\nBlue\nA\n\n\n484\n172.50\n69.94\nBrown\nA\n\n\n485\n161.80\n87.26\nBrown\nB\n\n\n486\n168.51\n89.39\nBrown\nA\n\n\n487\n166.50\n90.21\nBlue\nB\n\n\n488\n165.63\n77.65\nBrown\nA\n\n\n489\n165.10\n74.46\nBlue\nB\n\n\n490\n180.50\n76.57\nGrey\nA\n\n\n491\n174.84\n66.39\nBrown\nA\n\n\n492\n174.10\n85.40\nBlue\nA\n\n\n493\n169.37\n65.70\nBrown\nB\n\n\n494\n165.84\n75.62\nGreen\nA\n\n\n495\n174.03\n91.78\nBrown\nA\n\n\n496\n161.91\n76.69\nGreen\nB\n\n\n497\n165.82\n84.49\nBrown\nA\n\n\n498\n176.45\n77.73\nGreen\nA\n\n\n499\n163.76\n84.89\nGreen\nB\n\n\n500\n175.34\n74.79\nGrey\nA\n\n\n501\n176.19\n79.02\nGreen\nA\n\n\n502\n177.80\n68.79\nBrown\nA\n\n\n503\n166.40\n74.26\nBrown\nB\n\n\n504\n171.40\n64.85\nBrown\nA\n\n\n505\n172.94\n89.82\nBrown\nB\n\n\n506\n158.85\n95.25\nBlue\nB\n\n\n507\n178.19\n95.69\nBlue\nA\n\n\n508\n174.64\n71.65\nBrown\nB\n\n\n509\n173.31\n61.63\nGreen\nB\n\n\n510\n168.41\n89.21\nGrey\nB\n\n\n511\n167.37\n83.13\nBrown\nB\n\n\n512\n163.14\n77.23\nBrown\nB\n\n\n513\n154.94\n78.91\nBrown\nB\n\n\n514\n177.54\n84.86\nGrey\nB\n\n\n515\n168.35\n80.72\nGrey\nA\n\n\n516\n165.30\n71.59\nGreen\nB\n\n\n517\n171.38\n90.15\nGrey\nA\n\n\n518\n178.61\n78.10\nBrown\nA\n\n\n519\n169.61\n86.23\nGreen\nA\n\n\n520\n173.33\n71.01\nGrey\nA\n\n\n521\n167.47\n72.63\nGrey\nB\n\n\n522\n163.41\n98.54\nBlue\nB\n\n\n523\n161.36\n86.59\nGreen\nB\n\n\n524\n168.90\n88.02\nBrown\nA\n\n\n525\n170.51\n79.95\nGrey\nA\n\n\n526\n170.22\n80.82\nBrown\nA\n\n\n527\n176.16\n76.19\nBlue\nA\n\n\n528\n185.36\n82.77\nGrey\nB\n\n\n529\n180.28\n92.57\nGrey\nB\n\n\n530\n166.17\n91.87\nBrown\nA\n\n\n531\n170.42\n74.92\nBlue\nA\n\n\n532\n166.80\n99.50\nGreen\nB\n\n\n533\n162.70\n91.72\nBrown\nA\n\n\n534\n179.32\n87.12\nGreen\nA\n\n\n535\n171.36\n74.33\nGreen\nB\n\n\n536\n164.12\n75.92\nBrown\nB\n\n\n537\n181.18\n82.27\nBlue\nB\n\n\n538\n170.65\n75.14\nBlue\nB\n\n\n539\n170.26\n97.25\nBrown\nB\n\n\n540\n165.03\n93.62\nGreen\nA\n\n\n541\n168.03\n87.08\nBlue\nA\n\n\n542\n172.68\n74.72\nGrey\nB\n\n\n543\n171.32\n88.08\nBrown\nB\n\n\n544\n167.97\n76.15\nBrown\nB\n\n\n545\n175.27\n76.56\nGreen\nB\n\n\n546\n174.32\n87.44\nBrown\nB\n\n\n547\n173.53\n90.18\nBlue\nB\n\n\n548\n170.63\n80.79\nGrey\nA\n\n\n549\n170.35\n83.07\nGrey\nB\n\n\n550\n179.55\n81.76\nGrey\nB\n\n\n551\n170.29\n84.38\nGrey\nA\n\n\n552\n173.69\n81.86\nBrown\nA\n\n\n553\n165.75\n83.78\nGrey\nB\n\n\n554\n174.85\n92.61\nBlue\nA\n\n\n555\n165.42\n58.67\nBrown\nB\n\n\n556\n168.46\n77.87\nGrey\nB\n\n\n557\n180.39\n87.33\nGrey\nB\n\n\n558\n173.58\n72.97\nGrey\nB\n\n\n559\n175.95\n86.27\nBrown\nB\n\n\n560\n165.24\n94.69\nBlue\nA\n\n\n561\n162.92\n80.14\nBlue\nA\n\n\n562\n168.76\n78.44\nGrey\nB\n\n\n563\n154.97\n69.71\nGrey\nB\n\n\n564\n166.60\n73.44\nBlue\nA\n\n\n565\n178.39\n82.86\nBlue\nB\n\n\n566\n170.43\n67.07\nBrown\nA\n\n\n567\n181.70\n80.12\nGreen\nA\n\n\n568\n167.61\n60.33\nBlue\nA\n\n\n569\n172.11\n75.91\nGreen\nB\n\n\n570\n164.07\n84.60\nBlue\nB\n\n\n571\n171.30\n94.47\nGrey\nB\n\n\n572\n165.82\n86.66\nGreen\nA\n\n\n573\n177.87\n103.07\nGrey\nA\n\n\n574\n167.39\n93.45\nGrey\nB\n\n\n575\n170.40\n65.86\nGrey\nA\n\n\n576\n172.24\n71.97\nGrey\nA\n\n\n577\n177.66\n66.35\nBlue\nA\n\n\n578\n158.44\n71.75\nBrown\nB\n\n\n579\n167.43\n69.87\nGreen\nA\n\n\n580\n177.33\n83.16\nBrown\nA\n\n\n581\n167.91\n89.62\nBrown\nB\n\n\n582\n168.51\n84.52\nGrey\nB\n\n\n583\n166.01\n81.04\nBlue\nB\n\n\n584\n167.88\n84.78\nBrown\nA\n\n\n585\n169.27\n94.25\nBlue\nA\n\n\n586\n162.54\n76.27\nGreen\nA\n\n\n587\n163.42\n81.45\nGreen\nB\n\n\n588\n169.89\n92.00\nBlue\nA\n\n\n589\n182.90\n79.78\nGrey\nA\n\n\n590\n170.13\n77.98\nBlue\nA\n\n\n591\n160.36\n46.75\nGreen\nB\n\n\n592\n163.62\n84.18\nGrey\nA\n\n\n593\n176.19\n74.64\nBrown\nB\n\n\n594\n182.83\n98.21\nBrown\nB\n\n\n595\n155.29\n78.63\nGreen\nB\n\n\n596\n181.12\n75.33\nBlue\nA\n\n\n597\n170.06\n94.14\nGreen\nA\n\n\n598\n160.65\n93.49\nBlue\nA\n\n\n599\n173.07\n71.78\nBlue\nB\n\n\n600\n182.79\n94.18\nGreen\nA\n\n\n601\n177.19\n79.02\nGreen\nA\n\n\n602\n166.34\n81.89\nBlue\nB\n\n\n603\n160.35\n85.94\nGreen\nA\n\n\n604\n169.03\n76.71\nBrown\nA\n\n\n605\n175.38\n61.18\nBrown\nB\n\n\n606\n175.66\n67.94\nGrey\nA\n\n\n607\n167.22\n70.38\nGreen\nB\n\n\n608\n172.13\n74.97\nGreen\nB\n\n\n609\n168.19\n64.19\nGrey\nB\n\n\n610\n175.92\n58.16\nGrey\nA\n\n\n611\n156.76\n76.10\nBrown\nA\n\n\n612\n171.81\n80.61\nGreen\nA\n\n\n613\n176.90\n89.31\nBrown\nB\n\n\n614\n180.21\n91.56\nBlue\nA\n\n\n615\n166.66\n75.97\nGrey\nA\n\n\n616\n169.62\n55.54\nGrey\nB\n\n\n617\n169.89\n68.11\nBrown\nB\n\n\n618\n177.50\n71.99\nBlue\nA\n\n\n619\n177.44\n90.41\nBrown\nA\n\n\n620\n157.05\n67.50\nGrey\nA\n\n\n621\n161.00\n77.55\nBrown\nB\n\n\n622\n171.80\n79.21\nGreen\nB\n\n\n623\n170.97\n84.51\nGrey\nA\n\n\n624\n178.70\n77.99\nGreen\nB\n\n\n625\n178.94\n95.44\nBrown\nA\n\n\n626\n167.65\n72.41\nBrown\nA\n\n\n627\n169.16\n76.84\nBrown\nB\n\n\n628\n163.32\n72.46\nBlue\nA\n\n\n629\n167.14\n77.53\nBrown\nA\n\n\n630\n164.91\n79.02\nBrown\nA\n\n\n631\n167.98\n82.24\nGreen\nB\n\n\n632\n181.76\n95.77\nGreen\nA\n\n\n633\n164.63\n85.10\nBrown\nA\n\n\n634\n172.93\n77.99\nGreen\nB\n\n\n635\n165.38\n94.88\nBrown\nA\n\n\n636\n170.85\n82.02\nBrown\nB\n\n\n637\n172.30\n91.33\nGrey\nA\n\n\n638\n178.09\n75.24\nGreen\nA\n\n\n639\n168.69\n67.43\nGrey\nA\n\n\n640\n176.22\n79.82\nGrey\nA\n\n\n641\n181.02\n79.45\nGrey\nB\n\n\n642\n167.53\n56.23\nGreen\nB\n\n\n643\n175.85\n83.48\nGreen\nA\n\n\n644\n182.90\n73.67\nGreen\nB\n\n\n645\n173.30\n86.46\nGrey\nA\n\n\n646\n174.78\n84.77\nBlue\nB\n\n\n647\n174.23\n82.70\nBrown\nA\n\n\n648\n175.45\n89.24\nBrown\nA\n\n\n649\n169.58\n77.48\nGrey\nA\n\n\n650\n171.74\n94.38\nGreen\nB\n\n\n651\n173.35\n74.55\nBrown\nA\n\n\n652\n165.43\n74.13\nBlue\nB\n\n\n653\n170.78\n85.91\nBlue\nA\n\n\n654\n175.53\n96.12\nGreen\nB\n\n\n655\n181.40\n85.99\nBlue\nB\n\n\n656\n169.12\n76.91\nGrey\nA\n\n\n657\n174.09\n78.85\nBrown\nB\n\n\n658\n169.49\n85.84\nGrey\nA\n\n\n659\n176.24\n76.18\nGreen\nA\n\n\n660\n172.30\n82.70\nBrown\nB\n\n\n661\n167.43\n83.55\nGreen\nA\n\n\n662\n171.17\n91.64\nGreen\nA\n\n\n663\n168.20\n77.96\nBlue\nA\n\n\n664\n166.97\n94.01\nGreen\nA\n\n\n665\n167.31\n60.13\nGrey\nB\n\n\n666\n167.29\n75.21\nBrown\nB\n\n\n667\n178.23\n81.45\nBlue\nA\n\n\n668\n166.20\n67.92\nGrey\nA\n\n\n669\n169.02\n87.46\nBlue\nB\n\n\n670\n167.85\n72.56\nBlue\nB\n\n\n671\n158.30\n78.00\nGreen\nB\n\n\n672\n180.64\n73.73\nBlue\nA\n\n\n673\n174.76\n78.45\nBlue\nA\n\n\n674\n160.15\n67.49\nBlue\nA\n\n\n675\n173.60\n88.69\nBrown\nA\n\n\n676\n183.11\n87.03\nGrey\nB\n\n\n677\n162.92\n87.31\nBlue\nB\n\n\n678\n167.58\n83.90\nGrey\nA\n\n\n679\n170.65\n99.31\nGrey\nA\n\n\n680\n170.93\n87.30\nBrown\nA\n\n\n681\n174.07\n85.59\nBrown\nB\n\n\n682\n158.26\n62.89\nBlue\nA\n\n\n683\n171.36\n92.67\nBrown\nA\n\n\n684\n167.58\n90.09\nBlue\nB\n\n\n685\n160.64\n74.61\nBlue\nA\n\n\n686\n167.97\n86.69\nGrey\nA\n\n\n687\n171.31\n60.75\nBrown\nB\n\n\n688\n170.41\n85.50\nGreen\nB\n\n\n689\n178.91\n82.58\nGrey\nB\n\n\n690\n170.73\n79.52\nBlue\nA\n\n\n691\n182.26\n70.28\nGreen\nA\n\n\n692\n169.20\n83.84\nGrey\nB\n\n\n693\n175.44\n84.43\nBlue\nB\n\n\n694\n171.66\n81.24\nBlue\nA\n\n\n695\n168.24\n81.01\nGreen\nB\n\n\n696\n170.88\n81.51\nBlue\nA\n\n\n697\n179.31\n68.41\nBrown\nA\n\n\n698\n173.38\n58.64\nBlue\nA\n\n\n699\n171.78\n81.16\nGreen\nB\n\n\n700\n176.31\n66.76\nBlue\nB\n\n\n701\n173.27\n81.98\nBlue\nA\n\n\n702\n180.20\n80.06\nBrown\nA\n\n\n703\n163.07\n76.68\nBlue\nB\n\n\n704\n178.05\n85.70\nBlue\nA\n\n\n705\n169.52\n97.65\nGrey\nA\n\n\n706\n174.09\n80.44\nBlue\nA\n\n\n707\n162.67\n83.88\nGreen\nB\n\n\n708\n162.50\n97.09\nBrown\nB\n\n\n709\n174.33\n70.61\nBrown\nA\n\n\n710\n164.46\n75.45\nBrown\nA\n\n\n711\n160.98\n86.42\nGrey\nA\n\n\n712\n164.09\n75.25\nBrown\nB\n\n\n713\n165.32\n99.93\nBlue\nB\n\n\n714\n165.29\n92.95\nBrown\nA\n\n\n715\n171.09\n70.43\nGrey\nB\n\n\n716\n176.25\n73.65\nBlue\nB\n\n\n717\n170.16\n88.82\nBrown\nB\n\n\n718\n167.17\n94.42\nGreen\nB\n\n\n719\n176.76\n87.31\nBlue\nA\n\n\n720\n167.00\n86.57\nBlue\nA\n\n\n721\n163.93\n94.25\nGreen\nB\n\n\n722\n177.56\n77.72\nGrey\nA\n\n\n723\n157.02\n85.21\nGreen\nA\n\n\n724\n171.27\n99.56\nBlue\nA\n\n\n725\n170.19\n73.13\nBrown\nB\n\n\n726\n168.61\n85.38\nBlue\nA\n\n\n727\n174.91\n92.50\nBrown\nA\n\n\n728\n174.96\n78.84\nBrown\nB\n\n\n729\n167.10\n68.13\nGrey\nB\n\n\n730\n159.56\n70.02\nBlue\nB\n\n\n731\n171.81\n86.07\nGreen\nB\n\n\n732\n172.12\n77.30\nGrey\nB\n\n\n733\n169.40\n98.71\nGreen\nA\n\n\n734\n172.63\n90.88\nBlue\nB\n\n\n735\n172.14\n81.16\nBlue\nB\n\n\n736\n171.49\n93.67\nGreen\nB\n\n\n737\n164.27\n75.76\nGrey\nB\n\n\n738\n174.75\n71.92\nBlue\nB\n\n\n739\n178.49\n65.73\nBrown\nB\n\n\n740\n162.42\n73.07\nBlue\nB\n\n\n741\n173.25\n65.75\nGreen\nA\n\n\n742\n161.81\n74.14\nGrey\nA\n\n\n743\n167.29\n76.99\nGreen\nA\n\n\n744\n167.55\n70.70\nBlue\nB\n\n\n745\n166.57\n90.76\nBrown\nA\n\n\n746\n165.45\n74.49\nGreen\nB\n\n\n747\n166.44\n61.70\nBrown\nA\n\n\n748\n171.62\n77.31\nBlue\nB\n\n\n749\n182.53\n96.66\nBlue\nB\n\n\n750\n175.17\n98.71\nGrey\nA\n\n\n751\n170.34\n64.02\nGrey\nB\n\n\n752\n182.65\n75.77\nBrown\nB\n\n\n753\n164.66\n83.33\nBlue\nA\n\n\n754\n173.94\n90.77\nGreen\nB\n\n\n755\n174.00\n70.61\nGreen\nB\n\n\n756\n167.35\n73.08\nGreen\nB\n\n\n757\n164.04\n70.76\nGrey\nB\n\n\n758\n167.68\n86.44\nBlue\nA\n\n\n759\n172.91\n59.66\nBrown\nA\n\n\n760\n172.79\n87.22\nGreen\nB\n\n\n761\n165.05\n87.25\nGrey\nB\n\n\n762\n170.16\n72.49\nBrown\nB\n\n\n763\n174.74\n85.97\nBrown\nA\n\n\n764\n166.85\n75.14\nBrown\nA\n\n\n765\n171.46\n84.01\nGrey\nB\n\n\n766\n171.50\n82.97\nGrey\nA\n\n\n767\n166.24\n55.69\nGreen\nB\n\n\n768\n174.29\n74.66\nBlue\nB\n\n\n769\n168.51\n93.75\nBrown\nA\n\n\n770\n176.74\n70.11\nGreen\nB\n\n\n771\n169.57\n82.94\nGrey\nA\n\n\n772\n176.23\n81.36\nBrown\nB\n\n\n773\n172.81\n84.42\nGrey\nB\n\n\n774\n171.68\n78.66\nGreen\nB\n\n\n775\n164.13\n62.75\nGreen\nB\n\n\n776\n170.14\n86.67\nBrown\nB\n\n\n777\n160.52\n87.41\nBlue\nA\n\n\n778\n164.57\n58.84\nBrown\nB\n\n\n779\n150.87\n81.67\nBlue\nB\n\n\n780\n159.05\n86.09\nBlue\nB\n\n\n781\n167.98\n85.85\nGreen\nB\n\n\n782\n164.63\n65.40\nBrown\nB\n\n\n783\n158.62\n74.42\nBrown\nB\n\n\n784\n172.93\n88.11\nBrown\nA\n\n\n785\n174.53\n92.38\nGreen\nB\n\n\n786\n172.78\n76.66\nBlue\nA\n\n\n787\n169.17\n88.52\nBrown\nB\n\n\n788\n171.81\n94.81\nGrey\nB\n\n\n789\n167.42\n87.61\nGrey\nB\n\n\n790\n175.75\n76.07\nGrey\nB\n\n\n791\n162.83\n78.39\nBrown\nA\n\n\n792\n170.01\n79.74\nBrown\nA\n\n\n793\n175.42\n110.79\nBlue\nB\n\n\n794\n174.97\n88.70\nGrey\nB\n\n\n795\n169.20\n84.18\nGreen\nA\n\n\n796\n175.68\n79.86\nGreen\nA\n\n\n797\n172.42\n67.29\nBlue\nB\n\n\n798\n170.98\n73.88\nBlue\nB\n\n\n799\n165.07\n80.21\nBrown\nA\n\n\n800\n177.50\n63.72\nGreen\nA\n\n\n801\n166.25\n70.62\nBrown\nA\n\n\n802\n166.30\n77.81\nGrey\nA\n\n\n803\n181.52\n85.54\nBrown\nB\n\n\n804\n175.26\n60.75\nGrey\nA\n\n\n805\n173.38\n70.63\nBrown\nB\n\n\n806\n164.51\n78.71\nBlue\nA\n\n\n807\n171.78\n84.71\nBlue\nB\n\n\n808\n165.66\n68.92\nGreen\nA\n\n\n809\n167.41\n69.24\nBlue\nA\n\n\n810\n168.40\n68.86\nGrey\nB\n\n\n811\n172.10\n74.32\nGrey\nB\n\n\n812\n174.84\n69.74\nGrey\nB\n\n\n813\n168.22\n76.41\nGrey\nB\n\n\n814\n175.35\n80.89\nGrey\nB\n\n\n815\n168.27\n86.86\nGrey\nB\n\n\n816\n161.11\n60.64\nGreen\nB\n\n\n817\n171.52\n96.93\nGreen\nA\n\n\n818\n168.70\n71.37\nBrown\nA\n\n\n819\n172.92\n71.97\nGrey\nB\n\n\n820\n157.79\n78.13\nBrown\nB\n\n\n821\n160.46\n84.63\nGreen\nB\n\n\n822\n167.48\n73.23\nBlue\nA\n\n\n823\n164.42\n87.27\nBrown\nA\n\n\n824\n167.16\n85.06\nGreen\nB\n\n\n825\n164.22\n76.83\nBlue\nA\n\n\n826\n167.38\n85.58\nGrey\nA\n\n\n827\n167.92\n88.72\nBlue\nB\n\n\n828\n169.62\n70.35\nBlue\nA\n\n\n829\n169.46\n76.79\nGreen\nA\n\n\n830\n169.55\n69.88\nGreen\nA\n\n\n831\n180.36\n80.48\nBrown\nB\n\n\n832\n169.79\n96.97\nGrey\nA\n\n\n833\n168.62\n90.98\nGrey\nB\n\n\n834\n165.75\n78.29\nBlue\nB\n\n\n835\n178.58\n97.01\nBrown\nA\n\n\n836\n170.68\n88.18\nBrown\nB\n\n\n837\n170.55\n90.80\nGrey\nA\n\n\n838\n174.44\n106.58\nBlue\nA\n\n\n839\n159.85\n71.98\nGrey\nB\n\n\n840\n164.98\n80.27\nGreen\nA\n\n\n841\n168.35\n70.84\nGrey\nA\n\n\n842\n175.64\n73.48\nBrown\nA\n\n\n843\n164.38\n77.01\nBlue\nA\n\n\n844\n171.51\n82.79\nBrown\nA\n\n\n845\n164.49\n94.40\nGrey\nA\n\n\n846\n165.46\n91.62\nBrown\nA\n\n\n847\n167.85\n71.71\nGreen\nB\n\n\n848\n158.98\n105.06\nGrey\nA\n\n\n849\n165.87\n86.96\nGrey\nA\n\n\n850\n176.04\n71.93\nGrey\nB\n\n\n851\n177.04\n102.43\nGreen\nB\n\n\n852\n177.15\n71.39\nBlue\nB\n\n\n853\n168.60\n85.07\nBrown\nB\n\n\n854\n169.89\n80.78\nGreen\nA\n\n\n855\n177.50\n94.62\nGrey\nA\n\n\n856\n162.37\n87.83\nBrown\nB\n\n\n857\n174.27\n77.46\nBrown\nB\n\n\n858\n169.69\n68.66\nBrown\nA\n\n\n859\n168.12\n67.13\nGrey\nA\n\n\n860\n176.64\n89.09\nGreen\nA\n\n\n861\n170.16\n80.37\nBlue\nB\n\n\n862\n176.18\n77.91\nBlue\nB\n\n\n863\n181.13\n94.62\nGreen\nA\n\n\n864\n169.48\n55.62\nBlue\nB\n\n\n865\n160.23\n76.41\nGreen\nB\n\n\n866\n173.59\n64.94\nGreen\nA\n\n\n867\n177.58\n59.83\nGreen\nB\n\n\n868\n158.73\n77.83\nBlue\nB\n\n\n869\n166.51\n59.24\nGrey\nB\n\n\n870\n171.16\n76.54\nGreen\nA\n\n\n871\n166.75\n87.15\nBlue\nB\n\n\n872\n167.07\n74.78\nBlue\nB\n\n\n873\n170.56\n65.90\nBlue\nB\n\n\n874\n170.69\n92.34\nBrown\nA\n\n\n875\n178.77\n72.60\nGrey\nA\n\n\n876\n184.79\n80.71\nBrown\nB\n\n\n877\n169.02\n81.13\nBrown\nA\n\n\n878\n177.64\n72.06\nBlue\nB\n\n\n879\n178.17\n79.74\nBrown\nA\n\n\n880\n168.18\n86.66\nGreen\nA\n\n\n881\n173.20\n73.21\nGreen\nB\n\n\n882\n160.10\n74.50\nGrey\nB\n\n\n883\n166.98\n75.43\nBlue\nB\n\n\n884\n162.33\n69.60\nBrown\nB\n\n\n885\n155.45\n75.84\nBrown\nA\n\n\n886\n165.47\n90.28\nBrown\nA\n\n\n887\n156.23\n84.61\nBlue\nA\n\n\n888\n167.94\n72.79\nGreen\nA\n\n\n889\n169.33\n80.79\nBlue\nB\n\n\n890\n170.70\n93.83\nBrown\nA\n\n\n891\n179.78\n91.38\nGreen\nB\n\n\n892\n174.66\n85.08\nGrey\nA\n\n\n893\n166.21\n72.35\nGreen\nB\n\n\n894\n173.43\n86.37\nBrown\nB\n\n\n895\n176.87\n73.21\nBrown\nA\n\n\n896\n171.07\n80.50\nGreen\nB\n\n\n897\n171.75\n83.00\nBrown\nB\n\n\n898\n167.14\n82.98\nBrown\nA\n\n\n899\n164.90\n78.77\nGrey\nB\n\n\n900\n173.61\n84.14\nGrey\nA\n\n\n901\n169.68\n80.68\nBrown\nA\n\n\n902\n160.71\n69.94\nGrey\nA\n\n\n903\n168.64\n78.60\nBlue\nB\n\n\n904\n167.91\n73.80\nGrey\nA\n\n\n905\n174.40\n66.62\nBrown\nB\n\n\n906\n163.77\n81.82\nBrown\nA\n\n\n907\n175.53\n62.93\nBrown\nA\n\n\n908\n171.69\n93.54\nGreen\nB\n\n\n909\n164.33\n69.30\nBlue\nB\n\n\n910\n164.88\n75.57\nBrown\nA\n\n\n911\n175.25\n93.02\nGrey\nB\n\n\n912\n174.47\n62.93\nBrown\nB\n\n\n913\n176.47\n72.97\nGreen\nB\n\n\n914\n173.87\n77.61\nBlue\nB\n\n\n915\n175.21\n76.88\nBrown\nB\n\n\n916\n162.28\n70.08\nBlue\nB\n\n\n917\n169.21\n95.13\nGreen\nA\n\n\n918\n166.93\n88.95\nBlue\nB\n\n\n919\n171.42\n75.58\nGreen\nA\n\n\n920\n168.69\n84.96\nGrey\nA\n\n\n921\n179.67\n66.91\nGreen\nB\n\n\n922\n173.80\n89.03\nGreen\nB\n\n\n923\n177.50\n77.55\nGreen\nB\n\n\n924\n163.92\n64.20\nBlue\nA\n\n\n925\n164.48\n84.95\nGreen\nA\n\n\n926\n167.46\n86.53\nGrey\nB\n\n\n927\n168.09\n94.25\nBlue\nA\n\n\n928\n179.06\n77.81\nBlue\nA\n\n\n929\n173.58\n94.34\nBrown\nB\n\n\n930\n162.63\n59.21\nGrey\nB\n\n\n931\n168.75\n78.31\nGreen\nB\n\n\n932\n165.95\n72.21\nGreen\nB\n\n\n933\n166.54\n93.45\nGreen\nB\n\n\n934\n172.17\n86.66\nBlue\nA\n\n\n935\n161.55\n84.90\nBlue\nA\n\n\n936\n175.02\n74.64\nBrown\nB\n\n\n937\n173.98\n75.45\nGreen\nB\n\n\n938\n170.29\n77.07\nGreen\nB\n\n\n939\n172.37\n60.96\nGreen\nB\n\n\n940\n174.49\n57.51\nBlue\nB\n\n\n941\n178.04\n76.22\nBrown\nB\n\n\n942\n178.74\n67.61\nBlue\nB\n\n\n943\n166.51\n59.70\nGrey\nB\n\n\n944\n174.46\n89.68\nBrown\nA\n\n\n945\n174.72\n87.87\nBlue\nB\n\n\n946\n169.27\n97.67\nGreen\nA\n\n\n947\n174.95\n72.39\nGreen\nB\n\n\n948\n174.66\n76.67\nGreen\nA\n\n\n949\n172.80\n84.31\nGreen\nB\n\n\n950\n170.07\n88.15\nGrey\nA\n\n\n951\n165.03\n85.43\nBlue\nA\n\n\n952\n167.46\n85.82\nGrey\nB\n\n\n953\n168.82\n84.41\nBrown\nB\n\n\n954\n173.16\n85.40\nGrey\nB\n\n\n955\n170.28\n95.64\nBrown\nA\n\n\n956\n180.22\n62.63\nBrown\nB\n\n\n957\n179.28\n88.18\nGrey\nB\n\n\n958\n174.96\n84.55\nGrey\nB\n\n\n959\n170.05\n79.81\nBlue\nA\n\n\n960\n171.82\n72.94\nGreen\nA\n\n\n961\n174.35\n87.19\nGreen\nA\n\n\n962\n162.12\n64.63\nGreen\nA\n\n\n963\n172.52\n72.67\nGrey\nB\n\n\n964\n169.63\n81.66\nGrey\nA\n\n\n965\n180.01\n71.99\nGrey\nB\n\n\n966\n171.54\n79.13\nBrown\nA\n\n\n967\n164.75\n88.90\nGreen\nB\n\n\n968\n174.17\n87.39\nGrey\nB\n\n\n969\n180.01\n69.61\nGrey\nB\n\n\n970\n178.25\n68.80\nBrown\nB\n\n\n971\n170.21\n71.18\nGrey\nA\n\n\n972\n177.13\n73.50\nBlue\nB\n\n\n973\n176.37\n87.15\nGrey\nB\n\n\n974\n172.05\n98.06\nBlue\nB\n\n\n975\n165.59\n78.70\nBlue\nB\n\n\n976\n172.64\n77.49\nGreen\nA\n\n\n977\n175.23\n86.77\nGreen\nB\n\n\n978\n167.40\n72.21\nGrey\nB\n\n\n979\n183.37\n81.27\nBlue\nB\n\n\n980\n162.39\n77.16\nBlue\nB\n\n\n981\n169.06\n81.30\nGreen\nB\n\n\n982\n175.67\n81.96\nBlue\nA\n\n\n983\n163.60\n70.58\nGrey\nB\n\n\n984\n171.01\n64.23\nBlue\nA\n\n\n985\n177.98\n69.07\nGrey\nA\n\n\n986\n172.03\n85.05\nGrey\nA\n\n\n987\n166.31\n77.44\nGreen\nA\n\n\n988\n182.59\n78.23\nGrey\nA\n\n\n989\n160.62\n93.77\nBrown\nA\n\n\n990\n174.78\n64.95\nBrown\nA\n\n\n991\n173.50\n80.96\nGrey\nB\n\n\n992\n176.38\n86.78\nBlue\nA\n\n\n993\n167.54\n86.58\nBrown\nA\n\n\n994\n159.14\n84.11\nBrown\nB\n\n\n995\n168.76\n76.32\nGreen\nA\n\n\n996\n165.34\n88.06\nBlue\nB\n\n\n997\n172.12\n83.62\nBrown\nB\n\n\n998\n166.88\n69.48\nGrey\nA\n\n\n999\n173.81\n67.12\nGreen\nA\n\n\n1000\n171.24\n78.69\nGrey\nA"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#check-how-similar-they-are",
    "href": "content/slides/01-07-experiments.html#check-how-similar-they-are",
    "title": "Causal Effects and Experiments",
    "section": "Check how similar they are",
    "text": "Check how similar they are"
  },
  {
    "objectID": "content/slides/01-07-experiments.html#increasingly-similar",
    "href": "content/slides/01-07-experiments.html#increasingly-similar",
    "title": "Causal Effects and Experiments",
    "section": "Increasingly similar",
    "text": "Increasingly similar\nIn fact, if we did this many, many, many times, these groups would be, on average, increasingly identical!\nWhy?\n\nCentral limit theorem\nLaw of large numbers\n\n\n\nWe will talk more about those concepts later in the course."
  },
  {
    "objectID": "content/slides/01-07-experiments.html#do-international-monitors-deter-election-day-fraud",
    "href": "content/slides/01-07-experiments.html#do-international-monitors-deter-election-day-fraud",
    "title": "Causal Effects and Experiments",
    "section": "Do international monitors deter election-day fraud?",
    "text": "Do international monitors deter election-day fraud?\nYes!\n\nThe international community monitored the 2003 Armenian Presidential elections\nMonitors were assigned randomly to the polling stations\nHyde found a big average difference between the vote share received by the cheating party at monitored stations compared to non-monitored stations."
  },
  {
    "objectID": "content/slides/01-01-data_viz_intro.html#data-visualization",
    "href": "content/slides/01-01-data_viz_intro.html#data-visualization",
    "title": "Data Visualization",
    "section": "Data visualization",
    "text": "Data visualization\nWe will use data visualization to answer the following question:\n\nDo cars with big engines use more fuel than cars with small engines?"
  },
  {
    "objectID": "content/slides/01-01-data_viz_intro.html#exercise",
    "href": "content/slides/01-01-data_viz_intro.html#exercise",
    "title": "Data Visualization",
    "section": "EXERCISE",
    "text": "EXERCISE\n\nWhat do you think is the answer to this question?\n\n\n\nHow would you prove your answer? What information about cars would you need?"
  },
  {
    "objectID": "content/slides/01-01-data_viz_intro.html#r4ds",
    "href": "content/slides/01-01-data_viz_intro.html#r4ds",
    "title": "Data Visualization",
    "section": "R4DS",
    "text": "R4DS\nThis session will borrow (read: steal) heavily from Hadley Wickham’s R for Data Science book.\n\nSource: R4DS"
  },
  {
    "objectID": "content/slides/01-01-data_viz_intro.html#data-visualization-a-critical-skill",
    "href": "content/slides/01-01-data_viz_intro.html#data-visualization-a-critical-skill",
    "title": "Data Visualization",
    "section": "Data visualization: a critical skill",
    "text": "Data visualization: a critical skill\n\nYou can learn an incredible amount about your data from plotting it\nEasier to digest the vast amount of information required to do data analysis from a plot than raw numbers"
  },
  {
    "objectID": "content/slides/01-01-data_viz_intro.html#skipping-to-the-end",
    "href": "content/slides/01-01-data_viz_intro.html#skipping-to-the-end",
    "title": "Data Visualization",
    "section": "Skipping to the end",
    "text": "Skipping to the end"
  },
  {
    "objectID": "content/slides/01-01-data_viz_intro.html#how-did-we-do-this",
    "href": "content/slides/01-01-data_viz_intro.html#how-did-we-do-this",
    "title": "Data Visualization",
    "section": "How did we do this?",
    "text": "How did we do this?\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(colour = class)) + \n  geom_smooth(method = \"lm\") + \n  theme(\n    legend.position = \"bottom\",\n    panel.grid = element_blank(),\n    panel.background = element_blank(),\n    plot.title.position = \"plot\",\n    plot.title = element_text(face = \"bold\")\n  ) + \n  labs(\n    title = \"Engine displacement and highway miles per gallon\",\n    subtitle = \"Values for seven different classes of cars\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway miles per gallon\"\n  )\n\n\n\n\n\n\n\n\n\nWe will step through this now. By the end of this session, you will be able to make your own colourful data visualisations using R."
  },
  {
    "objectID": "content/slides/01-06-categorical.html#working-with-categorical-data",
    "href": "content/slides/01-06-categorical.html#working-with-categorical-data",
    "title": "Working with Categorical Data",
    "section": "Working with categorical data",
    "text": "Working with categorical data\nWe often want to explore patterns in categorical (or discrete) data. We need new tools to do this."
  },
  {
    "objectID": "content/slides/01-06-categorical.html#working-with-categorical-data-1",
    "href": "content/slides/01-06-categorical.html#working-with-categorical-data-1",
    "title": "Working with Categorical Data",
    "section": "Working with categorical data",
    "text": "Working with categorical data\n\nselect(mpg, manufacturer, model, drv)\n\n# A tibble: 234 × 3\n   manufacturer model      drv  \n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;\n 1 audi         a4         f    \n 2 audi         a4         f    \n 3 audi         a4         f    \n 4 audi         a4         f    \n 5 audi         a4         f    \n 6 audi         a4         f    \n 7 audi         a4         f    \n 8 audi         a4 quattro 4    \n 9 audi         a4 quattro 4    \n10 audi         a4 quattro 4    \n# ℹ 224 more rows"
  },
  {
    "objectID": "content/slides/01-06-categorical.html#visualizing-distributions",
    "href": "content/slides/01-06-categorical.html#visualizing-distributions",
    "title": "Working with Categorical Data",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\n\nggplot(mpg, aes(x = drv)) + \n  geom_bar()"
  },
  {
    "objectID": "content/slides/01-06-categorical.html#visualizing-distributions-1",
    "href": "content/slides/01-06-categorical.html#visualizing-distributions-1",
    "title": "Working with Categorical Data",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nReorder in relation to frequency\n\nggplot(mpg, aes(x = fct_infreq(drv))) +\n  geom_bar()"
  },
  {
    "objectID": "content/slides/01-06-categorical.html#visualizing-numeric-variables",
    "href": "content/slides/01-06-categorical.html#visualizing-numeric-variables",
    "title": "Working with Categorical Data",
    "section": "Visualizing numeric variables",
    "text": "Visualizing numeric variables\n\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram()"
  },
  {
    "objectID": "content/slides/01-06-categorical.html#visualizing-numeric-variables-1",
    "href": "content/slides/01-06-categorical.html#visualizing-numeric-variables-1",
    "title": "Working with Categorical Data",
    "section": "Visualizing numeric variables",
    "text": "Visualizing numeric variables\n\nggplot(mpg, aes(x = hwy)) +\n  geom_density()"
  },
  {
    "objectID": "content/slides/01-06-categorical.html#visualizing-numeric-variables-2",
    "href": "content/slides/01-06-categorical.html#visualizing-numeric-variables-2",
    "title": "Working with Categorical Data",
    "section": "Visualizing numeric variables",
    "text": "Visualizing numeric variables\n\nggplot(mpg, aes(x = hwy, colour = drv, fill = drv)) +\n  geom_density(alpha = 0.5)"
  },
  {
    "objectID": "content/02-transformation.html",
    "href": "content/02-transformation.html",
    "title": "Data Transformation",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(\"gapminder\")\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nThis section focuses on introducing you to some new R skills. Data rarely come to us perfectly formatted and without annoying errors or inconsistencies. We can use R to clean our data up and get them ready for analysis.",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#set-up",
    "href": "content/02-transformation.html#set-up",
    "title": "Data Transformation",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(\"gapminder\")\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nThis section focuses on introducing you to some new R skills. Data rarely come to us perfectly formatted and without annoying errors or inconsistencies. We can use R to clean our data up and get them ready for analysis.",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#gapminder",
    "href": "content/02-transformation.html#gapminder",
    "title": "Data Transformation",
    "section": "Gapminder",
    "text": "Gapminder\nThis session, we will explore the long-studied relationship between countries’ health and wealth. Attention was brought to this relationship by the Gapminder project. You can read more about their research here.\nBefore we get started, check out this video that describes (very enthusiastically) the relationship between health and wealth:",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#introduction",
    "href": "content/02-transformation.html#introduction",
    "title": "Data Transformation",
    "section": "Introduction",
    "text": "Introduction\nToday, I am going to introduce you to some of my most widely used functions in R. The dplyr R package, which houses these functions, focuses on providing you with some very helpful tools to tidy your data. This step in the data analysis process is critical, and often one of the most time consuming, so let’s get started!\nR objects\nAs a reminder, objects sit in R’s memory. You can see which ones exist in your current session and their values over in the environment tab.\nTo create a new object, use: &lt;-. You can update that object at any time.\n\n\n\n\n\n\nNote\n\n\n\nSome people use = instead of &lt;-. I strongly recommend against this. It makes your script difficult to read, and it can lead to syntax errors.\n\n\nR functions\nFunctions are helpful short-hands that perform specific tasks in R. Many of these functions come straight out of the box with R. For example, you can run the following code without loading any packages into your current session:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe seq() function takes three main arguments: a start value, end value, and interval value. Above I have asked it to start at 1 and end at 10. By default, its interval value is 1. Therefore, its output is every whole number from 1 to 10.\n\n\nYou can even create objects with R functions:\n\nx &lt;- seq(1, 10)\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nNow, the x object is a vector of all 10 whole numbers from 1 to 10.\nData\nAs mentioned above, we are going to be using the Gapminder project’s data on countries’ health and wealth to explore data transformation. Happily, there is an R package that provides that data for us. We will now install it:\n\ninstall.packages(\"gapminder\")\n\nNext, you want to create a new script for this session. At the top of that script, include the code to load in the packages we will be using today:\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nYou can now access the Gapminder data directly:\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nAbove, R prints out some useful information about our data. First, we learn that it is a tibble. This is the tidyverse version of a data frame.\nSecond, we learn how many rows and columns are included in our data set. We have 1,704 rows and 6 columns.\nNext, we learn the column (or variable) titles:\n\ncolnames(gapminder)\n\n[1] \"country\"   \"continent\" \"year\"      \"lifeExp\"   \"pop\"       \"gdpPercap\"\n\n\nNext, we see some funny looking three letter words under each column name. These refer to the column’s data type. We will discuss these below.\nFinally, we see the first few rows of our data set. We can see that each row provides a country’s information in a single year.\nData types\nThose funny three-letter words refer to the column’s data type. In this data set, we have three different types:\n\n&lt;fct&gt;, which stands for factor (R’s word for categorical variable)\n&lt;int&gt;, which stands for integer\n&lt;dbl&gt;, which stands for double (or real number)\n\nSome other common data types include:\n\n&lt;chr&gt;, which stands for character (or string)\n&lt;dttm&gt;, which stands for date-time\n&lt;lgl&gt;, which stands for local (which can only be TRUE or FALSE)\n\nData types are important because each column can only contain one data type. You cannot, for example, include a character in an integer column (imagine trying to run some calculation on that column: how would R treat the stray character value?).\n\ndplyr basics\nNow we know a little bit more about our data set, we are going to use it to explore five of the most commonly used functions in R:\n\nfilter()\narrange()\nselect()\nmutate()\nsummarise()\nExercises\nWhat is the unit of observation for the gapminder data set?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\nThe unit of observation is country-year.\n\n\n\nHow many country-years are included in the gapminder data set?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nnrow(gapminder)\n\n[1] 1704\n\n\n\n\n\nWhich two variables in the gapminder data set are factors?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\ncountry and continent are factors (or categorical variables).",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#filtering-your-data",
    "href": "content/02-transformation.html#filtering-your-data",
    "title": "Data Transformation",
    "section": "Filtering your data",
    "text": "Filtering your data\nWe are going to start by learning how to filter our data to only include those observations we are interested in exploring.\nAll dplyr functions are structured the same way. They take the data object you want to transform as their first argument. You then need to identify the columns within that data object you want to transform and what you want to do with them.\nFor example, the following code takes our data set, gapminder, and tells filter() that we want to only include rows in which the country column matches “Australia” and the year column is greater than 2000:\n\nfilter(gapminder, country == \"Australia\", year &gt; 2000)\n\n# A tibble: 2 × 6\n  country   continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Australia Oceania    2002    80.4 19546792    30688.\n2 Australia Oceania    2007    81.2 20434176    34435.\n\n\nWe can ask filter() to include rows that match multiple values:\n\nfilter(gapminder, continent %in% c(\"Asia\", \"Oceania\"))\n\n# A tibble: 420 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 410 more rows\n\n\nHere, I have used the %in% value and a vector (which is c(...)) of strings to ask filter to include all rows in which the continent column matches the values “Asia” and “Oceania”.\nWe can ask filter() to include values within a range. The following code asks filter() to include all rows in which the pop column is greater than 500,000 and less than 1,000,000:\n\nfilter(gapminder, pop &gt; 500000 & pop &lt; 1000000)\n\n# A tibble: 88 × 6\n   country  continent  year lifeExp    pop gdpPercap\n   &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt;\n 1 Bahrain  Asia       1992    72.6 529491    19036.\n 2 Bahrain  Asia       1997    73.9 598561    20292.\n 3 Bahrain  Asia       2002    74.8 656397    23404.\n 4 Bahrain  Asia       2007    75.6 708573    29796.\n 5 Botswana Africa     1962    51.5 512764      984.\n 6 Botswana Africa     1967    53.3 553541     1215.\n 7 Botswana Africa     1972    56.0 619351     2264.\n 8 Botswana Africa     1977    59.3 781472     3215.\n 9 Botswana Africa     1982    61.5 970347     4551.\n10 Comoros  Africa     1997    60.7 527982     1174.\n# ℹ 78 more rows\n\n\nThe following code asks filter() to include all rows in which the pop column is greater than 500,000 or less than 1,000,000.\n\nfilter(gapminder, pop &gt; 500000 | pop &lt; 1000000)\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nR includes some handy operators:\n\n== is equal to\n!= is not equal to\n&gt;= is greater than or equal to\n&lt;= is less than or equal to\n| is OR\n& is AND\n%in% is in\n\nExercises\nFind all country-years that have populations greater than 1 billion people.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nfilter(gapminder, pop &gt; 1e9)\n\n# A tibble: 8 × 6\n  country continent  year lifeExp        pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;      &lt;int&gt;     &lt;dbl&gt;\n1 China   Asia       1982    65.5 1000281000      962.\n2 China   Asia       1987    67.3 1084035000     1379.\n3 China   Asia       1992    68.7 1164970000     1656.\n4 China   Asia       1997    70.4 1230075000     2289.\n5 China   Asia       2002    72.0 1280400000     3119.\n6 China   Asia       2007    73.0 1318683096     4959.\n7 India   Asia       2002    62.9 1034172547     1747.\n8 India   Asia       2007    64.7 1110396331     2452.\n\n\nNote: 1e9 is scientific notation for 1 billion (which has nine zeros).\n\n\n\nFind all countries in Oceania.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nfilter(gapminder, continent == \"Oceania\")\n\n# A tibble: 24 × 6\n   country   continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Australia Oceania    1952    69.1  8691212    10040.\n 2 Australia Oceania    1957    70.3  9712569    10950.\n 3 Australia Oceania    1962    70.9 10794968    12217.\n 4 Australia Oceania    1967    71.1 11872264    14526.\n 5 Australia Oceania    1972    71.9 13177000    16789.\n 6 Australia Oceania    1977    73.5 14074100    18334.\n 7 Australia Oceania    1982    74.7 15184200    19477.\n 8 Australia Oceania    1987    76.3 16257249    21889.\n 9 Australia Oceania    1992    77.6 17481977    23425.\n10 Australia Oceania    1997    78.8 18565243    26998.\n# ℹ 14 more rows\n\n\n\n\n\nFind all countries in both Asia and Europe.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nfilter(gapminder, continent %in% c(\"Asia\", \"Europe\"))\n\n# A tibble: 756 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 746 more rows\n\n\n\n\n\nFind all country-years that have a life expectancy greater than 50 years and less than 60 years.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nfilter(gapminder, lifeExp &gt; 50 & lifeExp &lt; 60)\n\n# A tibble: 336 × 6\n   country    continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;      &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Albania    Europe     1952    55.2  1282697     1601.\n 2 Albania    Europe     1957    59.3  1476505     1942.\n 3 Algeria    Africa     1967    51.4 12760499     3247.\n 4 Algeria    Africa     1972    54.5 14760787     4183.\n 5 Algeria    Africa     1977    58.0 17152804     4910.\n 6 Bahrain    Asia       1952    50.9   120447     9867.\n 7 Bahrain    Asia       1957    53.8   138655    11636.\n 8 Bahrain    Asia       1962    56.9   171863    12753.\n 9 Bahrain    Asia       1967    59.9   202182    14805.\n10 Bangladesh Asia       1982    50.0 93074406      677.\n# ℹ 326 more rows\n\n\n\n\n\nFind all country-years that have a life expectancy less than 50 years or greater than 60 years.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nfilter(gapminder, lifeExp &lt; 50 | lifeExp &gt; 60)\n\n# A tibble: 1,368 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,358 more rows",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#sorting-your-data",
    "href": "content/02-transformation.html#sorting-your-data",
    "title": "Data Transformation",
    "section": "Sorting your data",
    "text": "Sorting your data\nWe can control the order of our data using arrange(). The following code sorts our data by country name (alphabetically) and year (in ascending numerical order):\n\narrange(gapminder, country, year)\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nTo sort in descending order, simply wrap your column name in the desc() function:\n\narrange(gapminder, country, desc(year))\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2007    43.8 31889923      975.\n 2 Afghanistan Asia       2002    42.1 25268405      727.\n 3 Afghanistan Asia       1997    41.8 22227415      635.\n 4 Afghanistan Asia       1992    41.7 16317921      649.\n 5 Afghanistan Asia       1987    40.8 13867957      852.\n 6 Afghanistan Asia       1982    39.9 12881816      978.\n 7 Afghanistan Asia       1977    38.4 14880372      786.\n 8 Afghanistan Asia       1972    36.1 13079460      740.\n 9 Afghanistan Asia       1967    34.0 11537966      836.\n10 Afghanistan Asia       1962    32.0 10267083      853.\n# ℹ 1,694 more rows\n\n\nYou can combine arrange() and filter() using the slice_X() functions. For example, you can filter for the smallest value in a column using slice_min(). The following code filters for the smallest value in the lifeExp column:\n\nslice_min(gapminder, lifeExp)\n\n# A tibble: 1 × 6\n  country continent  year lifeExp     pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n1 Rwanda  Africa     1992    23.6 7290203      737.\n\n\nThe following code filters for the largest value in this column:\n\nslice_max(gapminder, lifeExp)\n\n# A tibble: 1 × 6\n  country continent  year lifeExp       pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n1 Japan   Asia       2007    82.6 127467972    31656.\n\n\nExercises\nWhich country-year has the lowest life expectancy?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\narrange(gapminder, lifeExp)\n\n# A tibble: 1,704 × 6\n   country      continent  year lifeExp     pop gdpPercap\n   &lt;fct&gt;        &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n 1 Rwanda       Africa     1992    23.6 7290203      737.\n 2 Afghanistan  Asia       1952    28.8 8425333      779.\n 3 Gambia       Africa     1952    30    284320      485.\n 4 Angola       Africa     1952    30.0 4232095     3521.\n 5 Sierra Leone Africa     1952    30.3 2143249      880.\n 6 Afghanistan  Asia       1957    30.3 9240934      821.\n 7 Cambodia     Asia       1977    31.2 6978607      525.\n 8 Mozambique   Africa     1952    31.3 6446316      469.\n 9 Sierra Leone Africa     1957    31.6 2295678     1004.\n10 Burkina Faso Africa     1952    32.0 4469979      543.\n# ℹ 1,694 more rows\n\n\nNote: check out slice_min() and slice_max() for more efficient ways of doing this.\n\n\n\nWhich country-year has the largest population?\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\narrange(gapminder, desc(pop))\n\n# A tibble: 1,704 × 6\n   country continent  year lifeExp        pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;      &lt;int&gt;     &lt;dbl&gt;\n 1 China   Asia       2007    73.0 1318683096     4959.\n 2 China   Asia       2002    72.0 1280400000     3119.\n 3 China   Asia       1997    70.4 1230075000     2289.\n 4 China   Asia       1992    68.7 1164970000     1656.\n 5 India   Asia       2007    64.7 1110396331     2452.\n 6 China   Asia       1987    67.3 1084035000     1379.\n 7 India   Asia       2002    62.9 1034172547     1747.\n 8 China   Asia       1982    65.5 1000281000      962.\n 9 India   Asia       1997    61.8  959000000     1459.\n10 China   Asia       1977    64.0  943455000      741.\n# ℹ 1,694 more rows\n\n\nNote: check out slice_min() and slice_max() for more efficient ways of doing this.",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#selecting-relevant-columns",
    "href": "content/02-transformation.html#selecting-relevant-columns",
    "title": "Data Transformation",
    "section": "Selecting relevant columns",
    "text": "Selecting relevant columns\nYou can focus your data set on only those variables you are interested in using select(). The following code selects only the country, year, and pop variables from our larger gapminder data set:\n\nselect(gapminder, country, year, pop)\n\n# A tibble: 1,704 × 3\n   country      year      pop\n   &lt;fct&gt;       &lt;int&gt;    &lt;int&gt;\n 1 Afghanistan  1952  8425333\n 2 Afghanistan  1957  9240934\n 3 Afghanistan  1962 10267083\n 4 Afghanistan  1967 11537966\n 5 Afghanistan  1972 13079460\n 6 Afghanistan  1977 14880372\n 7 Afghanistan  1982 12881816\n 8 Afghanistan  1987 13867957\n 9 Afghanistan  1992 16317921\n10 Afghanistan  1997 22227415\n# ℹ 1,694 more rows\n\n\ndplyr includes some operators that help keep your code clean when working with a lot of data. For example, you can use a colon (:) to select all columns between two columns:\n\nselect(gapminder, country:pop)\n\n# A tibble: 1,704 × 5\n   country     continent  year lifeExp      pop\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;\n 1 Afghanistan Asia       1952    28.8  8425333\n 2 Afghanistan Asia       1957    30.3  9240934\n 3 Afghanistan Asia       1962    32.0 10267083\n 4 Afghanistan Asia       1967    34.0 11537966\n 5 Afghanistan Asia       1972    36.1 13079460\n 6 Afghanistan Asia       1977    38.4 14880372\n 7 Afghanistan Asia       1982    39.9 12881816\n 8 Afghanistan Asia       1987    40.8 13867957\n 9 Afghanistan Asia       1992    41.7 16317921\n10 Afghanistan Asia       1997    41.8 22227415\n# ℹ 1,694 more rows\n\n\nYou can use a negative sign (-) to specify which columns you want to exclude:\n\nselect(gapminder, -(lifeExp:pop))\n\n# A tibble: 1,704 × 4\n   country     continent  year gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952      779.\n 2 Afghanistan Asia       1957      821.\n 3 Afghanistan Asia       1962      853.\n 4 Afghanistan Asia       1967      836.\n 5 Afghanistan Asia       1972      740.\n 6 Afghanistan Asia       1977      786.\n 7 Afghanistan Asia       1982      978.\n 8 Afghanistan Asia       1987      852.\n 9 Afghanistan Asia       1992      649.\n10 Afghanistan Asia       1997      635.\n# ℹ 1,694 more rows\n\n\nExercises\nSelect only the country, year, and lifeExp variables from gapminder.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nselect(gapminder, country, year, lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\n\n\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvars &lt;- c(\"country\", \"year\", \"lifeExp\", \"boop\")\n\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nselect(gapminder, any_of(vars))\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\n\n\n\n\nWhat does the following code produce?\n\nselect(gapminder, starts_with(\"c\"))\n\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nselect(gapminder, starts_with(\"c\"))\n\n# A tibble: 1,704 × 2\n   country     continent\n   &lt;fct&gt;       &lt;fct&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ℹ 1,694 more rows",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#create-or-change-columns-of-data",
    "href": "content/02-transformation.html#create-or-change-columns-of-data",
    "title": "Data Transformation",
    "section": "Create or change columns of data",
    "text": "Create or change columns of data\nOftentimes, we need to create new columns from existing ones or change in a consistent way existing columns of data. You can use mutate() to do this. For example, the following code creates a new column, gdp, which is the product of gdpPercap and pop:\n\nmutate(gapminder, gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap          gdp\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n# ℹ 1,694 more rows\n\n\nThe following code transforms the existing column gdpPercap to its logged form:\n\nmutate(gapminder, gdpPercap = log(gdpPercap))\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      6.66\n 2 Afghanistan Asia       1957    30.3  9240934      6.71\n 3 Afghanistan Asia       1962    32.0 10267083      6.75\n 4 Afghanistan Asia       1967    34.0 11537966      6.73\n 5 Afghanistan Asia       1972    36.1 13079460      6.61\n 6 Afghanistan Asia       1977    38.4 14880372      6.67\n 7 Afghanistan Asia       1982    39.9 12881816      6.89\n 8 Afghanistan Asia       1987    40.8 13867957      6.75\n 9 Afghanistan Asia       1992    41.7 16317921      6.48\n10 Afghanistan Asia       1997    41.8 22227415      6.45\n# ℹ 1,694 more rows\n\n\nYou can combine select() and mutate() using the transmute() function:\n\ntransmute(gapminder, country, year, gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 3\n   country      year          gdp\n   &lt;fct&gt;       &lt;int&gt;        &lt;dbl&gt;\n 1 Afghanistan  1952  6567086330.\n 2 Afghanistan  1957  7585448670.\n 3 Afghanistan  1962  8758855797.\n 4 Afghanistan  1967  9648014150.\n 5 Afghanistan  1972  9678553274.\n 6 Afghanistan  1977 11697659231.\n 7 Afghanistan  1982 12598563401.\n 8 Afghanistan  1987 11820990309.\n 9 Afghanistan  1992 10595901589.\n10 Afghanistan  1997 14121995875.\n# ℹ 1,694 more rows\n\n\nExercises\nCreate a new variable that provides each country-year’s GDP (which you can get by multiplying its GDP with its population).\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nmutate(gapminder, gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap          gdp\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n# ℹ 1,694 more rows\n\n\n\n\n\nCreate a new data set that only includes information on each country-year’s name, year, and GDP.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\ntransmute(gapminder, country, year, gdp = gdpPercap * pop)\n\n# A tibble: 1,704 × 3\n   country      year          gdp\n   &lt;fct&gt;       &lt;int&gt;        &lt;dbl&gt;\n 1 Afghanistan  1952  6567086330.\n 2 Afghanistan  1957  7585448670.\n 3 Afghanistan  1962  8758855797.\n 4 Afghanistan  1967  9648014150.\n 5 Afghanistan  1972  9678553274.\n 6 Afghanistan  1977 11697659231.\n 7 Afghanistan  1982 12598563401.\n 8 Afghanistan  1987 11820990309.\n 9 Afghanistan  1992 10595901589.\n10 Afghanistan  1997 14121995875.\n# ℹ 1,694 more rows",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#summarize-your-data",
    "href": "content/02-transformation.html#summarize-your-data",
    "title": "Data Transformation",
    "section": "Summarize your data",
    "text": "Summarize your data\nFinally, you can summarize (or aggregate) your data. For example, we often want to find the average of our observations. The following code finds the average population and GDP per capita for all country-years in our data set:\n\nsummarise(\n  gapminder, avg_pop = mean(pop), avg_gdp_per_cap = mean(gdpPercap)\n)\n\n# A tibble: 1 × 2\n    avg_pop avg_gdp_per_cap\n      &lt;dbl&gt;           &lt;dbl&gt;\n1 29601212.           7215.\n\n\nNote that the output is now one row long.\nThe following finds both the average and mean of our country-years’ populations and GDPs per capita:\n\nsummarise(\n  gapminder, \n  avg_pop = mean(pop), \n  median_pop = median(pop), \n  avg_gdp_per_cap = mean(gdpPercap),\n  median_gdp_per_cap = median(gdpPercap)\n)\n\n# A tibble: 1 × 4\n    avg_pop median_pop avg_gdp_per_cap median_gdp_per_cap\n      &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;\n1 29601212.   7023596.           7215.              3532.\n\n\nSometimes, we want to summarize our data within meaningful groups. For example, we may want to find the average population and GDP per capita for each continent included in our data set. To do this, you need to use the group_by() function to group your data. You can then use the summarise() function to summarize it within those groups:\n\ngapminder_continent &lt;- group_by(gapminder, continent)\n\nsummarise(\n  gapminder_continent, \n  avg_pop = mean(pop), \n  avg_gdp_per_cap = mean(gdpPercap)\n)\n\n# A tibble: 5 × 3\n  continent   avg_pop avg_gdp_per_cap\n  &lt;fct&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1 Africa     9916003.           2194.\n2 Americas  24504795.           7136.\n3 Asia      77038722.           7902.\n4 Europe    17169765.          14469.\n5 Oceania    8874672.          18622.\n\n\nExercises\nCalculate each country-year’s GDP.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nmutate(gapminder, gdp = pop * gdpPercap)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap          gdp\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n 2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n 3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n 4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n 5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n 6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n 7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n 8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n 9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n# ℹ 1,694 more rows\n\n\n\n\n\nFind each country’s average GDP across all years in the gapminder data set.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\ngapminder_country &lt;- group_by(gapminder, country)\n\ngapminder_country_avg &lt;- summarise(\n  mutate(gapminder_country, gdp = pop * gdpPercap), avg_gdp = mean(gdp)\n)\n\ngapminder_country_avg\n\n# A tibble: 142 × 2\n   country           avg_gdp\n   &lt;fct&gt;               &lt;dbl&gt;\n 1 Afghanistan  12709647583.\n 2 Albania       9094669267.\n 3 Algeria      96735171261.\n 4 Angola       25532681843.\n 5 Argentina   266754123835.\n 6 Australia   320253755823.\n 7 Austria     158579002935.\n 8 Bahrain       7694793798.\n 9 Bangladesh   80648494456.\n10 Belgium     197371599665.\n# ℹ 132 more rows\n\n\n\n\n\nFind the country with the smallest average GDP across these years.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\narrange(gapminder_country_avg, avg_gdp)\n\n# A tibble: 142 × 2\n   country                   avg_gdp\n   &lt;fct&gt;                       &lt;dbl&gt;\n 1 Sao Tome and Principe  151723722.\n 2 Comoros                450509962.\n 3 Gambia                 551998132.\n 4 Guinea-Bissau          583673324.\n 5 Djibouti               647199614.\n 6 Liberia               1054141313.\n 7 Equatorial Guinea     1143738921.\n 8 Lesotho               1257957241.\n 9 Eritrea               1683400010.\n10 Burundi               2253225196.\n# ℹ 132 more rows\n\n\n\n\n\nFind the country with the largest average GDP across these years.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\narrange(gapminder_country_avg, desc(avg_gdp))\n\n# A tibble: 142 × 2\n   country        avg_gdp\n   &lt;fct&gt;            &lt;dbl&gt;\n 1 United States  6.40e12\n 2 Japan          2.12e12\n 3 China          1.70e12\n 4 Germany        1.62e12\n 5 United Kingdom 1.11e12\n 6 France         1.04e12\n 7 Italy          9.11e11\n 8 India          8.62e11\n 9 Brazil         8.11e11\n10 Mexico         5.95e11\n# ℹ 132 more rows",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/02-transformation.html#the-pipe",
    "href": "content/02-transformation.html#the-pipe",
    "title": "Data Transformation",
    "section": "The pipe",
    "text": "The pipe\nThat got messy! We had a lot of different objects representing intermediate steps in our calculations, but we never need those objects again. Can we avoid creating them?\nLet’s introduce perhaps the defining feature of the tidyverse: the pipe.\nRead the pipe (|&gt;) as:\nTake this |&gt; (and then…)\ndo this |&gt; (and then…)\ndo this\nFor example:\n\ngapminder |&gt; \n  group_by(continent) |&gt; \n  summarise(avg_pop = mean(pop), avg_gdp_per_cap = mean(gdpPercap)) |&gt;\n  arrange(avg_gdp_per_cap)\n\n# A tibble: 5 × 3\n  continent   avg_pop avg_gdp_per_cap\n  &lt;fct&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1 Africa     9916003.           2194.\n2 Americas  24504795.           7136.\n3 Asia      77038722.           7902.\n4 Europe    17169765.          14469.\n5 Oceania    8874672.          18622.\n\n\nReads as:\n\nTake the gapminder data set, and then…\nGroup it by continent, and then…\nFind the average population and GDP per capita for each of those continents, and then…\nSort those summaries by their average GDPs per capita (in ascending order).\n\nYou can combine all tidyverse functions with the pipe:\n\ngapminder |&gt; \n  group_by(continent) |&gt; \n  summarise(avg_pop = mean(pop), avg_gdp_per_cap = mean(gdpPercap)) |&gt; \n  ggplot(aes(x = continent, y = avg_gdp_per_cap)) + \n  geom_col() + \n  theme_minimal()\n\n\n\n\n\n\n\nIt is worth noting that there are two versions of the pipe running around. The first |&gt; is the base pipe. This comes straight out of the box with R. You do not need to load in any packages to use it. The second, %&gt;%, is the tidyverse pipe. To use it, you need to load in either dplyr or the magrittr package.\nI have switched to using the base pipe, so that’s the one you will see in all of my code. You can use whichever you prefer!\nExercises\nUse the pipe to calculate the average GDP per capita for countries in the Americas in all years including or after 2000.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\ngapminder_americas_2000 &lt;- gapminder |&gt; \n  filter(continent == \"Americas\", year &gt; 2000) |&gt; \n  group_by(country) |&gt; \n  summarise(avg_gdp_per_cap = mean(gdpPercap))\n\ngapminder_americas_2000\n\n# A tibble: 25 × 2\n   country            avg_gdp_per_cap\n   &lt;fct&gt;                        &lt;dbl&gt;\n 1 Argentina                   10789.\n 2 Bolivia                      3618.\n 3 Brazil                       8599.\n 4 Canada                      34824.\n 5 Chile                       11975.\n 6 Colombia                     6381.\n 7 Costa Rica                   8684.\n 8 Cuba                         7644.\n 9 Dominican Republic           5295.\n10 Ecuador                      6323.\n# ℹ 15 more rows\n\n\n\n\n\nPlot your results.\n\n\n\n\n\n\nCheck your answer\n\n\n\n\n\n\nggplot(gapminder_americas_2000, aes(x = avg_gdp_per_cap, y = reorder(country, avg_gdp_per_cap))) + \n  geom_col() +\n  theme_minimal() + \n  labs(title = \"Average GDP per capita for countries in the Americas in 2002 and 2007\",\n       x = \"Average GDP per capita (US$)\",\n       y = NULL) + \n  scale_x_continuous(labels = scales::label_dollar())\n\n\n\n\n\n\n\nNote: Check out the scales R package for very handy formatting functions: https://scales.r-lib.org.",
    "crumbs": [
      "Content",
      "Session 2",
      "Data Transformation"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html",
    "href": "content/07-uncertainty.html",
    "title": "Measuring Uncertainty",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"fitzRoy\", \"tidyverse\", \"broom\", \"modelsummary\", \"ggdist\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(fitzRoy)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(ggdist)\n\nset.seed(1234)",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#set-up",
    "href": "content/07-uncertainty.html#set-up",
    "title": "Measuring Uncertainty",
    "section": "",
    "text": "To complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"fitzRoy\", \"tidyverse\", \"broom\", \"modelsummary\", \"ggdist\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(fitzRoy)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(ggdist)\n\nset.seed(1234)",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#introduction",
    "href": "content/07-uncertainty.html#introduction",
    "title": "Measuring Uncertainty",
    "section": "Introduction",
    "text": "Introduction\nThis session, we are going to bring everything together. We will explore the linear relationship between two variables focusing not only on the direction and strength of that relationship, but also how certain we can be in that the relationship. At the end of this session you will be able to describe how those two variables relate to one another, and whether that relationship is statistically significant.\nAs usual, we will build our understanding using an example. As an Australian (more specifically, a Victorian), I love the Australian Football League. It’s the best sport out there! We’re going to depart from our usual political science examples to explore some interesting relationships in the way AFL is played.\nFirst, let’s watch this brief introduction to the game:\n\nLet’s explore the relationship between disposal efficiency and Dream Team points. Your disposal efficiency describes the percentage of your disposals of the ball (kicks, handballs, etc.) that lead to a positive outcome for your team. The Dream Team is a popular fantasy football competition. Each player gets points for various actions they take on the field. For example, you get six Dream Team points for scoring a goal and you lose three points for having a free kick awarded against you. This is a useful proxy measure of a player’s effectiveness on the field.",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#loading-in-our-data",
    "href": "content/07-uncertainty.html#loading-in-our-data",
    "title": "Measuring Uncertainty",
    "section": "Loading in our data",
    "text": "Loading in our data\nWe are going to explore some different player-level outcomes from the 10th round of the most recent AFLW season. This is the last round played before finals. First, we need to collect our data. fitzRoy::fetch_player_stats() retrieves some useful variables about each player from the official AFL website.\n\nafl_df &lt;- fetch_player_stats(2024, round = 10, comp = \"AFLW\")\nafl_df\n\n# A tibble: 379 × 70\n   providerId      utcStartTime           status compSeason.shortName round.name\n   &lt;chr&gt;           &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;                &lt;chr&gt;     \n 1 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 2 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 3 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 4 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 5 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 6 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 7 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 8 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n 9 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n10 CD_M20242641006 2024-11-01T08:15:00.0… CONCL… 2024 NAB AFLW Season Week 10   \n# ℹ 369 more rows\n# ℹ 65 more variables: round.roundNumber &lt;int&gt;, venue.name &lt;chr&gt;,\n#   home.team.name &lt;chr&gt;, home.team.club.name &lt;chr&gt;, away.team.name &lt;chr&gt;,\n#   away.team.club.name &lt;chr&gt;, player.jumperNumber &lt;int&gt;,\n#   player.photoURL &lt;chr&gt;, player.player.position &lt;chr&gt;,\n#   player.player.player.playerId &lt;chr&gt;, player.player.player.captain &lt;lgl&gt;,\n#   player.player.player.playerJumperNumber &lt;int&gt;, …\n\n\nIn this round, 379 players played in 9 games. We have access to 70 variables describing how these players performed in this round.",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#uncovering-the-relationship-between-two-variables",
    "href": "content/07-uncertainty.html#uncovering-the-relationship-between-two-variables",
    "title": "Measuring Uncertainty",
    "section": "Uncovering the relationship between two variables",
    "text": "Uncovering the relationship between two variables\nFirst, we want to determine the direction of the relationship between our variables of interest. When a player’s disposal efficiency increases, do they tend to get a greater or fewer number of Dream Team points?\nWe can often identify the direction of the relationship by plotting our data:\n\nggplot(afl_df, aes(x = disposalEfficiency, y = dreamTeamPoints)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\n\nThe relationship looks positive: as your efficiency increases, so too do your Dream Team points. This makes sense. A good player is more effective (tends to produce good results from their kicks and handballs). Our Dream Team points are a proxy for an effective player. However, there is a fair bit of noise here. This relationship doesn’t look very strong.\nNext, we want to formalize that relationship. We want to find the line that best fits all of these points. In other words, what line minimizes the distance between itself and all of the observed points marking each player’s Dream Team points and disposal efficiency?\n\n\n\n\n\n\nTip\n\n\n\nRemember, this is the basis for Ordinary Least Squares regression.\n\n\nWe can use ggplot2::geom_smooth() to fit this line graphically:\n\nggplot(afl_df, aes(x = disposalEfficiency, y = dreamTeamPoints)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\n\n\nWe correctly identified that there is a positive relationship between those two variables. Yay!\nOften we need more information than simply the direction of a relationship. For example, we want to know how pronounced the effect of a one-unit increase in our independent variable will be on our outcome of interest. Here, we want to learn how many additional Dream Team points are associated with a a one percentage point increase in a player’s disposal efficiency, on average.\nWe can use lm() to determine this information:\n\n\n\n\n\n\nTip\n\n\n\nRemember to put your dependent variable first, then a ~, then you independent variable(s).\n\n\n\nm &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_df)\nsummary(m)\n\n\nCall:\nlm(formula = dreamTeamPoints ~ disposalEfficiency, data = afl_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-51.16 -18.77  -4.29  14.42 115.49 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        43.53517    4.42859   9.830   &lt;2e-16 ***\ndisposalEfficiency  0.11626    0.06751   1.722   0.0858 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.71 on 377 degrees of freedom\nMultiple R-squared:  0.007806,  Adjusted R-squared:  0.005174 \nF-statistic: 2.966 on 1 and 377 DF,  p-value: 0.08585\n\n\nGreat! We now have a linear regression model of the relationship between a player’s disposal efficiency and their Dream Team points.\nWe predict that players that have a disposal efficiency of zero (none of their disposals result in a good outcome for their team) have, on average, 43.5 Dream Team points. Every one percentage point increase in a player’s efficiency is associated with a gain of 0.116 Dream Team points, on average.\nGreat! We can use this information to understand this relationship more meaningfully. For example, we note that a player with a disposal efficiency of 1.16 percentage points greater than another player has, on average, 10 more Dream Team points than the other player.",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#uncertainty-around-this-relationship",
    "href": "content/07-uncertainty.html#uncertainty-around-this-relationship",
    "title": "Measuring Uncertainty",
    "section": "Uncertainty around this relationship",
    "text": "Uncertainty around this relationship\nHow confident can we be in this modeled relationship? Am I really sure that efficiency has a significant, positive impact on a player’s Dream Team points? I mean, that line looks pretty flat. Could this really just be driven by noise? To answer these questions, we need to draw on everything we have learned about probability theory and inference so far.\nWe want to make a claim about the relationship between some outcome and some variables that we think are important determinants of that outcome. Here, we used the observed Dream Team points gained by a player and their disposal efficiency for round 10 in the 2024 season of AFLW to make a broader statement about the relationship between efficiency and Dream Team points. We are not actually interested in the specific relationship between these variables in round 10 of the 2024 season: we want to know whether a player’s efficiency is an important determinant of their Dream Team points generally.\nIn other words, we are inferring from a sample a general relationship. You can imagine that if we had a different sample, a linear regression model would have a different set of estimates for our intercept and coefficient. They would (hopefully) look very similar to the ones we found above, but they would be slightly different. For example, imagine that some of the random elements of a game of AFL were different: the wind blew in a different way, the crowd cheered a little louder, a player ran a little faster. These would change the game played in slight and random ways. A player might make a clanger instead of a clean disposal. The wind may blow an otherwise goal into a behind. Consequently, the players’ disposal efficiency would be slightly different. We would subsequently get different model estimates.\nLet’s illustrate this. To be able to access this variation to illustrate my point, I am going to take pure random samples from our 379 players. Let’s take a completely random sample of 250 players from our data and fit our regression against that sample:\n\nafl_sample_1 &lt;- afl_df |&gt; \n  sample_n(250) |&gt;\n  select(player.player.player.givenName, \n         player.player.player.surname,\n         dreamTeamPoints,\n         disposalEfficiency)\nafl_sample_1\n\n# A tibble: 250 × 4\n   player.player.player.givenName player.player.player.surname dreamTeamPoints\n   &lt;chr&gt;                          &lt;chr&gt;                                  &lt;dbl&gt;\n 1 Bridie                         Hipwell                                   17\n 2 Katherine                      Smith                                     39\n 3 Julie                          O'Sullivan                                29\n 4 Amy                            Franklin                                  30\n 5 Taylah                         Gatt                                      41\n 6 Brooke                         Lochland                                  56\n 7 Ruby                           Sargent-Wilson                            51\n 8 Rene                           Caris                                     14\n 9 Montana                        Ham                                       46\n10 Isabel                         Huntington                                49\n# ℹ 240 more rows\n# ℹ 1 more variable: disposalEfficiency &lt;dbl&gt;\n\n\n\nm_1 &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample_1)\n\ntidy(m_1)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)         44.9       5.43        8.28 7.92e-15\n2 disposalEfficiency   0.0885    0.0834      1.06 2.89e- 1\n\n\nOkay, so we got a different intercept and coefficient estimates than we got above. This is despite the fact that I took a pure random sample from the full set of players.\nLet’s do this again. I will take a different random sample of 250 players and fit a regression for them:\n\nafl_sample_2 &lt;- afl_df |&gt; \n  sample_n(250) |&gt;\n  select(player.player.player.givenName, \n         player.player.player.surname,\n         dreamTeamPoints,\n         disposalEfficiency)\nafl_sample_2\n\n# A tibble: 250 × 4\n   player.player.player.givenName player.player.player.surname dreamTeamPoints\n   &lt;chr&gt;                          &lt;chr&gt;                                  &lt;dbl&gt;\n 1 Brooke                         Walker                                    48\n 2 Abbie                          McKay                                      0\n 3 Clara                          Fitzpatrick                               19\n 4 Nat                            Exon                                      49\n 5 Alana                          Porter                                    40\n 6 Niamh                          Kelly                                     67\n 7 Gabby                          Biedenweg-Webster                         11\n 8 Emma                           Swanson                                   38\n 9 Georgia                        Gee                                       91\n10 Georgia                        Nanscawen                                 91\n# ℹ 240 more rows\n# ℹ 1 more variable: disposalEfficiency &lt;dbl&gt;\n\n\n\nm_2 &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample_2)\n\ntidy(m_2)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)         45.5       5.52        8.24 9.87e-15\n2 disposalEfficiency   0.0882    0.0837      1.05 2.93e- 1\n\n\nAgain, we got a different set of coefficients. The only thing driving this difference is random chance. We took a random sample of the same number of players as above from the same population.\nLet’s go again:\n\nafl_sample_3 &lt;- afl_df |&gt; \n  sample_n(250) |&gt;\n  select(player.player.player.givenName, \n         player.player.player.surname,\n         dreamTeamPoints,\n         disposalEfficiency)\nafl_sample_3\n\n# A tibble: 250 × 4\n   player.player.player.givenName player.player.player.surname dreamTeamPoints\n   &lt;chr&gt;                          &lt;chr&gt;                                  &lt;dbl&gt;\n 1 Joanne                         Cregg                                     30\n 2 Sophie                         Alexander                                 51\n 3 Ash                            Riddell                                  111\n 4 Shanae                         Davison                                   18\n 5 Brodee                         Mowbray                                   31\n 6 Paige                          Trudgeon                                  38\n 7 Mikayla                        Hyde                                      20\n 8 Ella                           Roberts                                   78\n 9 Daria                          Bannister                                 76\n10 Hannah                         Priest                                    38\n# ℹ 240 more rows\n# ℹ 1 more variable: disposalEfficiency &lt;dbl&gt;\n\n\n\nm_3 &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample_3)\n\ntidy(m_3)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          38.7      5.40        7.18 8.33e-12\n2 disposalEfficiency    0.185    0.0825      2.25 2.55e- 2\n\n\nAgain, we get different estimates. The only difference was pure random chance.\nWe can see these differences more clearly by plotting them:\n\nafl_sample_1 |&gt; \n  mutate(sample = \"One\") |&gt; \n  bind_rows(\n    mutate(afl_sample_2, sample = \"Two\")\n  ) |&gt; \n  bind_rows(\n    mutate(afl_sample_3, sample = \"Three\")\n  ) |&gt; \n  ggplot(aes(x = disposalEfficiency, y = dreamTeamPoints, colour = sample)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\n\n\nEach of our pure random samples led our model to estimate a slightly different relationship between disposal efficiency and Dream Team points. For example, sample one leads us to believe that a one percentage point increase in a player’s disposal efficiency is associated with a 0.09 point increase in their Dream Team points, on average. Model 3, on the other hand, would suggest that this same change in disposal efficiency is associated with a 0.19 point increase in a player’s Dream Team points, on average.\nLet’s do this 1,000 times and take a look at the range of different estimates produced by random chance. First, I write a function that takes a random sample of 250 players from the full player list. It then fits a linear regression model using that sample. Finally, it returns the coefficients for that model in a format that is easy for us to work with.\n\nafl_sample_regression &lt;- function(i) {\n  \n  afl_sample &lt;- sample_n(afl_df, 250)\n  \n  m &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample)\n  \n  model_results &lt;- tibble(trial = i, model = tidy(m))\n  \n  return(model_results)\n  \n}\n\nafl_regressions &lt;- map(1:1000, afl_sample_regression, .progress = T) |&gt; \n  bind_rows()\n\nNow we can take a look at those different regression coefficients:\n\nunnest(afl_regressions, model)\n\n# A tibble: 2,000 × 6\n   trial term               estimate std.error statistic  p.value\n   &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 (Intercept)         44.2       5.76        7.67 3.80e-13\n 2     1 disposalEfficiency   0.103     0.0873      1.18 2.40e- 1\n 3     2 (Intercept)         44.3       5.73        7.73 2.60e-13\n 4     2 disposalEfficiency   0.108     0.0870      1.24 2.16e- 1\n 5     3 (Intercept)         45.5       5.62        8.10 2.54e-14\n 6     3 disposalEfficiency   0.112     0.0843      1.32 1.87e- 1\n 7     4 (Intercept)         42.7       5.52        7.73 2.66e-13\n 8     4 disposalEfficiency   0.104     0.0842      1.23 2.19e- 1\n 9     5 (Intercept)         45.6       5.85        7.79 1.88e-13\n10     5 disposalEfficiency   0.0923    0.0883      1.05 2.97e- 1\n# ℹ 1,990 more rows\n\n\nThis shows the intercept and coefficient for the relationship between disposal efficiency and Dream Team points for each of the models we generated from our 1,000 different random samples. Remember, the only thing driving the differences between each model’s intercept and coefficient is random chance.\nLet’s start with the disposal efficiency coefficients:\n\nafl_regressions |&gt; \n  unnest(model) |&gt; \n  filter(term == \"disposalEfficiency\") |&gt; \n  ggplot(aes(x = estimate)) + \n  stat_halfeye() + \n  theme_minimal() +\n  labs(x = \"Coefficient estimate\",\n       y = \"Density\",\n       caption = \"Median is shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\n\n\n\nHere is the distribution of those 1,000 different coefficients for disposal efficiency for each model. We can see a wide range of estimates for the relationship between disposal efficiency and Dream Team points. Some samples even led our model to estimate a negative relationship!\nNext, we can look at the intercept estimates:\n\nafl_regressions |&gt; \n  unnest(model) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  ggplot(aes(x = estimate)) + \n  stat_halfeye() + \n  theme_minimal() + \n  labs(x = \"Coefficient estimate\",\n       y = \"Density\",\n       caption = \"Median is shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\n\n\n\nAgain, we have a wide range of estimates. Some are more common than others: the average of our 1,000 estimates (43.637) is the most common one produced. Estimates far from that average are less common. Similarly, the average of the 1,000 estimates produced for the disposal efficiency variable (0.115) was the most common one produced.\nWe can use this knowledge to focus back on that single regression model that we built right at the start of this session. Those averages are very, very close to the estimates produced by that first model:\n\nsummary(m)\n\n\nCall:\nlm(formula = dreamTeamPoints ~ disposalEfficiency, data = afl_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-51.16 -18.77  -4.29  14.42 115.49 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        43.53517    4.42859   9.830   &lt;2e-16 ***\ndisposalEfficiency  0.11626    0.06751   1.722   0.0858 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.71 on 377 degrees of freedom\nMultiple R-squared:  0.007806,  Adjusted R-squared:  0.005174 \nF-statistic: 2.966 on 1 and 377 DF,  p-value: 0.08585",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#building-uncertainty-into-our-single-regression-model",
    "href": "content/07-uncertainty.html#building-uncertainty-into-our-single-regression-model",
    "title": "Measuring Uncertainty",
    "section": "Building uncertainty into our single regression model",
    "text": "Building uncertainty into our single regression model\nGenerally, we have a single sample that we need to use to fit our models. We still need to account for the uncertainty (demonstrated above) created by random chance. Unfortunately, we only have one sample. We, therefore, need to estimate the sampling distribution that would arise from an infinite number of samples from our population using this one lonely sample. To do this, we assume that the estimates our one sample produced are (close to) the estimates that would be produced by a model fit against the population. So, we center our estimated sampling distributions at these best guesses.\n\ntidy(m) |&gt; \n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term               estimate\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 (Intercept)          43.5  \n2 disposalEfficiency    0.116\n\n\nThese are our best guesses at the true linear relationship between disposal efficiency and Dream Team points. Our best guess is that players who have a disposal efficiency of zero have, on average, 43.5 Dream Team points. Every one percentage point increase in a player’s efficiency is associated with a gain of 0.12 Dream Team points, on average.\nNow, we need to build out the plausible set of intercepts and coefficients that would result from an infinite number of random samples drawn from our population. To do this, we need to work out how spread out these intercepts and coefficients would be from their center points. Formally, this spread is called the standard deviation.\nThe standard deviation, \\(s\\), is calculated using two pieces of information. First, it looks at how well our line of best fit (our regression model) fits our observed data. How far are the predicted values (represented by the blue line on the below graph) from the observed values (represented by the black dots)?\n\nggplot(afl_df, aes(x = disposalEfficiency, y = dreamTeamPoints)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\n\n\nWe will have more uncertainty around our intercept and coefficient if the distance between the predicted values and observed values is large. This makes sense: if the predicted values and the observed values were very similar, we are unlikely to find a wildly different line of best fit from a different random sample from our population.\nThe standard deviation also takes into account the amount of information you have used to generate this intercept and coefficient. Greater sample sizes using less variables result in less uncertainty around those model coefficients.\nIn sum, the standard deviation is calculated as:\n\\[\ns = \\sqrt{\\frac{\\sum{e_i^2}}{n-k-1}}\n\\]\nWhere \\(e_i\\) is the difference between each predicted value and observed value (the vertical distance between each point and the blue line on the graph above).\n\\(n-k-1\\) is the degrees of freedom in the model. It accounts for the amount of information you used to build the model, with \\(n\\) equal to the number of observations you used and \\(k\\) equal to the number of independent variables you included. Here, our \\(n =\\) 379 because that’s the number of observations (players) we used to fit the model, and \\(k=1\\) because we are only using one independent variable: disposal efficiency.\nWe can use this spread to work out our standard errors around our coefficients. Broadly, the standard error places this uncertainty within the context of the model.\nFor the intercept, it is:\n\\[\nSE_{\\beta_0} = s(\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum{(x_i-\\bar{x})^2}}})\n\\]\nFor the coefficients, it is:\n\\[\nSE_{\\beta_1} = \\frac{s}{\\sqrt{\\sum{(x_i-\\bar{x})^2}}}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nYou will calculate all of this in the background. Do not worry about memorizing these formulas. I put them here to help build your intuition about what goes into your uncertainty around the relationship you find between your outcome of interest and the variables you think drive that outcome.\n\n\nHappily, broom::tidy() calculates all of this for us:\n\ntidy(m) |&gt; \n  select(term:std.error)\n\n# A tibble: 2 × 3\n  term               estimate std.error\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)          43.5      4.43  \n2 disposalEfficiency    0.116    0.0675\n\n\nOkay, so we have a good sense of the spread around our best guess. Let’s visualize this. We will start with the disposal efficiency coefficient estimate:\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_halfeye() + \n  theme_minimal() + \n  labs(x = \"Coefficient estimate\",\n       y = \"Density\")\n\n\n\n\n\n\n\nAbove, we show the plausible set of coefficients describing the effect of a one percentage point increase in a player’s disposal efficiency on their Dream Team points, on average. Remember, our best guess (taken from our regression model) is that a one percentage point increase in efficiency is associated with a 0.12 increase in a player’s Dream Team points, on average. However, as we saw above when we built 1,000 different models from 1,000 different random samples, it is entirely plausible that we could get different coefficients from a model built from a different sample. These differences are the product of random chance. That’s fine! We just need to acknowledge that.\nLet’s also visualize all of the plausible intercepts we could get:\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"(Intercept)\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"(Intercept)\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_halfeye() + \n  theme_minimal()\n\n\n\n\n\n\n\nAgain, our best guess is that players who have a disposal efficiency of zero have, on average, 43.5 Dream Team points. This is one of many plausible intercepts that we could generate.",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#connecting-this-to-our-research-question",
    "href": "content/07-uncertainty.html#connecting-this-to-our-research-question",
    "title": "Measuring Uncertainty",
    "section": "Connecting this to our research question",
    "text": "Connecting this to our research question\nWe used our data to uncover an interesting relationship between disposal efficiency and Dream Team points. On average, as your disposal efficiency increases, so too do your Dream Team points. We found that if players who increased their disposal efficiency by one percentage point saw an increase of 0.12 Dream Team points, on average.\nNow, imagine that you are a player and you want to increase your Dream Team points. Do you trust this? Do you really believe that by increasing your disposal efficiency you will increase your Dream Team points? What if, in fact, there is no relationship between these two variables?\nWe know from above that there are a range of plausible intercepts and coefficients that result from random chance. Does this plausible range include zero? In other words, is it plausible that there is no relationship between the outcome of interest (Dream Team points) and your independent variable (disposal efficiency)?\nFirst, we need to define what we mean by a plausible range of relationships. Traditionally, we are willing to accept a five percent risk that we declare that there is a relationship between an outcome and an independent variable when there is, in fact, no relationship. Let’s stick with that threshold.\n\n\n\n\n\n\nNote\n\n\n\nThe political science journal articles you have read will often talk about a p-value of 0.05. This threshold of a five percent risk that we declare that there is a relationship between an outcome and an independent variable when there is, in fact, no relationship is the functional definition of that p-value.\n\n\nOkay, so now we need to work out where the other 95 percent of plausible values sit. Let’s start with the coefficient describing the relationship between Dream Team points and disposal efficiency. Here is our representation of the coefficients drawn from an infinite number random samples pulled from our population (it is the same as printed above):\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_halfeye() + \n  theme_minimal() + \n  labs(x = \"Coefficient estimate\",\n       y = \"Density\")\n\n\n\n\n\n\n\nWhere do 95 percent of these coefficients fall around our best guess? Because we estimated the spread of our estimates using the standard error, we cannot use the usual (and helpful) standard Normal distribution to answer this question. Instead, we need to look at a similar, but different distribution: the t-distribution.\n\n\n\n\n\n\nThe t-distribution\n\n\n\nThe t-distribution is very similar to the Normal distribution, but it depends on one important factor that the Normal distribution does not: the amount of information you used to fit your model.\nThere are an infinite number of t-distributions. Each is defined by its degrees of freedom. Degrees of freedom account for both the number of observations and the number of independent variables you used to fit your model. We will discuss how to use the t-distribution below, but for now here are some t-distributions with different degrees of freedom:\n\nggplot() +\n  geom_density(aes(x = rt(1e6, df = 5)), colour = \"red\") +\n  geom_density(aes(x = rt(1e6, df = 50)), colour = \"blue\") +\n  geom_density(aes(x = rt(1e6, df = 5000)), colour = \"grey\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-5, 5))\n\n\n\n\n\n\n\nThe important thing to note here is that the t-distribution punishes you for using too little information to fit your models. The tails on the distribution with the smallest degrees of freedom (coloured in red) are fatter than the distributions with higher degrees of freedom. As you will learn below, this makes the bar required to reach statistical significance more difficult to clear.\nAnother thing to note is that this effect is diminishing in the amount of information (degrees of freedom) you have. Note that the difference between the distribution with five degrees of freedom and that with 50 is much larger than the difference between the distribution with 50 degrees of freedom and that with 5000. Practically, once you have more than a few observations and you do not cram your model with many independent variables, you will not be punished too harshly.\nFinally, as the degrees of freedom increase, the t-distribution increasingly resembles the Normal distribution.\n\n\nWe need to use our knowledge of the t-distribution to answer this question. We know that center point (our coefficient):\n\nbeta_1 &lt;- tidy(m) |&gt; \n  filter(term == \"disposalEfficiency\") |&gt; \n  pull(estimate)\nbeta_1\n\n[1] 0.1162632\n\n\nWe know how spread out our alternative coefficients are from that center point (our standard error):\n\nse_beta_1 &lt;- tidy(m) |&gt; \n  filter(term == \"disposalEfficiency\") |&gt; \n  pull(std.error)\nse_beta_1\n\n[1] 0.06750683\n\n\nAnd we know our threshold (5%). We need to translate that threshold into its t-statistic, accounting for the degrees of freedom we have access to:\n\nsample_size &lt;- nrow(afl_df)\nsample_size\n\n[1] 379\n\nno_of_IVs &lt;- 1\nno_of_IVs\n\n[1] 1\n\ndf &lt;- sample_size - no_of_IVs - 1\ndf\n\n[1] 377\n\n\nTherefore:\n\nt_stat_95 &lt;- qt(0.025, df = df, lower.tail = F)\nt_stat_95\n\n[1] 1.966276\n\n\nThis is the point beyond which 2.5% of all values along the t-distribution fall.\n\n\n\n\n\n\nTip\n\n\n\nRemember, we want to find out where 95 percent of these alternative coefficients sit around the center point. So, we need to distribute our remaining five percent between the two tails. That’s why we find the t-statistic beyond which 2.5% of the data fall.\n\n\nNow we can find the boundaries within which 95 percent of these alternative coefficients fall using the following formula:\n\\[\nCI = \\beta_1 \\pm t*SE_{\\beta_1}\n\\]\n\nlower_ci &lt;- beta_1 - t_stat_95*se_beta_1\nlower_ci\n\n[1] -0.01647391\n\nupper_ci &lt;- beta_1 + t_stat_95*se_beta_1\nupper_ci\n\n[1] 0.2490003\n\n\nLet’s place those in context:\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_ci | x &gt; upper_ci))) + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(x = \"Coefficient estimate\",\n       y = \"Density\")\n\n\n\n\n\n\n\nBrilliant! Now we know where 95 percent of all alternative coefficients drawn from a random sample of our population could fall. These are our plausible alternative coefficients.\nSo, do these plausible alternatives include zero? In other words, is it plausible that there is no relationship between a player’s disposal efficiency and their Dream Team points?\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_ci | x &gt; upper_ci))) + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(x = \"Coefficient estimate\",\n       y = \"Density\")\n\n\n\n\n\n\n\nYes! A null relationship sits within our plausible set of coefficients for disposal efficiency. We cannot reject the notion that there is no relationship between disposal efficiency and Dream Team points at this threshold.\nWhat about a more forgiving threshold? What if we are willing to accept a 10 percent chance that we reject a true null relationship?\nFirst, we need to find this new threshold’s t-statistic:\n\nt_stat_90 &lt;- qt(0.05, df = df, lower.tail = F)\nt_stat_90\n\n[1] 1.648905\n\n\nAnd; therefore, our new boundaries (within which 90 percent of alternative coefficients sit):\n\nlower_ci &lt;- beta_1 - t_stat_90*se_beta_1\nlower_ci\n\n[1] 0.004950792\n\nupper_ci &lt;- beta_1 + t_stat_90*se_beta_1\nupper_ci\n\n[1] 0.2275756\n\n\nDo these contain a null relationship?\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_ci | x &gt; upper_ci))) + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(x = \"Coefficient estimate\",\n       y = \"Density\") + \n  labs(x = \"Coefficient estimate\",\n       y = \"Density\")\n\n\n\n\n\n\n\nNo! So, if we are happy to accept a 10 percent risk that we will believe a relationship exists where one, if fact, does not, we can reject the null hypothesis that there is no relationship between disposal efficiency and Dream Team points. We can tell the players that we have found a statistically significant relationship between disposal efficiency and Dream Team points at the 0.1 threshold.\nWe can approach this question from the other direction. If the null hypothesis were true, how likely would we be to see our estimate?\nFirst, let’s set up our null world:\n\ntibble(\n  x = rt(1e6, df = df)\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(x = \"t\",\n       y = \"Density\")\n\n\n\n\n\n\n\nI am back to random draws from our t-distribution.\nHere, we have centered our distribution of alternative coefficients (resulting only from differences in our samples brought about by random chance) at zero. We are in the null world: there is no relationship between disposal efficiency and Dream Team points.\nWhere does the estimate we found in our sample sit within this null world? First, we need to translate that observed estimate into its t-statistic:\n\\[\nt = \\frac{\\beta_1}{SE_{\\beta_1}}\n\\]\n\nt_stat &lt;- beta_1 / se_beta_1\nt_stat\n\n[1] 1.722243\n\n\nLet’s place this in the context of the null world:\n\ntibble(\n  x = rt(1e6, df = df)\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab() + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nHow likely is it that I would get this coefficient or a more extreme coefficient if we did, in fact, live in the null world? In other words, what proportion of these alternative coefficients (highlighted in dark gray on the graph below) are equal to or more extreme than our observed estimate?\n\n\n\n\n\n\nTip\n\n\n\nRemember that we are conducting a two-tailed test of our null hypothesis. We need to be open to the estimate being greater or smaller than the null.\n\n\n\ntibble(\n  x = rt(1e6, df = df)\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; -t_stat | x &gt; t_stat))) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nThe proportion of alternative coefficients that are equal to or more extreme than the one we observed is:\n\np_value &lt;- 2 * pt(t_stat, df = df, lower.tail = F)\np_value\n\n[1] 0.08584565\n\n\nWe would observe an estimate of 0.116 or a more extreme estimate 9% of the time if the null hypothesis were true. When we are only happy to accept a five percent chance that we would reject a true null hypothesis, we cannot reject that null hypothesis. If, on the other hand, we are happy to accept a 10 percent chance that we reject a true null hypothesis, we can reject it.",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/07-uncertainty.html#reading-our-regression-outputs",
    "href": "content/07-uncertainty.html#reading-our-regression-outputs",
    "title": "Measuring Uncertainty",
    "section": "Reading our regression outputs",
    "text": "Reading our regression outputs\nLet’s return to our original model:\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          43.5      4.43        9.83 1.87e-20\n2 disposalEfficiency    0.116    0.0675      1.72 8.58e- 2\n\n\nWe now have information on every component of this output.\nWe can translate these estimates, in the estimate column. Our model suggests that players that have a disposal efficiency of zero (none of their disposals result in a good outcome for their team) have, on average, 43.5 Dream Team points. Every one percentage point increase in a player’s efficiency is associated with a gain of 0.12 Dream Team points, on average.\nWe know that these estimates are our best guess of the true linear relationship between Dream Team points and disposal efficiency. Our best guess may be different from the true relationship because of random chance. How confident are we of that best guess? Well, we know from the standard error (std.error) how spread out around that best guess alternative coefficients sit.\nWe know where our estimate (translated into its t-statistic, or statistic) sits within the null world.\nAnd finally, we know the probability (p.value) that we would observe the estimate we found if it were actually equal to zero.\nAll of that work we did above is replicated here, in this one line of code. In fact, we can also add our confidence intervals around our estimate to the broom::tidy() output:\n\ntidy(m, conf.int = T)\n\n# A tibble: 2 × 7\n  term               estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)          43.5      4.43        9.83 1.87e-20  34.8       52.2  \n2 disposalEfficiency    0.116    0.0675      1.72 8.58e- 2  -0.0165     0.249\n\n\nThese are exactly as we manually calculated above.\nWhat would this all look like in a published article?\n\nmodelsummary(m,\n             coef_rename = c(\"disposalEfficiency\" = \"Disposal efficiency\"),\n             statistic = c(\"std.error\"),\n             stars = T)\n\n\n\n    \n\n      \n\n \n                (1)\n              \n+ p \n\n\n(Intercept)\n                  43.535***\n                \n\n\n                  (4.429)\n                \n\nDisposal efficiency\n                  0.116+\n                \n\n\n                  (0.068)\n                \n\nNum.Obs.\n                  379\n                \n\nR2\n                  0.008\n                \n\nR2 Adj.\n                  0.005\n                \n\nAIC\n                  3540.6\n                \n\nBIC\n                  3552.5\n                \n\nLog.Lik.\n                  -1767.321\n                \n\nRMSE\n                  25.64",
    "crumbs": [
      "Content",
      "Session 7",
      "Measuring Uncertainty"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html",
    "href": "content/03-surveys_to_pops.html",
    "title": "From Samples to the Population",
    "section": "",
    "text": "A lot the data we use in political science research are incomplete. Rarely do we have data on all actors in our population of interest. We, therefore, need to work out the degree to which our sample of actors resembles what we would see in the population. This session introduces you to the tools you need to do this.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#introduction",
    "href": "content/03-surveys_to_pops.html#introduction",
    "title": "From Samples to the Population",
    "section": "",
    "text": "A lot the data we use in political science research are incomplete. Rarely do we have data on all actors in our population of interest. We, therefore, need to work out the degree to which our sample of actors resembles what we would see in the population. This session introduces you to the tools you need to do this.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#set-up",
    "href": "content/03-surveys_to_pops.html#set-up",
    "title": "From Samples to the Population",
    "section": "Set up",
    "text": "Set up\nTo complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"wbstats\", \"scales\", \"here\", \"dataverse\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(scales)\nlibrary(here)\nlibrary(dataverse)",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#introduction-1",
    "href": "content/03-surveys_to_pops.html#introduction-1",
    "title": "From Samples to the Population",
    "section": "Introduction",
    "text": "Introduction\nEntire populations are very difficult to collect data on. Even the census (which aims to account for every citizen living within a country) misses people. Happily, we do not need to learn information about the whole population to discover general trends within it. Instead, we can use good surveys of the population.\nWhat do I mean by “good”? The sample of your population surveyed must be representative of that population. This session focuses on defining what we mean by that.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#representative-samples",
    "href": "content/03-surveys_to_pops.html#representative-samples",
    "title": "From Samples to the Population",
    "section": "Representative samples",
    "text": "Representative samples\nSurveys are conducted on a subset of a our population of interest. This is because it is often unfeasible (or impossible) to ask these questions of our population.\nTo be able to infer from our sample information about our population, we need to ensure the sample is representative of that population. In other words, we need to try our best to make sure the sample looks very similar to our population.\nRemember back to the last session on experiments. In an ideal world, we would be able to create two parallel worlds (one with the treatment, one held as our control). These two worlds would be identical to each other prior to treatment. This allows us to confidently state that any differences in our outcome of interest were caused by the treatment. It was, after all, the only thing differentiating those two worlds.\nSadly, however, we have no time machine and cannot produce two parallel worlds. Instead, we needed to create two groups that are as identical to one another as possible prior to treatment. If they are (almost) identical, differences between their group-wide outcomes can be attributed to the treatment.\nOne very good way of getting two (almost) identical groups is to assign individuals to those groups randomly.\n\n\n\n\n\n\nTip\n\n\n\nFor more on why randomization works, please head back to Randomization in Causes and Effects.\n\n\nSimilarly, we can use randomization to pull a sample from our population that looks, on average, identical to that population. Drawing randomly from our population increases our chances of ending up with a sample that reflects that population.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#random-sampling",
    "href": "content/03-surveys_to_pops.html#random-sampling",
    "title": "From Samples to the Population",
    "section": "Random sampling",
    "text": "Random sampling\nTo sample randomly from your population, all individuals in the population need to have an equal chance of being selected for the sample. If this holds, you have a pure random sample.\nHowever, this is really hard to do! For example, think about how likely you are to answer a call from an unknown number. Pollsters rely on people picking up their calls. Imagine you do answer: how likely are you to sit through an interview with a pollster?\nThere are groups of people in the US voting population who are more likely to pick up a call from an unknown number. For example, people with land line phones are much more likely to answer that call (they cannot see it is from an unknown number). Similarly, there are groups of people who are more likely to sit through an interview with a pollster. For example, retired people who have the time to spare during their day. These people are systematically different from the population as a whole. People with land land phones tend to be older, on average, than the general population. Similarly, retired people tend to be older than the general population.\nThis is important because older people tend to be more conservative than the general population as well. Your survey (which will include a disproportionately large group of older people) is likely to overstate how conservative the population as a whole is.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#the-effects-of-an-unrepresentative-sample",
    "href": "content/03-surveys_to_pops.html#the-effects-of-an-unrepresentative-sample",
    "title": "From Samples to the Population",
    "section": "The effects of an unrepresentative sample",
    "text": "The effects of an unrepresentative sample\nLet’s step through this with some actual data. Consider trying to answer the question: what was the average GDP earned by countries globally in 2022?\nTo demonstrate the effects of a biased sample, we are going to download all available GDP values for 2022 using the World Bank’s application programming interface (API). We are going to use the wb_data() function from the wbstats R package to do this. I am going to assign the resulting data set to an object called gdp_df.\n\ngdp_df &lt;- wb_data(\"NY.GDP.MKTP.CD\", return_wide = F, start_date = 2022, end_date = 2022) |&gt; \n  # Select only the relevant data points\n  select(iso3c:value)\n\ngdp_df\n\n# A tibble: 217 × 4\n   iso3c country              date         value\n   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n 1 AFG   Afghanistan          2022  14497243872.\n 2 ALB   Albania              2022  19017242586.\n 3 DZA   Algeria              2022 225638456572.\n 4 ASM   American Samoa       2022    871000000 \n 5 AND   Andorra              2022   3380612573.\n 6 AGO   Angola               2022 104399746853.\n 7 ATG   Antigua and Barbuda  2022   1867733333.\n 8 ARG   Argentina            2022 632790070063.\n 9 ARM   Armenia              2022  19513506553.\n10 ABW   Aruba                2022   3279343544.\n# ℹ 207 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nRun ?wb_data in your console to see what arguments the function takes.\n\n\nWe are going to treat this data set as complete. In other words, we will assume that this data set includes the actual GDP values for all countries globally in 2022. This is cheeky for many reasons. Most obviously, we are missing a whole bunch of data points. Let’s identify those:\n\nfilter(gdp_df, is.na(value))\n\n# A tibble: 11 × 4\n   iso3c country                    date value\n   &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;\n 1 VGB   British Virgin Islands     2022    NA\n 2 CUB   Cuba                       2022    NA\n 3 ERI   Eritrea                    2022    NA\n 4 GIB   Gibraltar                  2022    NA\n 5 GRL   Greenland                  2022    NA\n 6 IMN   Isle of Man                2022    NA\n 7 PRK   Korea, Dem. People's Rep.  2022    NA\n 8 SSD   South Sudan                2022    NA\n 9 MAF   St. Martin (French part)   2022    NA\n10 VEN   Venezuela, RB              2022    NA\n11 YEM   Yemen, Rep.                2022    NA\n\n\n\n\n\n\n\n\nSpoiler alert\n\n\n\nCan you identify anything these missing countries have in common?\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe is.na() function is a logical function (returns either TRUE or FALSE) that asks whether a value is NA, or missing. For example:\n\nis.na(1)\n\n[1] FALSE\n\nis.na(NA)\n\n[1] TRUE\n\n\nWhen supplied to the filter() function, it filters out all the values that are not NA (and are marked as FALSE by the is.na() function).\nTo exclude missing values, you can negate is.na():\n\nfilter(gdp_df, !is.na(value))\n\n# A tibble: 206 × 4\n   iso3c country              date         value\n   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n 1 AFG   Afghanistan          2022  14497243872.\n 2 ALB   Albania              2022  19017242586.\n 3 DZA   Algeria              2022 225638456572.\n 4 ASM   American Samoa       2022    871000000 \n 5 AND   Andorra              2022   3380612573.\n 6 AGO   Angola               2022 104399746853.\n 7 ATG   Antigua and Barbuda  2022   1867733333.\n 8 ARG   Argentina            2022 632790070063.\n 9 ARM   Armenia              2022  19513506553.\n10 ABW   Aruba                2022   3279343544.\n# ℹ 196 more rows\n\n\nAlternatively, you can use the drop_na() function from the tidyr package:\n\ndrop_na(gdp_df, value)\n\n# A tibble: 206 × 4\n   iso3c country              date         value\n   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n 1 AFG   Afghanistan          2022  14497243872.\n 2 ALB   Albania              2022  19017242586.\n 3 DZA   Algeria              2022 225638456572.\n 4 ASM   American Samoa       2022    871000000 \n 5 AND   Andorra              2022   3380612573.\n 6 AGO   Angola               2022 104399746853.\n 7 ATG   Antigua and Barbuda  2022   1867733333.\n 8 ARG   Argentina            2022 632790070063.\n 9 ARM   Armenia              2022  19513506553.\n10 ABW   Aruba                2022   3279343544.\n# ℹ 196 more rows\n\n\n\n\nNonetheless, let’s pretend gdp_df includes the actual GDP values for every country in 2022. I am, therefore, going to update the gdp_df object to exclude those missing values:\n\ngdp_df &lt;- drop_na(gdp_df, value)\n\nNow, imagine I - the researcher who wants to work out the average GDP earned by countries globally in 2022 - do not have access to these data. I need to go out and collect them myself. To do this, I intend to send a request for information to every country’s Department of Statistics, asking them for their country’s GDP in 2022. Annoyingly, however, I can only find contact details for some departments. Specifically, I can only find details for departments with very flashy websites that provide a lot of detail on how to contact teams of people whose whole job is to answer such requests for information.\nLet’s update the gdp_df data set to include which countries I found contact details for. We are going to use some simulation here, which I will explain below.\n\n# Find the GDP value the marks the lowest 25 percent of all GDP values in 2022\nfirst_quartile_gdp &lt;- quantile(gdp_df$value, na.rm = T)[[2]]\n\ngdp_df &lt;- gdp_df |&gt; \n  # Create a variable that simulates my finding the contact details for countries with flashy \n  # websites (i.e. rich countries)\n  rowwise() |&gt; \n  mutate(contact_details_found = if_else(value &lt; first_quartile_gdp, \n                                         rbinom(1, 1, 0.10), \n                                         rbinom(1, 1, 0.75))) |&gt; \n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\nI am doing a fair bit mechanically here that could distract from my broader point about biased sampling. If, starting to read this note, you find yourself getting a bit lost, please just move on. We will come back to these concepts later.\nJumping ahead to the end, richer countries are more likely than poorer ones to have well-funded, large departments of statistics. To simulate this, I first need a definition of rich countries. I took the 25th percentile as my cut off point. The 25th percentile is the value below which 25 percent of all of the values in a series (ordered from smallest to largest) sit. So, the 25th percentile of my 2022 GDP values is the GDP value below which 25 percent of all other GDP values sit.\nRicher countries are more likely to have departments with accessible teams of people able to field my request for information. This does not mean that they definitely do, nor does it mean no poorer countries do. To account for this, I determine randomly whether a country has contact details based on its GDP value. For poorer countries (those in the lowest 25 percent of all countries), I give them a 10 percent chance I find their contact details. For richer countries (those in the highest 75 percent of all countries), I give them a 75 percent chance I find their contact details. I use the rbinom() function to do this. We will talk about this probability distribution function later in the course.\nI have now simulated data that reflects the constraints I would face as a researcher trying to answer my question.\n\n\nUndeterred, I send out my requests to those departments I have contact details for. I receive some responses:\n\ngdp_responses_df &lt;- gdp_df |&gt; \n  # Only sample from the countries I contacted\n  filter(contact_details_found == 1) |&gt; \n  # Simulate the uneven likelihood I receive responses back (based on wealth)\n  mutate(response_received = if_else(value &lt; first_quartile_gdp,\n                                     rbinom(1, 1, 0.5),\n                                     rbinom(1, 1, 0.8))) |&gt;\n  filter(response_received == 1) |&gt; \n  select(iso3c:value)\n\ngdp_responses_df\n\n# A tibble: 116 × 4\n   iso3c country      date   value\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 AFG   Afghanistan  2022 1.45e10\n 2 ALB   Albania      2022 1.90e10\n 3 DZA   Algeria      2022 2.26e11\n 4 AGO   Angola       2022 1.04e11\n 5 ARM   Armenia      2022 1.95e10\n 6 AUS   Australia    2022 1.69e12\n 7 AUT   Austria      2022 4.72e11\n 8 BHR   Bahrain      2022 4.67e10\n 9 BGD   Bangladesh   2022 4.60e11\n10 BEL   Belgium      2022 5.93e11\n# ℹ 106 more rows\n\n\nNow, if I use this sample to learn something about the average GDP in 2022 for all countries globally, I am going to be wrong.\n\n\n\n\n\n\nQuestion\n\n\n\nCan you guess which way I will be wrong? Will I overstate or understate the global average GDP?\n\n\nHere is the average GDP among my sample:\n\ngdp_responses_df |&gt; \n  summarise(avg_gdp = mean(value)) |&gt; \n  pull() |&gt; \n  dollar()\n\n[1] \"$506,556,083,940\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe scales package includes handy functions for formatting numbers. I have used its dollar() function to format the average GDP nicely.\n\n\nAnd here is the average GDP among the population:\n\ngdp_df |&gt; \n  summarise(avg_gdp = mean(value)) |&gt; \n  pull() |&gt; \n  dollar()\n\n[1] \"$487,688,575,556\"\n\n\nI am overstating the average by a lot! Why? Well, countries that have departments of statistics that are so well funded they can splash out for a fancy website and have a team of people dedicated to fielding requests for information from random academics tend to be wealthy. Poorer countries are less able to spare the funds required to meet my request. I am, therefore, less likely to have found a way to contact them and, even if I did, I am less likely to have received a response from them.\nThis is an example of sampling bias: my sample is systematically different from my population in ways important to my analysis. Wealth is what I am trying to measure, and wealth is influencing who gets into my sample and who does not. Because I do not have a representative sample, I cannot infer from that sample anything about my population.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#largeish-numbers",
    "href": "content/03-surveys_to_pops.html#largeish-numbers",
    "title": "From Samples to the Population",
    "section": "Large(ish) numbers",
    "text": "Large(ish) numbers\nIt is not sufficient to have a pure random sample of your population. You also need a sufficiently large sample. To illustrate, consider drawing just one person from your population. Even if you did so entirely randomly, it is unlikely they will reflect the diversity (or even the average view) of the population.\nLet’s return to our attempt to find the average GDP earned by countries globally in 2022 to illustrate this. I am going to select completely randomly five countries from the (cheeky) population using dplyr’s sample_n() function:\n\ngdp_5_df &lt;- sample_n(gdp_df, size = 5)\ngdp_5_df\n\n# A tibble: 5 × 5\n  iso3c country              date   value contact_details_found\n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;                 &lt;int&gt;\n1 TTO   Trinidad and Tobago  2022 3.01e10                     1\n2 IDN   Indonesia            2022 1.32e12                     1\n3 XKX   Kosovo               2022 9.35e 9                     1\n4 MDV   Maldives             2022 6.18e 9                     0\n5 MCO   Monaco               2022 8.80e 9                     1\n\n\nThe average GDP among this very small but pure random sample is $274,692,433,009, which is $212,996,142,547 dollars away from the population average.\nWe can do this again to see that it wasn’t just a fluke:\n\ngdp_5_df &lt;- sample_n(gdp_df, size = 5)\ngdp_5_df\n\n# A tibble: 5 × 5\n  iso3c country         date        value contact_details_found\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;                 &lt;int&gt;\n1 KIR   Kiribati        2022   270841698.                     0\n2 CUW   Curacao         2022  3075180835.                     0\n3 LCA   St. Lucia       2022  2342703704.                     1\n4 CIV   Cote d'Ivoire   2022 70173140101.                     1\n5 ASM   American Samoa  2022   871000000                      0\n\n\nThe average GDP among this small pure random sample is $15,346,573,268, a $472,342,002,288 dollar difference from the population average.\nIf we increase our sample size, we get closer to the population average:\n\ngdp_175_df &lt;- sample_n(gdp_df, size = 175)\ngdp_175_df\n\n# A tibble: 175 × 5\n   iso3c country                         date   value contact_details_found\n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;                 &lt;int&gt;\n 1 TTO   Trinidad and Tobago             2022 3.01e10                     1\n 2 OMN   Oman                            2022 1.12e11                     0\n 3 KNA   St. Kitts and Nevis             2022 9.81e 8                     0\n 4 PAN   Panama                          2022 7.63e10                     0\n 5 COL   Colombia                        2022 3.45e11                     1\n 6 VCT   St. Vincent and the Grenadines  2022 9.66e 8                     0\n 7 TZA   Tanzania                        2022 7.58e10                     0\n 8 ITA   Italy                           2022 2.10e12                     1\n 9 MHL   Marshall Islands                2022 2.53e 8                     0\n10 GRD   Grenada                         2022 1.22e 9                     0\n# ℹ 165 more rows\n\n\nThe average GDP among this small pure random sample is $546,787,923,128, a mere $59,099,347,572 dollar difference from the population average.\nThe larger your sample size, the closer you will get to the population average. You increase the likelihood you capture the diversity of the population as your sample size increases. This is referred to as sampling variability.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/03-surveys_to_pops.html#us-presidential-elections",
    "href": "content/03-surveys_to_pops.html#us-presidential-elections",
    "title": "From Samples to the Population",
    "section": "US Presidential Elections",
    "text": "US Presidential Elections\nUS Presidential elections involve thousands of surveys of the US population. Pollsters ask a small subset of that population questions about the election (including which candidate they intend to vote for). They hope to learn from the answers provided by that sample the views of the broader population.\nWe will look at surveys run in the 2020 US Presidential Election. We will learn about overall support for the two candidates (Joe Biden and Donald Trump), and how this support shifted across different demographics. We will then compare that survey to the end result to learn how accurately the survey reflected the population’s views.\nA survey of votes cast\nWe are going to look at the American National Election Studies pre-election survey. The ANES conducts surveys on voting, public opinion, and political participation. You can learn more about them from their website.\nTo download the data, you will need to head over to the ANES website, download the relevant file, and save it in your RProject. The following video demonstrates how to do this.\n\n\nOnce you have collected the data, you can read it in using the readr package’s read_csv() function. It takes the file’s path as its first argument.\n\nanes_raw &lt;- read_csv(here(\"content\", \"data\", \"anes_timeseries_2020_csv_20220210.csv\"))\nanes_raw\n\n# A tibble: 8,280 × 1,771\n   version  V200001 V160001_orig V200002 V200003 V200004 V200005 V200006 V200007\n   &lt;chr&gt;      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 ANES202…  200015       401318       3       2       3       0      -2      -2\n 2 ANES202…  200022       300261       3       2       3       0       4      -1\n 3 ANES202…  200039       400181       3       2       3       0      -2      -2\n 4 ANES202…  200046       300171       3       2       3       0      -2      -2\n 5 ANES202…  200053       405145       3       2       3       1      -2      -2\n 6 ANES202…  200060       400374       3       2       3       0      -2      -2\n 7 ANES202…  200084       407013       3       2       3       0      -2      -2\n 8 ANES202…  200091       407174       3       2       3       0      -2      -2\n 9 ANES202…  200107       406264       3       2       3       0      -2      -2\n10 ANES202…  200114       402782       3       2       3       1       4      -1\n# ℹ 8,270 more rows\n# ℹ 1,762 more variables: V200008 &lt;dbl&gt;, V200009 &lt;dbl&gt;, V200010a &lt;dbl&gt;,\n#   V200010b &lt;dbl&gt;, V200010c &lt;dbl&gt;, V200010d &lt;dbl&gt;, V200011a &lt;dbl&gt;,\n#   V200011b &lt;dbl&gt;, V200011c &lt;dbl&gt;, V200011d &lt;dbl&gt;, V200012a &lt;dbl&gt;,\n#   V200012b &lt;dbl&gt;, V200012c &lt;dbl&gt;, V200012d &lt;dbl&gt;, V200013a &lt;dbl&gt;,\n#   V200013b &lt;dbl&gt;, V200013c &lt;dbl&gt;, V200013d &lt;dbl&gt;, V200014a &lt;dbl&gt;,\n#   V200014b &lt;dbl&gt;, V200014c &lt;dbl&gt;, V200014d &lt;dbl&gt;, V200015a &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe here R package makes saving data in a robust way very easy. Using here::here(), you can automatically update file paths to reflect the computer you are currently using. To demonstrate, run the following code:\n\nhere::here()\n\n[1] \"/Users/harrietgoers/Documents/GVPT201\"\n\n\nYour output will be different to mine. here::here() locates the top level of the file directory in which you are located. This is helpful when you share your code with others (or switch to a new computer). Hard coded file paths will cause annoying errors.\nI like to save my data in a folder called data. You can use a structure you find works for you.\n\n\nEach row of this data set represents a single respondent. Therefore, we can quickly note that in 2020, the ANES surveyed 8,280 people.\nThe ANES asks a lot of questions of its respondents. Full details of these questions can be found in their survey questionnaire document. We are interested in learning whether the survey respondents reflect the behavior of all US voters in the 2020 US Presidential election. To answer this question, we are going to focus on their response to the question: which Presidential candidate did you vote for?\nThis question was only asked of respondents who had already told the interviewer that they voted for a US president. Not all respondents voted, so we need to remove those who did not from our data set.\nWe will then match these data to the total vote count each candidate won in the election to see if they are similar.\nLet’s start by transforming our data set into one that is more suitable for this analysis:\n\nanes_df &lt;- anes_raw |&gt; \n  select(respondent_id = V200001,\n         voted_for_pres = V202072,\n         pres_vote = V202073)\n\nanes_df\n\n# A tibble: 8,280 × 3\n   respondent_id voted_for_pres pres_vote\n           &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1        200015             -1        -1\n 2        200022              1         3\n 3        200039              1         1\n 4        200046              1         1\n 5        200053              1         2\n 6        200060              1         1\n 7        200084              1         2\n 8        200091             -1        -1\n 9        200107             -1        -1\n10        200114              1         1\n# ℹ 8,270 more rows\n\n\nIndividuals’ responses are coded as numbers, which are easily interpreted by the computer, but not by us humans. Let’s recode the responses so we know what they are. They are categorical variables (which R refers to as factors). Currently, they are coded as numbers (&lt;dbl&gt;). To convert them to factors, we need to use factor() within mutate():\n\nanes_df &lt;- anes_df |&gt; \n  mutate(voted_for_pres = factor(voted_for_pres, levels = 1:2, \n                                 labels = c(\"Yes, voted for President\",\n                                            \"No, didn’t vote for President\")),\n         pres_vote = factor(pres_vote, levels = 1:5,\n                            labels = c(\"DEMOCRAT\",\n                                       \"REPUBLICAN\",\n                                       \"LIBERTARIAN\",\n                                       \"GREENS\",\n                                       \"OTHER\")))\n\nanes_df\n\n# A tibble: 8,280 × 3\n   respondent_id voted_for_pres           pres_vote  \n           &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;      \n 1        200015 &lt;NA&gt;                     &lt;NA&gt;       \n 2        200022 Yes, voted for President LIBERTARIAN\n 3        200039 Yes, voted for President DEMOCRAT   \n 4        200046 Yes, voted for President DEMOCRAT   \n 5        200053 Yes, voted for President REPUBLICAN \n 6        200060 Yes, voted for President DEMOCRAT   \n 7        200084 Yes, voted for President REPUBLICAN \n 8        200091 &lt;NA&gt;                     &lt;NA&gt;       \n 9        200107 &lt;NA&gt;                     &lt;NA&gt;       \n10        200114 Yes, voted for President DEMOCRAT   \n# ℹ 8,270 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nI got the response labels from the survey questionnaire linked above.\n\n\n\n\n\n\n\n\nTip\n\n\n\nfactor() takes as its first argument the name of the column you would like to convert to a factor. You then specify which numbers you need to code as your values using the levels argument. Finally, you supply the more human-friendly name of those values in the labels argument.\n\n\nI now have a much cleaner version of my data set. Next, we need to exclude those respondents who did not vote for any Presidential candidate or who did not respond to the interviewer’s question about which candidate they voted for:\n\npres_voted_df &lt;- anes_df |&gt; \n  filter(voted_for_pres == \"Yes, voted for President\") |&gt; \n  drop_na(pres_vote)\n\npres_voted_df\n\n# A tibble: 5,877 × 3\n   respondent_id voted_for_pres           pres_vote  \n           &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;      \n 1        200022 Yes, voted for President LIBERTARIAN\n 2        200039 Yes, voted for President DEMOCRAT   \n 3        200046 Yes, voted for President DEMOCRAT   \n 4        200053 Yes, voted for President REPUBLICAN \n 5        200060 Yes, voted for President DEMOCRAT   \n 6        200084 Yes, voted for President REPUBLICAN \n 7        200114 Yes, voted for President DEMOCRAT   \n 8        200121 Yes, voted for President DEMOCRAT   \n 9        200138 Yes, voted for President DEMOCRAT   \n10        200152 Yes, voted for President REPUBLICAN \n# ℹ 5,867 more rows\n\n\nWe now have data on 5,877 individuals who voted for a Presidential candidate and provided their vote to the interviewer. Let’s take a look at the total number of votes each candidate received:\n\npres_voted_df |&gt; \n  count(pres_vote) |&gt; \n  ggplot(aes(x = n, y = reorder(pres_vote, n))) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"Votes received\",\n       y = NULL)\n\n\n\n\n\n\n\nJoe Biden (the Democratic candidate) received the most votes among the ANES survey respondents. Let’s represent this as vote proportions so we can more easily compare it to the votes received by each candidate nationally:\n\npres_voted_df |&gt; \n  count(pres_vote) |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  ggplot(aes(x = prop, y = reorder(pres_vote, prop))) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"Precentage of total votes\",\n       y = NULL) + \n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\nThe population of votes cast\nSo, were they right? To answer this question, we need to get data on the total number of votes each Presidential candidate received in 2020. The MIT Election Data + Science Lab provides these returns to the public. To access their data, we will use the Harvard Dataverse API. We will access it using the R package dataverse.\n\n\n\n\n\n\nNote\n\n\n\nThe dataverse package accesses the Harvard Dataverse Application Programming Interface (API) in the background. To learn more about how to access this API directly, check out the Harvard Dataverse documentation.\n\n\nWe will be accessing the U.S. President 1976–2020 data set. To do this, we need three pieces of information:\n\nThe name of the file we want to download\nThe data set’s DOI\nThe data set’s format\n\nWe can get all of this information from the data set’s page on the Harvard Dataverse website.\n\npres_results_df &lt;- get_dataframe_by_name(\n  filename = \"1976-2020-president.tab\",\n  dataset = \"10.7910/DVN/42MVDX\",\n  server = \"dataverse.harvard.edu\",\n  original = T,\n  .f = readr::read_csv\n)\n\nThis code programmatically pulls the most up-to-date data set from the API. Once the MIT Election Lab publishes the 2024 Presidential Election data, you will only need to update the file name to be able to access it.\nWe are interested in looking at the 2020 Presidential Election, so we will transform our data to exclude all others. Also, these data are at the state-level. We need to aggregate them up to the national level.\nWe do both of these things in the following code chunk:\n\npres_results_2020_df &lt;- pres_results_df |&gt; \n  filter(year == 2020) |&gt; \n  group_by(party_simplified) |&gt; \n  summarise(candidatevotes = sum(candidatevotes)) |&gt; \n  ungroup()\n\npres_results_2020_df\n\n# A tibble: 4 × 2\n  party_simplified candidatevotes\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 DEMOCRAT               81268908\n2 LIBERTARIAN             1797355\n3 OTHER                   1246094\n4 REPUBLICAN             74216146\n\n\nFinally, we will calculate the proportion of votes each candidate received to make it comparable to the survey results:\n\npres_results_national_df &lt;- pres_results_2020_df |&gt; \n  mutate(prop_pop = candidatevotes / sum(candidatevotes))\n\nggplot(pres_results_national_df, aes(x = prop_pop, y = reorder(party_simplified, prop_pop))) + \n  geom_col() +\n  theme_minimal() + \n  labs(x = \"Percentage of total votes\",\n       y = NULL) + \n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\nThe MIT Election Lab folds the Greens party into the “other” category, whereas the ANES separates them out. To make these comparable, we need to fold the Greens votes into the “other” category. We can do this using the forcats package’s (loaded in with the tidyverse) fct_recode() function:\n\nanes_df &lt;- mutate(anes_df, pres_vote = fct_recode(pres_vote, OTHER = \"GREENS\"))\n\nanes_national_results &lt;- anes_df |&gt; \n  filter(voted_for_pres == \"Yes, voted for President\" & !is.na(pres_vote)) |&gt; \n  count(pres_vote) |&gt; \n  transmute(party_simplified = pres_vote, prop_survey = n / sum(n))\n\nNow we can compare these two:\n\npres_results_national_df |&gt; \n  left_join(anes_national_results, by = join_by(party_simplified)) |&gt; \n  pivot_longer(prop_pop:prop_survey) |&gt; \n  mutate(name = case_when(name == \"prop_pop\" ~ \"Population\",\n                          name == \"prop_survey\" ~ \"Survey\")) |&gt; \n  ggplot(aes(x = value, y = reorder(party_simplified, value), fill = name)) + \n  geom_col(position = \"dodge\") + \n  theme_minimal() + \n  labs(x = \"Percentage of total votes\",\n       y = NULL, \n       fill = NULL) + \n  scale_x_continuous(labels = percent)\n\n\n\n\n\n\n\nIt looks like the ANES did a fairly good job of capturing the US voting population’s behavior. The proportion of survey respondents who said they voted for each candidate (represented by the blue bar) is roughly equal to the proportion of votes won by each candidate (represented by the pink bar).\nOne interesting thing to note is that the survey did overstate support for Joe Biden and understated support for Donald Trump. We might see this despite having a representative sample. For example, people - when asked who they voted for - may be inclined to say they voted for the winner (regardless of who that winner was). Also, individuals may be hesitant to tell a pollster that they voted for Donald Trump given how divisive he and his campaign were.\nIt is; however, very likely that the ANES did not reach a representative sample of the US voting population. As we discussed above: this is very difficult (if not impossible) to do. ANES runs very good surveys precisely because they acknowledge and, to the best of their ability, account for this. Generally, surveys will compare the demographics of their respondents to those of the voting population (or population more generally). If they find that some individuals are under-represented in their sample (for example, young black women make up a smaller proportion of their sample than they make up in the population), they will make the responses of individuals who fit those demographics count for more than one response in their calculations. Similarly, if they find a demographic is over-represented in their sample, they will make their responses count for less than one response in their calculations. This practice is called weighting and it is standard across all major and respected surveys.",
    "crumbs": [
      "Content",
      "Session 3",
      "From Samples to the Population"
    ]
  },
  {
    "objectID": "content/02-experiments.html",
    "href": "content/02-experiments.html",
    "title": "Causes and Effects",
    "section": "",
    "text": "We often want to better understand what factors lead to (or cause) certain outcomes of interest. This session explains how we might go about identifying these causal effects in the real-world. Our goal as political scientists is to explain whether (and why) changes to some variables lead to changes in our outcomes of interest. For example:\n\nDoes increasing the number of voting booths close to a potential voter make that person more likely to vote?\nDo peace treaties signed with factionalized rebel groups more often lead to a return to conflict than those signed with a single, cohesive group?\nDoes trade between two countries make war between them less likely?\n\nThe questions can be reframed as causal statements that can then be tested:\n\nMore local voting booths lead to an increased likelihood individuals vote.\nMore factionalization among rebel groups lead to a higher likelihood the conflict will restart.\nMore trade between two countries leads to a lower likelihood that war will break out between them.\n\nHowever, proving that changes to one variable (more local voting booths, factionalization, or trade) caused changes to our outcome of interest is very difficult to do. This session, we are going to focus on experiments, which are often called the “gold-standard” of causal research. To guide us through this session, we are going to look at research conducted by Professor Susan Hyde.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#introduction",
    "href": "content/02-experiments.html#introduction",
    "title": "Causes and Effects",
    "section": "",
    "text": "We often want to better understand what factors lead to (or cause) certain outcomes of interest. This session explains how we might go about identifying these causal effects in the real-world. Our goal as political scientists is to explain whether (and why) changes to some variables lead to changes in our outcomes of interest. For example:\n\nDoes increasing the number of voting booths close to a potential voter make that person more likely to vote?\nDo peace treaties signed with factionalized rebel groups more often lead to a return to conflict than those signed with a single, cohesive group?\nDoes trade between two countries make war between them less likely?\n\nThe questions can be reframed as causal statements that can then be tested:\n\nMore local voting booths lead to an increased likelihood individuals vote.\nMore factionalization among rebel groups lead to a higher likelihood the conflict will restart.\nMore trade between two countries leads to a lower likelihood that war will break out between them.\n\nHowever, proving that changes to one variable (more local voting booths, factionalization, or trade) caused changes to our outcome of interest is very difficult to do. This session, we are going to focus on experiments, which are often called the “gold-standard” of causal research. To guide us through this session, we are going to look at research conducted by Professor Susan Hyde.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#set-up",
    "href": "content/02-experiments.html#set-up",
    "title": "Causes and Effects",
    "section": "Set up",
    "text": "Set up\nTo complete this session, you need to load in the following R packages:\n\n\n\n\n\n\nInstall packages\n\n\n\n\n\nTo install new R packages, run the following (excluding the packages you have already installed):\n\ninstall.packages(c(\"tidyverse\", \"DT\", \"patchwork\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(patchwork)\n\n\n\n\n\n\n\nWarning\n\n\n\nSome of the R code used in this and future sessions will currently be a bit advanced for you if you don’t have a background in R. You don’t need to be able to follow the code to successfully complete this session. I have left it here so you can return to it when you are further along in the course and are more comfortable with R.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#do-election-monitors-reduce-election-day-fraud",
    "href": "content/02-experiments.html#do-election-monitors-reduce-election-day-fraud",
    "title": "Causes and Effects",
    "section": "Do election monitors reduce election-day fraud?",
    "text": "Do election monitors reduce election-day fraud?\nHyde wanted to explore whether the international community’s tools for combating election fraud were effective. Specifically, she wanted to learn whether international election monitors effectively reduced or quashed election-day fraud.\nThis is a tricky question to research. Fraud is often difficult to observe. Those committing it tend not to want even curious academics to catch them in the act. To get around this problem, Hyde had to be clever. She worked out that although fraud itself is very difficult to see, its outcome is not. (Effective) cheating parties receive a greater share of the vote than they would have had they not cheated.\nIn her own words (Hyde 2007, 39):\n\nIf the presence of international observers causes a reduction in election-day fraud, the effect of observers should be visible at the subnational level by comparing polling stations that were visited by observers with those that were not visited. More specifically, if international monitoring reduces election day fraud directly, all else held equal, the cheating parties should gain less of their ill-gotten vote share in polling stations that were visited by international monitors.\n\nSo, Hyde offers the following sequence of events:\n\nInternational election monitors cause…\nLess fraud to be committed on election day at the booths at which they are stationed (which we cannot observe directly), which causes …\nThe cheating party to win a lower share of the vote than they would otherwise have won (which we can observe directly, kind of… stay tuned).",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#causal-relationships",
    "href": "content/02-experiments.html#causal-relationships",
    "title": "Causes and Effects",
    "section": "Causal relationships",
    "text": "Causal relationships\nHyde is interested in the causal relationship between international election monitors and election-day fraud. Formally, a causal relationship refers to the directional connection between a change in one variable and a corresponding change in another.\nFor causal relationships, direction really matters. You need to prove the following sequence of events:\n\nA change happened to some variable, then:\nA change happened to your outcome of interest\n\nLet’s label these factors to avoid confusion. The treatment variable is the variable causing changes to another variable. It’s the one that moves first. The outcome variable is the variable changing as a result of changes to another variable (the treatment). It’s the one that moves second. Hyde wanted to test whether international monitors stationed at election booths (the treatment) leads to less election-day fraud at those booths (the outcome).\nFocus for a moment on the treatment variable. At any given polling station in any given election, monitors may be: 1) present, or 2) not present. We, therefore, have two conditions:\n\nTreatment: monitors are present\nControl: monitors are not present\n\nBoth conditions are critical to experiments. You need to be able to compare your outcome of interest under treatment and control to determine the effect of the treatment. We will step through how to do this now.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#individual-causal-effects",
    "href": "content/02-experiments.html#individual-causal-effects",
    "title": "Causes and Effects",
    "section": "Individual causal effects",
    "text": "Individual causal effects\nWe want to know whether the treatment causes outcome of interest to change. To do this, we want to compare the vote share the cheating party received at the voting booth with election monitors present (under treatment) to that it received at the same voting booth without election monitors present (under control). Huh…\nLet’s step through this with a concrete example. Imagine that we are looking at a specific election held within a new democracy. There are 10 election booths set up for the election.\nIn an ideal world, we would run the election with no monitors and record the vote share each party received at each booth. We would then jump in our very handy time machine and go back to the start of the election. We would station monitors at every booth, run the election, and record the vote share each party received.\nWe could then directly compare the vote share each party received at each booth with and without monitors (in treatment and control). If Hyde’s hypothesis is correct, we should see that the cheating party receives a lower vote share in the timeline with election monitors than it does in the timeline without them.\nFor example, imagine that the following results were recorded for the cheating party at each of the 1,000 booths in both timelines:\n\nCoden &lt;- 1000\n\nindiv_effect_df &lt;- tibble(\n  polling_station_id = 1:n,\n  vote_share_monitored = rbeta(n, 4, 6),\n  vote_share_not_monitored = rbeta(n, 11, 2),\n  difference = vote_share_monitored - vote_share_not_monitored\n)\n\nindiv_effect_df |&gt; \n  mutate(across(vote_share_monitored:vote_share_not_monitored, \n                ~ scales::percent(.x, accuracy = 0.1))) |&gt; \n  select(polling_station_id:vote_share_not_monitored) |&gt; \n  rename(ID = polling_station_id,\n         `Monitored vote %` = vote_share_monitored,\n         `Non-monitored vote %` = vote_share_not_monitored) |&gt; \n  datatable(rownames = F, options = list(pageLength = 10, dom = 'tip'))\n\n\n\n\n\n\nBecause the only difference between these two versions of the election was the presence of election monitors, we can definitively state that the difference in vote shares won by each party was caused by the monitors. Let’s calculate those differences:\n\nCodeindiv_effect_df |&gt; \n  mutate(across(vote_share_monitored:difference, \n                ~ scales::percent(.x, accuracy = 0.1))) |&gt; \n  rename(ID = polling_station_id,\n         `Monitored vote %` = vote_share_monitored,\n         `Non-monitored vote %` = vote_share_not_monitored,\n         Difference = difference) |&gt; \n  datatable(rownames = F, options = list(pageLength = 10, dom = 'tip'))\n\n\n\n\n\n\nIn this hypothetical election, these differences are substantial! They are often the difference between a decisive victory and an embarrassing defeat.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#average-causal-effects",
    "href": "content/02-experiments.html#average-causal-effects",
    "title": "Causes and Effects",
    "section": "Average causal effects",
    "text": "Average causal effects\nSadly for us, however, we are yet to invent time machines. We cannot observe both the treatment and control for each individual booth. Rather what we see is the following:\n\nCodefactual_indiv_df &lt;- indiv_effect_df |&gt; \n  mutate(monitored = rbinom(n, 1, 0.5),\n         vote_share_monitored = if_else(monitored == 1, vote_share_monitored, NA_real_),\n         vote_share_not_monitored = if_else(monitored == 0, vote_share_not_monitored, NA_real_),\n         difference = vote_share_monitored - vote_share_not_monitored,\n         monitored = factor(monitored, labels = c(\"No\", \"Yes\"))) |&gt; \n  relocate(monitored, .after = polling_station_id)\n\nfactual_indiv_df |&gt; \n  mutate(across(vote_share_monitored:difference, \n                ~ scales::percent(.x, accuracy = 0.1))) |&gt; \n  rename(ID = polling_station_id,\n         Monitored = monitored,\n         `Monitored vote %` = vote_share_monitored,\n         `Non-monitored vote %` = vote_share_not_monitored,\n         Difference = difference) |&gt; \n  datatable(rownames = F, options = list(pageLength = 10, dom = 'tip'))\n\n\n\n\n\n\nWe are essentially missing data for the counter-factual for each booth. We cannot calculate the difference and identify the causal effect of election monitors for each individual booth. So, now what?\nWe need to move away from looking at individual election booths and start to look for patterns across all election booths that were monitored and those that were not. Let’s return to our two timelines. What was the difference between the vote share won by the cheating party with and without election monitors on average across all booths?\n\nCodeindiv_avg_df &lt;- indiv_effect_df |&gt; \n  summarise(vote_share_monitored = mean(vote_share_monitored),\n            vote_share_not_monitored = mean(vote_share_not_monitored),\n            difference = mean(difference)) |&gt; \n  mutate(across(everything(), ~ scales::percent(.x, accuracy = 0.1)))\n\nindiv_avg_df |&gt; \n  rename(`Avg. monitored vote %` = vote_share_monitored,\n         `Avg. non-monitored vote %` = vote_share_not_monitored,\n         Difference = difference) |&gt; \n  knitr::kable()\n\n\n\nAvg. monitored vote %\nAvg. non-monitored vote %\nDifference\n\n\n39.5%\n84.3%\n-44.8%\n\n\n\n\n\nOkay, and what is the difference, on average, in our real world?\n\nCodefactual_avg_df &lt;- factual_indiv_df |&gt; \n  summarise(vote_share_monitored = mean(vote_share_monitored, na.rm = T),\n            vote_share_not_monitored = mean(vote_share_not_monitored, na.rm = T),\n            difference = vote_share_monitored - vote_share_not_monitored) |&gt;    mutate(across(everything(), ~ scales::percent(.x, accuracy = 0.1)))\n\nfactual_avg_df |&gt; \n  rename(`Avg. monitored vote %` = vote_share_monitored,\n         `Avg. non-monitored vote %` = vote_share_not_monitored,\n         Difference = difference) |&gt; \n  knitr::kable()\n\n\n\nAvg. monitored vote %\nAvg. non-monitored vote %\nDifference\n\n\n40.2%\n84.5%\n-44.3%\n\n\n\n\n\nThe difference between the average with missing counter-factuals and that with full information (that relies on that handy time machine) is very small: only 0.5%. How does this work so well?\n\n\n\n\n\n\nNote\n\n\n\nThe difference between the real-world average and full-information average is, in fact, not statistically significant (this is good!). We will talk about this towards the end of the course.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#randomization",
    "href": "content/02-experiments.html#randomization",
    "title": "Causes and Effects",
    "section": "Randomization",
    "text": "Randomization\nThrough randomization! I assigned monitors to the 1,000 voting booths randomly. For each booth, I flipped a (R generated) coin to decide whether that booth would be monitored. At the end of that process, roughly half of the booths had monitors and half did not:\n\nCodefactual_indiv_df |&gt; \n  count(monitored) |&gt; \n  rename(Monitored = monitored,\n         `No. of booths` = n) |&gt; \n  knitr::kable()\n\n\n\nMonitored\nNo. of booths\n\n\n\nNo\n502\n\n\nYes\n498\n\n\n\n\n\nThe magic trick with random assignment is that you tend to end up with two groups that are on average roughly identical to one another prior to treatment.\nRemember, our goal is to create two groups (treatment and control) that are identical to one another prior to treatment. If the only difference between the groups is the treatment, we can say that any differences in our outcome of interest is caused by the treatment. Absent a time machine, we then need to set about creating two groups that are as identical to each other as possible. It turns out that random assignment does a very good job of achieving this.\n\n\n\n\n\n\nNote\n\n\n\nPractitioners have come up with other clever ways of doing this, including pairwise matching. We will not cover those in this course.\n\n\nYou don’t need to take my word for this. Let’s prove it with simulation! Imagine we have a group of 1,000 individuals. We know the following about them: their height, weight, and eye colour.\nHere are those data:\n\nCodegroup_df &lt;- tibble(\n  id = 1:1000,\n  height = rnorm(1000, 170, 6),\n  weight = rnorm(1000, 80, 10),\n  eye_colour = sample(c(\"Blue\", \"Green\", \"Brown\", \"Grey\"), 1000, replace = T)\n) \n\ngroup_df |&gt; \n  mutate(across(height:weight, ~ round(.x, 2))) |&gt; \n  rename(ID = id,\n         Height = height, \n         Weight = weight, \n         `Eye colour` = eye_colour) |&gt; \n  datatable(rownames = F, options = list(pageLength = 10, dom = 'tip'))\n\n\n\n\n\n\nI’m now going to flip (an imaginary, R-generated) coin for each of these 1,000 individuals to assigned them to either group A or B:\n\nCoderand_group &lt;- group_df |&gt; \n  mutate(\n    group = rbinom(1000, 1, 0.5),\n    group = factor(group, labels = c(\"A\", \"B\"))\n  )\n\nrand_group |&gt; \n  mutate(across(height:weight, ~ round(.x, 2))) |&gt; \n  rename(ID = id,\n         Height = height, \n         Weight = weight, \n         `Eye colour` = eye_colour, \n         Group = group) |&gt; \n  datatable(rownames = F, options = list(pageLength = 10, dom = 'tip'))\n\n\n\n\n\n\nNow we can check how similar these two groups are to one another. Let’s start with their heights:\n\nCodeggplot(rand_group, aes(x = height, fill = group)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() + \n  labs(x = \"Height (cm)\",\n       y = \"Density\",\n       fill = \"Group\")\n\n\n\n\n\n\n\nThe distribution of heights among individuals in groups A and B are roughly identical. The average height of individuals in group A is 170.1 cm and in group B is 170 cm. Pretty neat!\nLet’s check their weight:\n\nCodeggplot(rand_group, aes(x = weight, fill = group)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() + \n  labs(x = \"Weight (kg)\",\n       y = \"Density\",\n       fill = \"Group\")\n\n\n\n\n\n\n\nSimilarly, the distribution of weights among individuals in groups A and B are roughly identical. On average, individuals in group A weigh 79.9 kg. Individuals in group B weigh 79.6 kg, on average.\nFinally, let’s look at eye colour:\n\nCoderand_group |&gt; \n  count(group, eye_colour) |&gt; \n  group_by(group) |&gt;\n  mutate(prop = n / sum(n)) |&gt; \n  ggplot(aes(x = prop, y = reorder(eye_colour, n), fill = group)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  labs(x = \"Percent of individuals\",\n       y = \"Eye color\",\n       fill = \"Group\") + \n  scale_x_continuous(labels = scales::percent)\n\n\n\n\n\n\n\nAgain, the proportion of individuals within each group with each eye colour are roughly identical. This is all due to random assignment.\nLet’s repeat this process, assigning our 1,000 individuals randomly to each group, and make sure this wasn’t a fluke:\n\nCoderand_group &lt;- group_df |&gt; \n  mutate(\n    group = rbinom(1000, 1, 0.5),\n    group = factor(group, labels = c(\"A\", \"B\"))\n  )\n\nrand_group |&gt; \n  mutate(across(height:weight, ~ round(.x, 2))) |&gt; \n  rename(ID = id,\n         Height = height, \n         Weight = weight, \n         `Eye colour` = eye_colour, \n         Group = group) |&gt; \n  datatable(rownames = F, options = list(pageLength = 10, dom = 'tip'))\n\n\n\n\n\n\nAnd let’s see how similar these new, randomly-assigned groups are to each other:\n\nCodep1 &lt;- ggplot(rand_group, aes(x = height, fill = group)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(x = \"Height (cm)\",\n       y = \"Density\",\n       fill = \"Group\")\n\np2 &lt;- ggplot(rand_group, aes(x = weight, fill = group)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(x = \"Weight (kg)\",\n       y = \"Density\",\n       fill = \"Group\")\n\np3 &lt;- rand_group |&gt; \n  count(group, eye_colour) |&gt; \n  group_by(group) |&gt;\n  mutate(prop = n / sum(n)) |&gt; \n  ggplot(aes(x = prop, y = reorder(eye_colour, n), fill = group)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  labs(x = \"Percent of individuals\",\n       y = \"Eye color\",\n       fill = \"Group\") + \n  scale_x_continuous(labels = scales::percent)\n\np1 | p2 | p3\n\n\n\n\n\n\n\nAgain, these two groups are nearly identical to one another, on average. In fact, if we did this many, many, many times, these groups would be, on average, increasingly identical.\n\n\n\n\n\n\nWhy?\n\n\n\nBecause of the Central Limit Theorem and Law of Large Numbers. We will talk about these two concepts later in the course.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  },
  {
    "objectID": "content/02-experiments.html#returning-to-our-question",
    "href": "content/02-experiments.html#returning-to-our-question",
    "title": "Causes and Effects",
    "section": "Returning to our question",
    "text": "Returning to our question\nSo, do international election monitors deter election-fraud? Yes! The international community monitored the 2003 Armenian Presidential elections. Monitors were assigned randomly to the election’s polling stations. Hyde analysed the differences in votes won by each party at these booths. She found a large difference between the vote share received by the cheating party at monitored stations compared to non-monitored stations, on average.",
    "crumbs": [
      "Content",
      "Session 2",
      "Causes and Effects"
    ]
  }
]